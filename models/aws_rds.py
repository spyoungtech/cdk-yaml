from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams

#  autogenerated from aws_cdk.aws_rds.AuroraEngineVersion
class AuroraEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.AuroraEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AuroraEngineVersionDefConfig] = pydantic.Field(None)


class AuroraEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[AuroraEngineVersionDefOfParams]] = pydantic.Field(None, description='Create a new AuroraEngineVersion with an arbitrary version.')

class AuroraEngineVersionDefOfParams(pydantic.BaseModel):
    aurora_full_version: str = pydantic.Field(..., description='the full version string, for example "5.6.mysql_aurora.1.78.3.6".\n')
    aurora_major_version: typing.Optional[str] = pydantic.Field(None, description='the major version of the engine, defaults to "5.6".')
    return_config: typing.Optional[list[models.aws_rds.AuroraEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.AuroraMysqlEngineVersion
class AuroraMysqlEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.AuroraMysqlEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AuroraMysqlEngineVersionDefConfig] = pydantic.Field(None)


class AuroraMysqlEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[AuroraMysqlEngineVersionDefOfParams]] = pydantic.Field(None, description='Create a new AuroraMysqlEngineVersion with an arbitrary version.')

class AuroraMysqlEngineVersionDefOfParams(pydantic.BaseModel):
    aurora_mysql_full_version: str = pydantic.Field(..., description='the full version string, for example "5.7.mysql_aurora.2.78.3.6".\n')
    aurora_mysql_major_version: typing.Optional[str] = pydantic.Field(None, description='the major version of the engine, defaults to "5.7".')
    return_config: typing.Optional[list[models.aws_rds.AuroraMysqlEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.AuroraPostgresEngineVersion
class AuroraPostgresEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.AuroraPostgresEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[AuroraPostgresEngineVersionDefConfig] = pydantic.Field(None)


class AuroraPostgresEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[AuroraPostgresEngineVersionDefOfParams]] = pydantic.Field(None, description='Create a new AuroraPostgresEngineVersion with an arbitrary version.')

class AuroraPostgresEngineVersionDefOfParams(pydantic.BaseModel):
    aurora_postgres_full_version: str = pydantic.Field(..., description='the full version string, for example "9.6.25.1".\n')
    aurora_postgres_major_version: str = pydantic.Field(..., description='the major version of the engine, for example "9.6".\n')
    s3_export: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Aurora Postgres cluster engine supports the S3 data export feature. Default: false\n')
    s3_import: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Aurora Postgres cluster engine supports the S3 data import feature. Default: false')
    return_config: typing.Optional[list[models.aws_rds.AuroraPostgresEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.ClusterInstance
class ClusterInstanceDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['provisioned', 'serverless_v2']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterInstance'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['provisioned', 'serverless_v2']
    ...


    provisioned: typing.Optional[ClusterInstanceDefProvisionedParams] = pydantic.Field(None, description='Add a provisioned instance to the cluster.')
    serverless_v2: typing.Optional[ClusterInstanceDefServerlessV2Params] = pydantic.Field(None, description='Add a serverless v2 instance to the cluster.')
    resource_config: typing.Optional[ClusterInstanceDefConfig] = pydantic.Field(None)


class ClusterInstanceDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ClusterInstanceDefBindParams]] = pydantic.Field(None, description='Add the ClusterInstance to the cluster.')

class ClusterInstanceDefBindParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    cluster: typing.Union[models.aws_rds.DatabaseClusterBaseDef, models.aws_rds.DatabaseClusterDef, models.aws_rds.DatabaseClusterFromSnapshotDef] = pydantic.Field(..., description='-\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instances. Default: no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instances monitoring. Default: - A role is automatically created for you\n')
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description='The promotion tier of the cluster instance. This matters more for serverlessV2 instances. If a serverless instance is in tier 0-1 then it will scale with the writer. For provisioned instances this just determines the failover priority. If multiple instances have the same priority then one will be picked at random Default: 2\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy on the cluster. Default: - RemovalPolicy.DESTROY (cluster snapshot can restore)\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. This is only needed when using the isFromLegacyInstanceProps Default: - cluster subnet group is used')
    return_config: typing.Optional[list[models._interface_methods.AwsRdsIAuroraClusterInstanceDefConfig]] = pydantic.Field(None)
    ...

class ClusterInstanceDefProvisionedParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The cluster instance type. Default: db.t3.medium\n')
    is_from_legacy_instance_props: typing.Optional[bool] = pydantic.Field(None, description='Only used for migrating existing clusters from using ``instanceProps`` to ``writer`` and ``readers``. Default: false\n')
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description='The promotion tier of the cluster instance. Can be between 0-15 For provisioned instances this just determines the failover priority. If multiple instances have the same priority then one will be picked at random Default: 2\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow upgrade of major version for the DB instance. Default: - false\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable automatic upgrade of minor version for the DB instance. Default: - true\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier for the database instance. Default: - CloudFormation generated identifier\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. This is only needed if you need to configure different parameter groups for each individual instance, otherwise you should not provide this and just use the cluster parameter group Default: the cluster parameter group is used\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - true if the instance is placed in a public subnet\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    ClusterInstance.provisioned("ClusterInstance",\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.R6G, ec2.InstanceSize.XLARGE4)\n    )\n')
    ...

class ClusterInstanceDefServerlessV2Params(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    scale_with_writer: typing.Optional[bool] = pydantic.Field(None, description='Only applicable to reader instances. If this is true then the instance will be placed in promotion tier 1, otherwise it will be placed in promotion tier 2. For serverless v2 instances this means: - true: The serverless v2 reader will scale to match the writer instance (provisioned or serverless) - false: The serverless v2 reader will scale with the read workfload on the instance Default: false\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow upgrade of major version for the DB instance. Default: - false\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable automatic upgrade of minor version for the DB instance. Default: - true\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier for the database instance. Default: - CloudFormation generated identifier\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. This is only needed if you need to configure different parameter groups for each individual instance, otherwise you should not provide this and just use the cluster parameter group Default: the cluster parameter group is used\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - true if the instance is placed in a public subnet\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    ClusterInstance.serverless_v2("ClusterInstance",\n        scale_with_writer=True\n    )\n')
    ...


#  autogenerated from aws_cdk.aws_rds.ClusterInstanceType
class ClusterInstanceTypeDef(BaseClass):
    instance_type: str = pydantic.Field(..., description='-')
    type: aws_cdk.aws_rds.InstanceType = pydantic.Field(..., description='-')
    _init_params: typing.ClassVar[list[str]] = ['instance_type', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['provisioned', 'serverless_v2']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterInstanceType'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ClusterInstanceTypeDefConfig] = pydantic.Field(None)


class ClusterInstanceTypeDefConfig(pydantic.BaseModel):
    provisioned: typing.Optional[list[ClusterInstanceTypeDefProvisionedParams]] = pydantic.Field(None, description='Aurora Provisioned instance type.')
    serverless_v2: typing.Optional[list[ClusterInstanceTypeDefServerlessV2Params]] = pydantic.Field(None, description='Aurora Serverless V2 instance type.\n:see: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html')

class ClusterInstanceTypeDefProvisionedParams(pydantic.BaseModel):
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_rds.ClusterInstanceTypeDefConfig]] = pydantic.Field(None)
    ...

class ClusterInstanceTypeDefServerlessV2Params(pydantic.BaseModel):
    return_config: typing.Optional[list[models.aws_rds.ClusterInstanceTypeDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.Credentials
class CredentialsDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_generated_secret', 'from_password', 'from_secret', 'from_username']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.Credentials'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CredentialsDefConfig] = pydantic.Field(None)


class CredentialsDefConfig(pydantic.BaseModel):
    from_generated_secret: typing.Optional[list[CredentialsDefFromGeneratedSecretParams]] = pydantic.Field(None, description='Creates Credentials with a password generated and stored in Secrets Manager.')
    from_password: typing.Optional[list[CredentialsDefFromPasswordParams]] = pydantic.Field(None, description='Creates Credentials from a password.\nDo not put passwords in your CDK code directly.')
    from_secret: typing.Optional[list[CredentialsDefFromSecretParams]] = pydantic.Field(None, description='Creates Credentials from an existing Secrets Manager ``Secret`` (or ``DatabaseSecret``).\nThe Secret must be a JSON string with a ``username`` and ``password`` field::\n\n   {\n     ...\n     "username": <required: username>,\n     "password": <required: password>,\n   }')
    from_username: typing.Optional[list[CredentialsDefFromUsernameParams]] = pydantic.Field(None, description='Creates Credentials for the given username, and optional password and key.\nIf no password is provided, one will be generated and stored in Secrets Manager.')

class CredentialsDefFromGeneratedSecretParams(pydantic.BaseModel):
    username: str = pydantic.Field(..., description='-\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key to encrypt the generated secret. Default: - default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='The characters to exclude from the generated password. Has no effect if ``password`` has been provided. Default: - the DatabaseSecret default exclude character set (" %+~`#$&*()|[]{}:;<>?!\'/@"\\")\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated\n')
    secret_name: typing.Optional[str] = pydantic.Field(None, description='The name of the secret. Default: - A name is generated by CloudFormation.')
    return_config: typing.Optional[list[models.aws_rds.CredentialsDefConfig]] = pydantic.Field(None)
    ...

class CredentialsDefFromPasswordParams(pydantic.BaseModel):
    username: str = pydantic.Field(..., description='-\n')
    password: models.SecretValueDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_rds.CredentialsDefConfig]] = pydantic.Field(None)
    ...

class CredentialsDefFromSecretParams(pydantic.BaseModel):
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret where the credentials are stored.\n')
    username: typing.Optional[str] = pydantic.Field(None, description='The username defined in the secret. If specified the username will be referenced as a string and not a dynamic reference to the username field in the secret. This allows to replace the secret without replacing the instance or cluster.')
    return_config: typing.Optional[list[models.aws_rds.CredentialsDefConfig]] = pydantic.Field(None)
    ...

class CredentialsDefFromUsernameParams(pydantic.BaseModel):
    username: str = pydantic.Field(..., description='-\n')
    password: typing.Optional[models.SecretValueDef] = pydantic.Field(None, description='Password. Do not put passwords in your CDK code directly. Default: - a Secrets Manager generated password\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key to encrypt the generated secret. Default: - default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='The characters to exclude from the generated password. Has no effect if ``password`` has been provided. Default: - the DatabaseSecret default exclude character set (" %+~`#$&*()|[]{}:;<>?!\'/@"\\")\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated\n')
    secret_name: typing.Optional[str] = pydantic.Field(None, description='The name of the secret. Default: - A name is generated by CloudFormation.')
    return_config: typing.Optional[list[models.aws_rds.CredentialsDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseClusterBase
class DatabaseClusterBaseDef(BaseClass):
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID this resource belongs to. Default: - the resource is in the same account as the stack it belongs to\n')
    environment_from_arn: typing.Optional[str] = pydantic.Field(None, description='ARN to deduce region and account from. The ARN is parsed and the account and region are taken from the ARN. This should be used for imported resources. Cannot be supplied together with either ``account`` or ``region``. Default: - take environment from ``account``, ``region`` parameters, or use Stack environment.\n')
    physical_name: typing.Optional[str] = pydantic.Field(None, description='The value passed in by users to the physical name prop of the resource. - ``undefined`` implies that a physical name will be allocated by CloudFormation during deployment. - a concrete value implies a specific physical name - ``PhysicalName.GENERATE_IF_NEEDED`` is a marker that indicates that a physical will only be generated by the CDK if it is needed for cross-environment references. Otherwise, it will be allocated by CloudFormation. Default: - The physical name will be allocated by CloudFormation at deployment time\n')
    region: typing.Optional[str] = pydantic.Field(None, description='The AWS region this resource belongs to. Default: - the resource is in the same region as the stack it belongs to')
    _init_params: typing.ClassVar[list[str]] = ['account', 'environment_from_arn', 'physical_name', 'region']
    _method_names: typing.ClassVar[list[str]] = ['add_proxy', 'apply_removal_policy', 'metric', 'metric_cpu_utilization', 'metric_database_connections', 'metric_deadlocks', 'metric_engine_uptime', 'metric_free_local_storage', 'metric_freeable_memory', 'metric_network_receive_throughput', 'metric_network_throughput', 'metric_network_transmit_throughput', 'metric_snapshot_storage_used', 'metric_total_backup_storage_billed', 'metric_volume_bytes_used', 'metric_volume_read_io_ps', 'metric_volume_write_io_ps']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseClusterBase'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseClusterBaseDefConfig] = pydantic.Field(None)


class DatabaseClusterBaseDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[DatabaseClusterBaseDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this cluster.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    metric: typing.Optional[list[DatabaseClusterBaseDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBCluster.')
    metric_cpu_utilization: typing.Optional[list[DatabaseClusterBaseDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[DatabaseClusterBaseDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_deadlocks: typing.Optional[list[DatabaseClusterBaseDefMetricDeadlocksParams]] = pydantic.Field(None, description='The average number of deadlocks in the database per second.\nAverage over 5 minutes')
    metric_engine_uptime: typing.Optional[list[DatabaseClusterBaseDefMetricEngineUptimeParams]] = pydantic.Field(None, description='The amount of time that the instance has been running, in seconds.\nAverage over 5 minutes')
    metric_free_local_storage: typing.Optional[list[DatabaseClusterBaseDefMetricFreeLocalStorageParams]] = pydantic.Field(None, description='The amount of local storage available, in bytes.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[DatabaseClusterBaseDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory, in bytes.\nAverage over 5 minutes')
    metric_network_receive_throughput: typing.Optional[list[DatabaseClusterBaseDefMetricNetworkReceiveThroughputParams]] = pydantic.Field(None, description='The amount of network throughput received from clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_throughput: typing.Optional[list[DatabaseClusterBaseDefMetricNetworkThroughputParams]] = pydantic.Field(None, description='The amount of network throughput both received from and transmitted to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_transmit_throughput: typing.Optional[list[DatabaseClusterBaseDefMetricNetworkTransmitThroughputParams]] = pydantic.Field(None, description='The amount of network throughput sent to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_snapshot_storage_used: typing.Optional[list[DatabaseClusterBaseDefMetricSnapshotStorageUsedParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes consumed by all Aurora snapshots outside its backup retention window.\nAverage over 5 minutes')
    metric_total_backup_storage_billed: typing.Optional[list[DatabaseClusterBaseDefMetricTotalBackupStorageBilledParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes for which you are billed.\nAverage over 5 minutes')
    metric_volume_bytes_used: typing.Optional[list[DatabaseClusterBaseDefMetricVolumeBytesUsedParams]] = pydantic.Field(None, description='The amount of storage used by your Aurora DB instance, in bytes.\nAverage over 5 minutes')
    metric_volume_read_io_ps: typing.Optional[list[DatabaseClusterBaseDefMetricVolumeReadIoPsParams]] = pydantic.Field(None, description='The number of billed read I/O operations from a cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')
    metric_volume_write_io_ps: typing.Optional[list[DatabaseClusterBaseDefMetricVolumeWriteIoPsParams]] = pydantic.Field(None, description='The number of write disk I/O operations to the cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class DatabaseClusterBaseDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseClusterBaseDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricDeadlocksParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricEngineUptimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricFreeLocalStorageParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricNetworkReceiveThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricNetworkThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricNetworkTransmitThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricSnapshotStorageUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricTotalBackupStorageBilledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricVolumeBytesUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricVolumeReadIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterBaseDefMetricVolumeWriteIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseClusterEngine
class DatabaseClusterEngineDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['aurora', 'aurora_mysql', 'aurora_postgres']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseClusterEngine'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['aurora', 'aurora_mysql', 'aurora_postgres']
    ...


    aurora: typing.Optional[DatabaseClusterEngineDefAuroraParams] = pydantic.Field(None, description='Creates a new plain Aurora database cluster engine.')
    aurora_mysql: typing.Optional[DatabaseClusterEngineDefAuroraMysqlParams] = pydantic.Field(None, description='Creates a new Aurora MySQL database cluster engine.')
    aurora_postgres: typing.Optional[DatabaseClusterEngineDefAuroraPostgresParams] = pydantic.Field(None, description='Creates a new Aurora PostgreSQL database cluster engine.')

class DatabaseClusterEngineDefAuroraParams(pydantic.BaseModel):
    version: models.aws_rds.AuroraEngineVersionDef = pydantic.Field(..., description='The version of the Aurora cluster engine.')
    ...

class DatabaseClusterEngineDefAuroraMysqlParams(pydantic.BaseModel):
    version: models.aws_rds.AuroraMysqlEngineVersionDef = pydantic.Field(..., description='The version of the Aurora MySQL cluster engine.')
    ...

class DatabaseClusterEngineDefAuroraPostgresParams(pydantic.BaseModel):
    version: models.aws_rds.AuroraPostgresEngineVersionDef = pydantic.Field(..., description='The version of the Aurora PostgreSQL cluster engine.')
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceBase
class DatabaseInstanceBaseDef(BaseClass):
    account: typing.Optional[str] = pydantic.Field(None, description='The AWS account ID this resource belongs to. Default: - the resource is in the same account as the stack it belongs to\n')
    environment_from_arn: typing.Optional[str] = pydantic.Field(None, description='ARN to deduce region and account from. The ARN is parsed and the account and region are taken from the ARN. This should be used for imported resources. Cannot be supplied together with either ``account`` or ``region``. Default: - take environment from ``account``, ``region`` parameters, or use Stack environment.\n')
    physical_name: typing.Optional[str] = pydantic.Field(None, description='The value passed in by users to the physical name prop of the resource. - ``undefined`` implies that a physical name will be allocated by CloudFormation during deployment. - a concrete value implies a specific physical name - ``PhysicalName.GENERATE_IF_NEEDED`` is a marker that indicates that a physical will only be generated by the CDK if it is needed for cross-environment references. Otherwise, it will be allocated by CloudFormation. Default: - The physical name will be allocated by CloudFormation at deployment time\n')
    region: typing.Optional[str] = pydantic.Field(None, description='The AWS region this resource belongs to. Default: - the resource is in the same region as the stack it belongs to')
    _init_params: typing.ClassVar[list[str]] = ['account', 'environment_from_arn', 'physical_name', 'region']
    _method_names: typing.ClassVar[list[str]] = ['add_proxy', 'apply_removal_policy', 'grant_connect', 'metric', 'metric_cpu_utilization', 'metric_database_connections', 'metric_free_storage_space', 'metric_freeable_memory', 'metric_read_iops', 'metric_write_iops', 'on_event']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceBase'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    ...


    from_database_instance_attributes: typing.Optional[DatabaseInstanceBaseDefFromDatabaseInstanceAttributesParams] = pydantic.Field(None, description='Import an existing database instance.')
    resource_config: typing.Optional[DatabaseInstanceBaseDefConfig] = pydantic.Field(None)


class DatabaseInstanceBaseDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[DatabaseInstanceBaseDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_connect: typing.Optional[list[DatabaseInstanceBaseDefGrantConnectParams]] = pydantic.Field(None, description='Grant the given identity connection access to the database.')
    metric: typing.Optional[list[DatabaseInstanceBaseDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBInstance.')
    metric_cpu_utilization: typing.Optional[list[DatabaseInstanceBaseDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[DatabaseInstanceBaseDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_free_storage_space: typing.Optional[list[DatabaseInstanceBaseDefMetricFreeStorageSpaceParams]] = pydantic.Field(None, description='The amount of available storage space.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[DatabaseInstanceBaseDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory.\nAverage over 5 minutes')
    metric_read_iops: typing.Optional[list[DatabaseInstanceBaseDefMetricReadIopsParams]] = pydantic.Field(None, description='The average number of disk write I/O operations per second.\nAverage over 5 minutes')
    metric_write_iops: typing.Optional[list[DatabaseInstanceBaseDefMetricWriteIopsParams]] = pydantic.Field(None, description='The average number of disk read I/O operations per second.\nAverage over 5 minutes')
    on_event: typing.Optional[list[DatabaseInstanceBaseDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers for instance events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class DatabaseInstanceBaseDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseInstanceBaseDefFromDatabaseInstanceAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    instance_endpoint_address: str = pydantic.Field(..., description='The endpoint address.\n')
    instance_identifier: str = pydantic.Field(..., description='The instance identifier.\n')
    port: typing.Union[int, float] = pydantic.Field(..., description='The database port.\n')
    security_groups: typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(..., description='The security groups of the instance.\n')
    engine: typing.Optional[typing.Union[models.aws_rds.DatabaseInstanceEngineDef]] = pydantic.Field(None, description="The engine of the existing database Instance. Default: - the imported Instance's engine is unknown\n")
    instance_resource_id: typing.Optional[str] = pydantic.Field(None, description='The AWS Region-unique, immutable identifier for the DB instance. This identifier is found in AWS CloudTrail log entries whenever the AWS KMS key for the DB instance is accessed.')
    ...

class DatabaseInstanceBaseDefGrantConnectParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    db_user: typing.Optional[str] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefMetricFreeStorageSpaceParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefMetricReadIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefMetricWriteIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceBaseDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceEngine
class DatabaseInstanceEngineDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['maria_db', 'mysql', 'oracle_ee', 'oracle_ee_cdb', 'oracle_se2', 'oracle_se2_cdb', 'postgres', 'sql_server_ee', 'sql_server_ex', 'sql_server_se', 'sql_server_web']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceEngine'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['maria_db', 'mysql', 'oracle_ee', 'oracle_ee_cdb', 'oracle_se2', 'oracle_se2_cdb', 'postgres', 'sql_server_ee', 'sql_server_ex', 'sql_server_se', 'sql_server_web']
    ...


    maria_db: typing.Optional[DatabaseInstanceEngineDefMariaDbParams] = pydantic.Field(None, description='Creates a new MariaDB instance engine.')
    mysql: typing.Optional[DatabaseInstanceEngineDefMysqlParams] = pydantic.Field(None, description='Creates a new MySQL instance engine.')
    oracle_ee: typing.Optional[DatabaseInstanceEngineDefOracleEeParams] = pydantic.Field(None, description='Creates a new Oracle Enterprise Edition instance engine.')
    oracle_ee_cdb: typing.Optional[DatabaseInstanceEngineDefOracleEeCdbParams] = pydantic.Field(None, description='Creates a new Oracle Enterprise Edition (CDB) instance engine.')
    oracle_se2: typing.Optional[DatabaseInstanceEngineDefOracleSe2Params] = pydantic.Field(None, description='Creates a new Oracle Standard Edition 2 instance engine.')
    oracle_se2_cdb: typing.Optional[DatabaseInstanceEngineDefOracleSe2CdbParams] = pydantic.Field(None, description='Creates a new Oracle Standard Edition 2 (CDB) instance engine.')
    postgres: typing.Optional[DatabaseInstanceEngineDefPostgresParams] = pydantic.Field(None, description='Creates a new PostgreSQL instance engine.')
    sql_server_ee: typing.Optional[DatabaseInstanceEngineDefSqlServerEeParams] = pydantic.Field(None, description='Creates a new SQL Server Enterprise Edition instance engine.')
    sql_server_ex: typing.Optional[DatabaseInstanceEngineDefSqlServerExParams] = pydantic.Field(None, description='Creates a new SQL Server Express Edition instance engine.')
    sql_server_se: typing.Optional[DatabaseInstanceEngineDefSqlServerSeParams] = pydantic.Field(None, description='Creates a new SQL Server Standard Edition instance engine.')
    sql_server_web: typing.Optional[DatabaseInstanceEngineDefSqlServerWebParams] = pydantic.Field(None, description='Creates a new SQL Server Web Edition instance engine.')

class DatabaseInstanceEngineDefMariaDbParams(pydantic.BaseModel):
    version: models.aws_rds.MariaDbEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefMysqlParams(pydantic.BaseModel):
    version: models.aws_rds.MysqlEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefOracleEeParams(pydantic.BaseModel):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefOracleEeCdbParams(pydantic.BaseModel):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefOracleSe2Params(pydantic.BaseModel):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefOracleSe2CdbParams(pydantic.BaseModel):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefPostgresParams(pydantic.BaseModel):
    version: models.aws_rds.PostgresEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefSqlServerEeParams(pydantic.BaseModel):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefSqlServerExParams(pydantic.BaseModel):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefSqlServerSeParams(pydantic.BaseModel):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...

class DatabaseInstanceEngineDefSqlServerWebParams(pydantic.BaseModel):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.')
    ...


#  autogenerated from aws_cdk.aws_rds.Endpoint
class EndpointDef(BaseClass):
    address: str = pydantic.Field(..., description='-')
    port: typing.Union[int, float] = pydantic.Field(..., description='-')
    _init_params: typing.ClassVar[list[str]] = ['address', 'port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.Endpoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.MariaDbEngineVersion
class MariaDbEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.MariaDbEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[MariaDbEngineVersionDefConfig] = pydantic.Field(None)


class MariaDbEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[MariaDbEngineVersionDefOfParams]] = pydantic.Field(None, description='Create a new MariaDbEngineVersion with an arbitrary version.')

class MariaDbEngineVersionDefOfParams(pydantic.BaseModel):
    maria_db_full_version: str = pydantic.Field(..., description='the full version string, for example "10.5.28".\n')
    maria_db_major_version: str = pydantic.Field(..., description='the major version of the engine, for example "10.5".')
    return_config: typing.Optional[list[models.aws_rds.MariaDbEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.MysqlEngineVersion
class MysqlEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.MysqlEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[MysqlEngineVersionDefConfig] = pydantic.Field(None)


class MysqlEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[MysqlEngineVersionDefOfParams]] = pydantic.Field(None, description='Create a new MysqlEngineVersion with an arbitrary version.')

class MysqlEngineVersionDefOfParams(pydantic.BaseModel):
    mysql_full_version: str = pydantic.Field(..., description='the full version string, for example "8.1.43".\n')
    mysql_major_version: str = pydantic.Field(..., description='the major version of the engine, for example "8.1".')
    return_config: typing.Optional[list[models.aws_rds.MysqlEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.OracleEngineVersion
class OracleEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OracleEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[OracleEngineVersionDefConfig] = pydantic.Field(None)


class OracleEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[OracleEngineVersionDefOfParams]] = pydantic.Field(None, description='Creates a new OracleEngineVersion with an arbitrary version.')

class OracleEngineVersionDefOfParams(pydantic.BaseModel):
    oracle_full_version: str = pydantic.Field(..., description='the full version string, for example "19.0.0.0.ru-2019-10.rur-2019-10.r1".\n')
    oracle_major_version: str = pydantic.Field(..., description='the major version of the engine, for example "19".')
    return_config: typing.Optional[list[models.aws_rds.OracleEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.PostgresEngineVersion
class PostgresEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.PostgresEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[PostgresEngineVersionDefConfig] = pydantic.Field(None)


class PostgresEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[PostgresEngineVersionDefOfParams]] = pydantic.Field(None, description='Create a new PostgresEngineVersion with an arbitrary version.')

class PostgresEngineVersionDefOfParams(pydantic.BaseModel):
    postgres_full_version: str = pydantic.Field(..., description='the full version string, for example "13.11".\n')
    postgres_major_version: str = pydantic.Field(..., description='the major version of the engine, for example "13".\n')
    s3_export: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Postgres engine supports the S3 data export feature. Default: false\n')
    s3_import: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Postgres engine supports the S3 data import feature. Default: false')
    return_config: typing.Optional[list[models.aws_rds.PostgresEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.ProxyTarget
class ProxyTargetDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = ['bind']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_cluster', 'from_instance']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ProxyTarget'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ProxyTargetDefConfig] = pydantic.Field(None)


class ProxyTargetDefConfig(pydantic.BaseModel):
    bind: typing.Optional[list[ProxyTargetDefBindParams]] = pydantic.Field(None, description='Bind this target to the specified database proxy.')
    from_cluster: typing.Optional[list[ProxyTargetDefFromClusterParams]] = pydantic.Field(None, description='From cluster.')
    from_instance: typing.Optional[list[ProxyTargetDefFromInstanceParams]] = pydantic.Field(None, description='From instance.')

class ProxyTargetDefBindParams(pydantic.BaseModel):
    proxy: models.aws_rds.DatabaseProxyDef = pydantic.Field(..., description='-')
    ...

class ProxyTargetDefFromClusterParams(pydantic.BaseModel):
    cluster: typing.Union[models.aws_rds.DatabaseClusterBaseDef, models.aws_rds.DatabaseClusterDef, models.aws_rds.DatabaseClusterFromSnapshotDef] = pydantic.Field(..., description='RDS database cluster.')
    return_config: typing.Optional[list[models.aws_rds.ProxyTargetDefConfig]] = pydantic.Field(None)
    ...

class ProxyTargetDefFromInstanceParams(pydantic.BaseModel):
    instance: typing.Union[models.aws_rds.DatabaseInstanceBaseDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceReadReplicaDef, models.aws_rds.DatabaseInstanceReadReplicaDef] = pydantic.Field(..., description='RDS database instance.')
    return_config: typing.Optional[list[models.aws_rds.ProxyTargetDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.SessionPinningFilter
class SessionPinningFilterDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SessionPinningFilter'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SessionPinningFilterDefConfig] = pydantic.Field(None)


class SessionPinningFilterDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[SessionPinningFilterDefOfParams]] = pydantic.Field(None, description='custom filter.')

class SessionPinningFilterDefOfParams(pydantic.BaseModel):
    filter_name: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_rds.SessionPinningFilterDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.SnapshotCredentials
class SnapshotCredentialsDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['from_generated_password', 'from_generated_secret', 'from_password', 'from_secret']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SnapshotCredentials'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SnapshotCredentialsDefConfig] = pydantic.Field(None)


class SnapshotCredentialsDefConfig(pydantic.BaseModel):
    from_generated_password: typing.Optional[list[SnapshotCredentialsDefFromGeneratedPasswordParams]] = pydantic.Field(None, description='Generate a new password for the snapshot, using the existing username and an optional encryption key.\nNote - The username must match the existing master username of the snapshot.\n\nNOTE: use ``fromGeneratedSecret()`` for new Clusters and Instances. Switching from\n``fromGeneratedPassword()`` to ``fromGeneratedSecret()`` for already deployed Clusters\nor Instances will update their master password.')
    from_generated_secret: typing.Optional[list[SnapshotCredentialsDefFromGeneratedSecretParams]] = pydantic.Field(None, description='Generate a new password for the snapshot, using the existing username and an optional encryption key.\nThe new credentials are stored in Secrets Manager.\n\nNote - The username must match the existing master username of the snapshot.')
    from_password: typing.Optional[list[SnapshotCredentialsDefFromPasswordParams]] = pydantic.Field(None, description='Update the snapshot login with an existing password.')
    from_secret: typing.Optional[list[SnapshotCredentialsDefFromSecretParams]] = pydantic.Field(None, description='Update the snapshot login with an existing password from a Secret.\nThe Secret must be a JSON string with a ``password`` field::\n\n   {\n     ...\n     "password": <required: password>,\n   }')

class SnapshotCredentialsDefFromGeneratedPasswordParams(pydantic.BaseModel):
    username: str = pydantic.Field(..., description='-\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key to encrypt the generated secret. Default: - default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='The characters to exclude from the generated password. Default: - the DatabaseSecret default exclude character set (" %+~`#$&*()|[]{}:;<>?!\'/@"\\")\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated')
    return_config: typing.Optional[list[models.aws_rds.SnapshotCredentialsDefConfig]] = pydantic.Field(None)
    ...

class SnapshotCredentialsDefFromGeneratedSecretParams(pydantic.BaseModel):
    username: str = pydantic.Field(..., description='-\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key to encrypt the generated secret. Default: - default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='The characters to exclude from the generated password. Default: - the DatabaseSecret default exclude character set (" %+~`#$&*()|[]{}:;<>?!\'/@"\\")\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated')
    return_config: typing.Optional[list[models.aws_rds.SnapshotCredentialsDefConfig]] = pydantic.Field(None)
    ...

class SnapshotCredentialsDefFromPasswordParams(pydantic.BaseModel):
    password: models.SecretValueDef = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_rds.SnapshotCredentialsDefConfig]] = pydantic.Field(None)
    ...

class SnapshotCredentialsDefFromSecretParams(pydantic.BaseModel):
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_rds.SnapshotCredentialsDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.SqlServerEngineVersion
class SqlServerEngineVersionDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['of']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SqlServerEngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqlServerEngineVersionDefConfig] = pydantic.Field(None)


class SqlServerEngineVersionDefConfig(pydantic.BaseModel):
    of: typing.Optional[list[SqlServerEngineVersionDefOfParams]] = pydantic.Field(None, description='Create a new SqlServerEngineVersion with an arbitrary version.')

class SqlServerEngineVersionDefOfParams(pydantic.BaseModel):
    sql_server_full_version: str = pydantic.Field(..., description='the full version string, for example "15.00.3049.1.v1".\n')
    sql_server_major_version: str = pydantic.Field(..., description='the major version of the engine, for example "15.00".')
    return_config: typing.Optional[list[models.aws_rds.SqlServerEngineVersionDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseCluster
class DatabaseClusterDef(BaseConstruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    backtrack_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of seconds to set a cluster's target backtrack window to. This feature is only supported by the Aurora MySQL database engine and cannot be enabled on existing clusters. Default: 0 seconds (no backtrack)\n")
    backup: typing.Union[models.aws_rds.BackupPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Backup settings. Default: - Backup retention period for automated backups is 1 day. Backup preferred window is set to a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="Credentials for the administrative user. Default: - A username of 'admin' (or 'postgres' for PostgreSQL) and SecretsManager-generated password\n")
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, ``undefined`` otherwise, which will not enable deletion protection. To disable deletion protection after it has been enabled, you must explicitly set this value to ``false``.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier_base: typing.Optional[str] = pydantic.Field(None, description='Base identifier for instances. Every replica is named by appending the replica number to this string, 1-based. Default: - clusterIdentifier is used with the word "Instance" appended. If clusterIdentifier is not provided, the identifier is automatically generated.\n')
    instance_props: typing.Union[models.aws_rds.InstancePropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(deprecated) Settings for the individual instances that are launched.\n')
    instances: typing.Union[int, float, None] = pydantic.Field(None, description='(deprecated) How many replicas/instances to create. Has to be at least 1. Default: 2\n')
    instance_update_behaviour: typing.Optional[aws_cdk.aws_rds.InstanceUpdateBehaviour] = pydantic.Field(None, description='The ordering of updates for instances. Default: InstanceUpdateBehaviour.BULK\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instances. Default: no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instances monitoring. Default: - A role is automatically created for you\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - No parameter group.\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBClusterParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBClusterParameterGroup. Default: - None\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='What port to listen on. Default: - The default for the engine is used.\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description="A preferred maintenance window day/time range. Should be specified as a range ddd:hh24:mi-ddd:hh24:mi (24H Clock UTC). Example: 'Sun:23:45-Mon:00:15' Default: - 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n")
    readers: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.ClusterInstanceDef]]] = pydantic.Field(None, description='A list of instances to create as cluster reader instances. Default: - no readers are created. The cluster will have a single writer/reader\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportRole`` is used. For MySQL: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 export. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportRole`` is used. For MySQL: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 import. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: a new security group is created.\n')
    serverless_v2_max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 40, 40.5, 41, and so on. The largest value that you can use is 128 (256GB). The maximum capacity must be higher than 0.5 ACUs. Default: 2\n')
    serverless_v2_min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 8, 8.5, 9, and so on. The smallest value that you can use is 0.5. Default: 0.5\n')
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable storage encryption. Default: - true if storageEncryptionKey is provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key for storage encryption. If specified, ``storageEncrypted`` will be set to ``true``. Default: - if storageEncrypted is true then the default master key, no key otherwise\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.DBClusterStorageType] = pydantic.Field(None, description='The storage type to be associated with the DB cluster. Default: - DBClusterStorageType.AURORA_IOPT1\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group will be created.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='What subnets to run the RDS instances in. Must be at least 2 subnets in two different AZs.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. Default: - the Vpc default strategy if not specified.\n')
    writer: typing.Optional[typing.Union[models.aws_rds.ClusterInstanceDef]] = pydantic.Field(None, description='The instance to use for the cluster writer. Default: required if instanceProps is not provided')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'backtrack_window', 'backup', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'iam_authentication', 'instance_identifier_base', 'instance_props', 'instances', 'instance_update_behaviour', 'monitoring_interval', 'monitoring_role', 'network_type', 'parameter_group', 'parameters', 'port', 'preferred_maintenance_window', 'readers', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'serverless_v2_max_capacity', 'serverless_v2_min_capacity', 'storage_encrypted', 'storage_encryption_key', 'storage_type', 'subnet_group', 'vpc', 'vpc_subnets', 'writer']
    _method_names: typing.ClassVar[list[str]] = ['add_proxy', 'add_rotation_multi_user', 'add_rotation_single_user', 'apply_removal_policy', 'metric', 'metric_acu_utilization', 'metric_cpu_utilization', 'metric_database_connections', 'metric_deadlocks', 'metric_engine_uptime', 'metric_free_local_storage', 'metric_freeable_memory', 'metric_network_receive_throughput', 'metric_network_throughput', 'metric_network_transmit_throughput', 'metric_serverless_database_capacity', 'metric_snapshot_storage_used', 'metric_total_backup_storage_billed', 'metric_volume_bytes_used', 'metric_volume_read_io_ps', 'metric_volume_write_io_ps']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_database_cluster_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_database_cluster_attributes']
    ...


    from_database_cluster_attributes: typing.Optional[DatabaseClusterDefFromDatabaseClusterAttributesParams] = pydantic.Field(None, description='Import an existing DatabaseCluster from properties.')
    resource_config: typing.Optional[DatabaseClusterDefConfig] = pydantic.Field(None)


class DatabaseClusterDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[DatabaseClusterDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this cluster.')
    add_rotation_multi_user: typing.Optional[list[DatabaseClusterDefAddRotationMultiUserParams]] = pydantic.Field(None, description='Adds the multi user rotation to this cluster.\nSee `Alternating users rotation strategy <https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets_strategies.html#rotating-secrets-two-users>`_')
    add_rotation_single_user: typing.Optional[list[DatabaseClusterDefAddRotationSingleUserParams]] = pydantic.Field(None, description='Adds the single user rotation of the master password to this cluster.\nSee `Single user rotation strategy <https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets_strategies.html#rotating-secrets-one-user-one-password>`_')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    metric: typing.Optional[list[DatabaseClusterDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBCluster.')
    metric_acu_utilization: typing.Optional[list[DatabaseClusterDefMetricAcuUtilizationParams]] = pydantic.Field(None, description="This value is represented as a percentage.\nIt's calculated as the value of the\nServerlessDatabaseCapacity metric divided by the maximum ACU value of the DB cluster.\n\nIf this metric approaches a value of 100.0, the DB instance has scaled up as high as it can.\nConsider increasing the maximum ACU setting for the cluster.")
    metric_cpu_utilization: typing.Optional[list[DatabaseClusterDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[DatabaseClusterDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_deadlocks: typing.Optional[list[DatabaseClusterDefMetricDeadlocksParams]] = pydantic.Field(None, description='The average number of deadlocks in the database per second.\nAverage over 5 minutes')
    metric_engine_uptime: typing.Optional[list[DatabaseClusterDefMetricEngineUptimeParams]] = pydantic.Field(None, description='The amount of time that the instance has been running, in seconds.\nAverage over 5 minutes')
    metric_free_local_storage: typing.Optional[list[DatabaseClusterDefMetricFreeLocalStorageParams]] = pydantic.Field(None, description='The amount of local storage available, in bytes.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[DatabaseClusterDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory, in bytes.\nAverage over 5 minutes')
    metric_network_receive_throughput: typing.Optional[list[DatabaseClusterDefMetricNetworkReceiveThroughputParams]] = pydantic.Field(None, description='The amount of network throughput received from clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_throughput: typing.Optional[list[DatabaseClusterDefMetricNetworkThroughputParams]] = pydantic.Field(None, description='The amount of network throughput both received from and transmitted to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_transmit_throughput: typing.Optional[list[DatabaseClusterDefMetricNetworkTransmitThroughputParams]] = pydantic.Field(None, description='The amount of network throughput sent to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_serverless_database_capacity: typing.Optional[list[DatabaseClusterDefMetricServerlessDatabaseCapacityParams]] = pydantic.Field(None, description='As a cluster-level metric, it represents the average of the ServerlessDatabaseCapacity values of all the Aurora Serverless v2 DB instances in the cluster.')
    metric_snapshot_storage_used: typing.Optional[list[DatabaseClusterDefMetricSnapshotStorageUsedParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes consumed by all Aurora snapshots outside its backup retention window.\nAverage over 5 minutes')
    metric_total_backup_storage_billed: typing.Optional[list[DatabaseClusterDefMetricTotalBackupStorageBilledParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes for which you are billed.\nAverage over 5 minutes')
    metric_volume_bytes_used: typing.Optional[list[DatabaseClusterDefMetricVolumeBytesUsedParams]] = pydantic.Field(None, description='The amount of storage used by your Aurora DB instance, in bytes.\nAverage over 5 minutes')
    metric_volume_read_io_ps: typing.Optional[list[DatabaseClusterDefMetricVolumeReadIoPsParams]] = pydantic.Field(None, description='The number of billed read I/O operations from a cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')
    metric_volume_write_io_ps: typing.Optional[list[DatabaseClusterDefMetricVolumeWriteIoPsParams]] = pydantic.Field(None, description='The number of write disk I/O operations to the cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    multi_user_rotation_application_config: typing.Optional[models.aws_secretsmanager.SecretRotationApplicationDefConfig] = pydantic.Field(None)
    single_user_rotation_application_config: typing.Optional[models.aws_secretsmanager.SecretRotationApplicationDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)

class DatabaseClusterDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefAddRotationMultiUserParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret to rotate. It must be a JSON string with the following format:: { "engine": <required: database engine>, "host": <required: instance host name>, "username": <required: username>, "password": <required: password>, "dbname": <optional: database name>, "port": <optional: if not specified, default port will be used>, "masterarn": <required: the arn of the master secret which will be used to create users/change passwords> }\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseClusterDefAddRotationSingleUserParams(pydantic.BaseModel):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseClusterDefFromDatabaseClusterAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    cluster_identifier: str = pydantic.Field(..., description='Identifier for the cluster.\n')
    cluster_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Cluster endpoint address. Default: - no endpoint address\n')
    cluster_resource_identifier: typing.Optional[str] = pydantic.Field(None, description='The immutable identifier for the cluster; for example: cluster-ABCD1234EFGH5678IJKL90MNOP. This AWS Region-unique identifier is used to grant access to the cluster. Default: none\n')
    engine: typing.Optional[typing.Union[models.aws_rds.DatabaseClusterEngineDef]] = pydantic.Field(None, description="The engine of the existing Cluster. Default: - the imported Cluster's engine is unknown\n")
    instance_endpoint_addresses: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Endpoint addresses of individual instances. Default: - no instance endpoints\n')
    instance_identifiers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Identifier for the instances. Default: - no instance identifiers\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The database port. Default: - none\n')
    reader_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Reader endpoint address. Default: - no reader address\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups of the database cluster. Default: - no security groups')
    ...

class DatabaseClusterDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricAcuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricDeadlocksParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricEngineUptimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricFreeLocalStorageParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricNetworkReceiveThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricNetworkThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricNetworkTransmitThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricServerlessDatabaseCapacityParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricSnapshotStorageUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricTotalBackupStorageBilledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricVolumeBytesUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricVolumeReadIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterDefMetricVolumeWriteIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseClusterFromSnapshot
class DatabaseClusterFromSnapshotDef(BaseConstruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    snapshot_identifier: str = pydantic.Field(..., description='The identifier for the DB instance snapshot or DB cluster snapshot to restore from. You can use either the name or the Amazon Resource Name (ARN) to specify a DB cluster snapshot. However, you can use only the ARN to specify a DB instance snapshot.\n')
    backtrack_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of seconds to set a cluster's target backtrack window to. This feature is only supported by the Aurora MySQL database engine and cannot be enabled on existing clusters. Default: 0 seconds (no backtrack)\n")
    backup: typing.Union[models.aws_rds.BackupPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Backup settings. Default: - Backup retention period for automated backups is 1 day. Backup preferred window is set to a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="(deprecated) Credentials for the administrative user. Note - using this prop only works with ``Credentials.fromPassword()`` with the username of the snapshot, ``Credentials.fromUsername()`` with the username and password of the snapshot or ``Credentials.fromSecret()`` with a secret containing the username and password of the snapshot. Default: - A username of 'admin' (or 'postgres' for PostgreSQL) and SecretsManager-generated password that **will not be applied** to the cluster, use ``snapshotCredentials`` for the correct behavior.\n")
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, ``undefined`` otherwise, which will not enable deletion protection. To disable deletion protection after it has been enabled, you must explicitly set this value to ``false``.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier_base: typing.Optional[str] = pydantic.Field(None, description='Base identifier for instances. Every replica is named by appending the replica number to this string, 1-based. Default: - clusterIdentifier is used with the word "Instance" appended. If clusterIdentifier is not provided, the identifier is automatically generated.\n')
    instance_props: typing.Union[models.aws_rds.InstancePropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(deprecated) Settings for the individual instances that are launched.\n')
    instances: typing.Union[int, float, None] = pydantic.Field(None, description='(deprecated) How many replicas/instances to create. Has to be at least 1. Default: 2\n')
    instance_update_behaviour: typing.Optional[aws_cdk.aws_rds.InstanceUpdateBehaviour] = pydantic.Field(None, description='The ordering of updates for instances. Default: InstanceUpdateBehaviour.BULK\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instances. Default: no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instances monitoring. Default: - A role is automatically created for you\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - No parameter group.\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBClusterParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBClusterParameterGroup. Default: - None\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='What port to listen on. Default: - The default for the engine is used.\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description="A preferred maintenance window day/time range. Should be specified as a range ddd:hh24:mi-ddd:hh24:mi (24H Clock UTC). Example: 'Sun:23:45-Mon:00:15' Default: - 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n")
    readers: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.ClusterInstanceDef]]] = pydantic.Field(None, description='A list of instances to create as cluster reader instances. Default: - no readers are created. The cluster will have a single writer/reader\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportRole`` is used. For MySQL: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 export. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportRole`` is used. For MySQL: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 import. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: a new security group is created.\n')
    serverless_v2_max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 40, 40.5, 41, and so on. The largest value that you can use is 128 (256GB). The maximum capacity must be higher than 0.5 ACUs. Default: 2\n')
    serverless_v2_min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 8, 8.5, 9, and so on. The smallest value that you can use is 0.5. Default: 0.5\n')
    snapshot_credentials: typing.Optional[models.aws_rds.SnapshotCredentialsDef] = pydantic.Field(None, description='Master user credentials. Note - It is not possible to change the master username for a snapshot; however, it is possible to provide (or generate) a new password. Default: - The existing username and password from the snapshot will be used.\n')
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable storage encryption. Default: - true if storageEncryptionKey is provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key for storage encryption. If specified, ``storageEncrypted`` will be set to ``true``. Default: - if storageEncrypted is true then the default master key, no key otherwise\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.DBClusterStorageType] = pydantic.Field(None, description='The storage type to be associated with the DB cluster. Default: - DBClusterStorageType.AURORA_IOPT1\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group will be created.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='What subnets to run the RDS instances in. Must be at least 2 subnets in two different AZs.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. Default: - the Vpc default strategy if not specified.\n')
    writer: typing.Optional[typing.Union[models.aws_rds.ClusterInstanceDef]] = pydantic.Field(None, description='The instance to use for the cluster writer. Default: required if instanceProps is not provided')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'snapshot_identifier', 'backtrack_window', 'backup', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'iam_authentication', 'instance_identifier_base', 'instance_props', 'instances', 'instance_update_behaviour', 'monitoring_interval', 'monitoring_role', 'network_type', 'parameter_group', 'parameters', 'port', 'preferred_maintenance_window', 'readers', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'serverless_v2_max_capacity', 'serverless_v2_min_capacity', 'snapshot_credentials', 'storage_encrypted', 'storage_encryption_key', 'storage_type', 'subnet_group', 'vpc', 'vpc_subnets', 'writer']
    _method_names: typing.ClassVar[list[str]] = ['add_proxy', 'add_rotation_multi_user', 'add_rotation_single_user', 'apply_removal_policy', 'metric', 'metric_acu_utilization', 'metric_cpu_utilization', 'metric_database_connections', 'metric_deadlocks', 'metric_engine_uptime', 'metric_free_local_storage', 'metric_freeable_memory', 'metric_network_receive_throughput', 'metric_network_throughput', 'metric_network_transmit_throughput', 'metric_serverless_database_capacity', 'metric_snapshot_storage_used', 'metric_total_backup_storage_billed', 'metric_volume_bytes_used', 'metric_volume_read_io_ps', 'metric_volume_write_io_ps']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseClusterFromSnapshot'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseClusterFromSnapshotDefConfig] = pydantic.Field(None)


class DatabaseClusterFromSnapshotDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[DatabaseClusterFromSnapshotDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this cluster.')
    add_rotation_multi_user: typing.Optional[list[DatabaseClusterFromSnapshotDefAddRotationMultiUserParams]] = pydantic.Field(None, description='Adds the multi user rotation to this cluster.\nSee `Alternating users rotation strategy <https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets_strategies.html#rotating-secrets-two-users>`_')
    add_rotation_single_user: typing.Optional[list[DatabaseClusterFromSnapshotDefAddRotationSingleUserParams]] = pydantic.Field(None, description='Adds the single user rotation of the master password to this cluster.\nSee `Single user rotation strategy <https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets_strategies.html#rotating-secrets-one-user-one-password>`_')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    metric: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBCluster.')
    metric_acu_utilization: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricAcuUtilizationParams]] = pydantic.Field(None, description="This value is represented as a percentage.\nIt's calculated as the value of the\nServerlessDatabaseCapacity metric divided by the maximum ACU value of the DB cluster.\n\nIf this metric approaches a value of 100.0, the DB instance has scaled up as high as it can.\nConsider increasing the maximum ACU setting for the cluster.")
    metric_cpu_utilization: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_deadlocks: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricDeadlocksParams]] = pydantic.Field(None, description='The average number of deadlocks in the database per second.\nAverage over 5 minutes')
    metric_engine_uptime: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricEngineUptimeParams]] = pydantic.Field(None, description='The amount of time that the instance has been running, in seconds.\nAverage over 5 minutes')
    metric_free_local_storage: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricFreeLocalStorageParams]] = pydantic.Field(None, description='The amount of local storage available, in bytes.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory, in bytes.\nAverage over 5 minutes')
    metric_network_receive_throughput: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricNetworkReceiveThroughputParams]] = pydantic.Field(None, description='The amount of network throughput received from clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_throughput: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricNetworkThroughputParams]] = pydantic.Field(None, description='The amount of network throughput both received from and transmitted to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_network_transmit_throughput: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricNetworkTransmitThroughputParams]] = pydantic.Field(None, description='The amount of network throughput sent to clients by each instance, in bytes per second.\nAverage over 5 minutes')
    metric_serverless_database_capacity: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricServerlessDatabaseCapacityParams]] = pydantic.Field(None, description='As a cluster-level metric, it represents the average of the ServerlessDatabaseCapacity values of all the Aurora Serverless v2 DB instances in the cluster.')
    metric_snapshot_storage_used: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricSnapshotStorageUsedParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes consumed by all Aurora snapshots outside its backup retention window.\nAverage over 5 minutes')
    metric_total_backup_storage_billed: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricTotalBackupStorageBilledParams]] = pydantic.Field(None, description='The total amount of backup storage in bytes for which you are billed.\nAverage over 5 minutes')
    metric_volume_bytes_used: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricVolumeBytesUsedParams]] = pydantic.Field(None, description='The amount of storage used by your Aurora DB instance, in bytes.\nAverage over 5 minutes')
    metric_volume_read_io_ps: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricVolumeReadIoPsParams]] = pydantic.Field(None, description='The number of billed read I/O operations from a cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')
    metric_volume_write_io_ps: typing.Optional[list[DatabaseClusterFromSnapshotDefMetricVolumeWriteIoPsParams]] = pydantic.Field(None, description='The number of write disk I/O operations to the cluster volume, reported at 5-minute intervals.\nAverage over 5 minutes')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    multi_user_rotation_application_config: typing.Optional[models.aws_secretsmanager.SecretRotationApplicationDefConfig] = pydantic.Field(None)
    single_user_rotation_application_config: typing.Optional[models.aws_secretsmanager.SecretRotationApplicationDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)

class DatabaseClusterFromSnapshotDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefAddRotationMultiUserParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret to rotate. It must be a JSON string with the following format:: { "engine": <required: database engine>, "host": <required: instance host name>, "username": <required: username>, "password": <required: password>, "dbname": <optional: database name>, "port": <optional: if not specified, default port will be used>, "masterarn": <required: the arn of the master secret which will be used to create users/change passwords> }\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseClusterFromSnapshotDefAddRotationSingleUserParams(pydantic.BaseModel):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseClusterFromSnapshotDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseClusterFromSnapshotDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricAcuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricDeadlocksParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricEngineUptimeParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricFreeLocalStorageParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricNetworkReceiveThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricNetworkThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricNetworkTransmitThroughputParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricServerlessDatabaseCapacityParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricSnapshotStorageUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricTotalBackupStorageBilledParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricVolumeBytesUsedParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricVolumeReadIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseClusterFromSnapshotDefMetricVolumeWriteIoPsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseInstance
class DatabaseInstanceDef(BaseConstruct):
    character_set_name: typing.Optional[str] = pydantic.Field(None, description='For supported engines, specifies the character set to associate with the DB instance. Default: - RDS default character set name\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="Credentials for the administrative user. Default: - A username of 'admin' (or 'postgres' for PostgreSQL) and SecretsManager-generated password\n")
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is encrypted. Default: - true if storageEncryptionKey has been provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The KMS key that's used to encrypt the DB instance. Default: - default master key if storageEncrypted is true, no key otherwise\n")
    engine: typing.Union[models.aws_rds.DatabaseInstanceEngineDef] = pydantic.Field(..., description='The database engine.\n')
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The allocated storage size, specified in gibibytes (GiB). Default: 100\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow major version upgrades. Default: false\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of the database. Default: - no name\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The name of the compute and memory capacity for the instance. Default: - m5.large (or, more specifically, db.m5.large)\n')
    license_model: typing.Optional[aws_cdk.aws_rds.LicenseModel] = pydantic.Field(None, description='The license model. Default: - RDS default license model\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The time zone of the instance. This is currently supported only by Microsoft Sql Server. Default: - RDS default timezone\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets')
    _init_params: typing.ClassVar[list[str]] = ['character_set_name', 'credentials', 'storage_encrypted', 'storage_encryption_key', 'engine', 'allocated_storage', 'allow_major_version_upgrade', 'database_name', 'instance_type', 'license_model', 'parameters', 'timezone', 'vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = ['add_proxy', 'add_rotation_multi_user', 'add_rotation_single_user', 'apply_removal_policy', 'grant_connect', 'metric', 'metric_cpu_utilization', 'metric_database_connections', 'metric_free_storage_space', 'metric_freeable_memory', 'metric_read_iops', 'metric_write_iops', 'on_event']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstance'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    ...


    from_database_instance_attributes: typing.Optional[DatabaseInstanceDefFromDatabaseInstanceAttributesParams] = pydantic.Field(None, description='Import an existing database instance.')
    resource_config: typing.Optional[DatabaseInstanceDefConfig] = pydantic.Field(None)


class DatabaseInstanceDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[DatabaseInstanceDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this instance.')
    add_rotation_multi_user: typing.Optional[list[DatabaseInstanceDefAddRotationMultiUserParams]] = pydantic.Field(None, description='Adds the multi user rotation to this instance.')
    add_rotation_single_user: typing.Optional[list[DatabaseInstanceDefAddRotationSingleUserParams]] = pydantic.Field(None, description='Adds the single user rotation of the master password to this instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_connect: typing.Optional[list[DatabaseInstanceDefGrantConnectParams]] = pydantic.Field(None, description='Grant the given identity connection access to the database.')
    metric: typing.Optional[list[DatabaseInstanceDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBInstance.')
    metric_cpu_utilization: typing.Optional[list[DatabaseInstanceDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[DatabaseInstanceDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_free_storage_space: typing.Optional[list[DatabaseInstanceDefMetricFreeStorageSpaceParams]] = pydantic.Field(None, description='The amount of available storage space.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[DatabaseInstanceDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory.\nAverage over 5 minutes')
    metric_read_iops: typing.Optional[list[DatabaseInstanceDefMetricReadIopsParams]] = pydantic.Field(None, description='The average number of disk write I/O operations per second.\nAverage over 5 minutes')
    metric_write_iops: typing.Optional[list[DatabaseInstanceDefMetricWriteIopsParams]] = pydantic.Field(None, description='The average number of disk read I/O operations per second.\nAverage over 5 minutes')
    on_event: typing.Optional[list[DatabaseInstanceDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers for instance events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)

class DatabaseInstanceDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefAddRotationMultiUserParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret to rotate. It must be a JSON string with the following format:: { "engine": <required: database engine>, "host": <required: instance host name>, "username": <required: username>, "password": <required: password>, "dbname": <optional: database name>, "port": <optional: if not specified, default port will be used>, "masterarn": <required: the arn of the master secret which will be used to create users/change passwords> }\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseInstanceDefAddRotationSingleUserParams(pydantic.BaseModel):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseInstanceDefFromDatabaseInstanceAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    instance_endpoint_address: str = pydantic.Field(..., description='The endpoint address.\n')
    instance_identifier: str = pydantic.Field(..., description='The instance identifier.\n')
    port: typing.Union[int, float] = pydantic.Field(..., description='The database port.\n')
    security_groups: typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(..., description='The security groups of the instance.\n')
    engine: typing.Optional[typing.Union[models.aws_rds.DatabaseInstanceEngineDef]] = pydantic.Field(None, description="The engine of the existing database Instance. Default: - the imported Instance's engine is unknown\n")
    instance_resource_id: typing.Optional[str] = pydantic.Field(None, description='The AWS Region-unique, immutable identifier for the DB instance. This identifier is found in AWS CloudTrail log entries whenever the AWS KMS key for the DB instance is accessed.')
    ...

class DatabaseInstanceDefGrantConnectParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the Principal to grant the permissions to.\n')
    db_user: typing.Optional[str] = pydantic.Field(None, description='the name of the database user to allow connecting as to the db instance.\n\n:default: the default user, obtained from the Secret\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefMetricFreeStorageSpaceParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefMetricReadIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefMetricWriteIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceFromSnapshot
class DatabaseInstanceFromSnapshotDef(BaseConstruct):
    snapshot_identifier: str = pydantic.Field(..., description="The name or Amazon Resource Name (ARN) of the DB snapshot that's used to restore the DB instance. If you're restoring from a shared manual DB snapshot, you must specify the ARN of the snapshot.\n")
    credentials: typing.Optional[models.aws_rds.SnapshotCredentialsDef] = pydantic.Field(None, description='Master user credentials. Note - It is not possible to change the master username for a snapshot; however, it is possible to provide (or generate) a new password. Default: - The existing username and password from the snapshot will be used.\n')
    engine: typing.Union[models.aws_rds.DatabaseInstanceEngineDef] = pydantic.Field(..., description='The database engine.\n')
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The allocated storage size, specified in gibibytes (GiB). Default: 100\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow major version upgrades. Default: false\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of the database. Default: - no name\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The name of the compute and memory capacity for the instance. Default: - m5.large (or, more specifically, db.m5.large)\n')
    license_model: typing.Optional[aws_cdk.aws_rds.LicenseModel] = pydantic.Field(None, description='The license model. Default: - RDS default license model\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The time zone of the instance. This is currently supported only by Microsoft Sql Server. Default: - RDS default timezone\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets')
    _init_params: typing.ClassVar[list[str]] = ['snapshot_identifier', 'credentials', 'engine', 'allocated_storage', 'allow_major_version_upgrade', 'database_name', 'instance_type', 'license_model', 'parameters', 'timezone', 'vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = ['add_proxy', 'add_rotation_multi_user', 'add_rotation_single_user', 'apply_removal_policy', 'grant_connect', 'metric', 'metric_cpu_utilization', 'metric_database_connections', 'metric_free_storage_space', 'metric_freeable_memory', 'metric_read_iops', 'metric_write_iops', 'on_event']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceFromSnapshot'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    ...


    from_database_instance_attributes: typing.Optional[DatabaseInstanceFromSnapshotDefFromDatabaseInstanceAttributesParams] = pydantic.Field(None, description='Import an existing database instance.')
    resource_config: typing.Optional[DatabaseInstanceFromSnapshotDefConfig] = pydantic.Field(None)


class DatabaseInstanceFromSnapshotDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[DatabaseInstanceFromSnapshotDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this instance.')
    add_rotation_multi_user: typing.Optional[list[DatabaseInstanceFromSnapshotDefAddRotationMultiUserParams]] = pydantic.Field(None, description='Adds the multi user rotation to this instance.')
    add_rotation_single_user: typing.Optional[list[DatabaseInstanceFromSnapshotDefAddRotationSingleUserParams]] = pydantic.Field(None, description='Adds the single user rotation of the master password to this instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_connect: typing.Optional[list[DatabaseInstanceFromSnapshotDefGrantConnectParams]] = pydantic.Field(None, description='Grant the given identity connection access to the database.')
    metric: typing.Optional[list[DatabaseInstanceFromSnapshotDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBInstance.')
    metric_cpu_utilization: typing.Optional[list[DatabaseInstanceFromSnapshotDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[DatabaseInstanceFromSnapshotDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_free_storage_space: typing.Optional[list[DatabaseInstanceFromSnapshotDefMetricFreeStorageSpaceParams]] = pydantic.Field(None, description='The amount of available storage space.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[DatabaseInstanceFromSnapshotDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory.\nAverage over 5 minutes')
    metric_read_iops: typing.Optional[list[DatabaseInstanceFromSnapshotDefMetricReadIopsParams]] = pydantic.Field(None, description='The average number of disk write I/O operations per second.\nAverage over 5 minutes')
    metric_write_iops: typing.Optional[list[DatabaseInstanceFromSnapshotDefMetricWriteIopsParams]] = pydantic.Field(None, description='The average number of disk read I/O operations per second.\nAverage over 5 minutes')
    on_event: typing.Optional[list[DatabaseInstanceFromSnapshotDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers for instance events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)

class DatabaseInstanceFromSnapshotDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefAddRotationMultiUserParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret to rotate. It must be a JSON string with the following format:: { "engine": <required: database engine>, "host": <required: instance host name>, "username": <required: username>, "password": <required: password>, "dbname": <optional: database name>, "port": <optional: if not specified, default port will be used>, "masterarn": <required: the arn of the master secret which will be used to create users/change passwords> }\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseInstanceFromSnapshotDefAddRotationSingleUserParams(pydantic.BaseModel):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class DatabaseInstanceFromSnapshotDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseInstanceFromSnapshotDefFromDatabaseInstanceAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    instance_endpoint_address: str = pydantic.Field(..., description='The endpoint address.\n')
    instance_identifier: str = pydantic.Field(..., description='The instance identifier.\n')
    port: typing.Union[int, float] = pydantic.Field(..., description='The database port.\n')
    security_groups: typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(..., description='The security groups of the instance.\n')
    engine: typing.Optional[typing.Union[models.aws_rds.DatabaseInstanceEngineDef]] = pydantic.Field(None, description="The engine of the existing database Instance. Default: - the imported Instance's engine is unknown\n")
    instance_resource_id: typing.Optional[str] = pydantic.Field(None, description='The AWS Region-unique, immutable identifier for the DB instance. This identifier is found in AWS CloudTrail log entries whenever the AWS KMS key for the DB instance is accessed.')
    ...

class DatabaseInstanceFromSnapshotDefGrantConnectParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='the Principal to grant the permissions to.\n')
    db_user: typing.Optional[str] = pydantic.Field(None, description='the name of the database user to allow connecting as to the db instance.\n\n:default: the default user, obtained from the Secret\n')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefMetricFreeStorageSpaceParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefMetricReadIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefMetricWriteIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceFromSnapshotDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceReadReplica
class DatabaseInstanceReadReplicaDef(BaseConstruct):
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='The name of the compute and memory capacity classes.\n')
    source_database_instance: typing.Union[models.aws_rds.DatabaseInstanceBaseDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceReadReplicaDef, models.aws_rds.DatabaseInstanceReadReplicaDef] = pydantic.Field(..., description='The source database instance. Each DB instance can have a limited number of read replicas. For more information, see https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/USER_ReadRepl.html.\n')
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is encrypted. Default: - true if storageEncryptionKey has been provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The KMS key that's used to encrypt the DB instance. Default: - default master key if storageEncrypted is true, no key otherwise\n")
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets')
    _init_params: typing.ClassVar[list[str]] = ['instance_type', 'source_database_instance', 'storage_encrypted', 'storage_encryption_key', 'vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = ['add_proxy', 'apply_removal_policy', 'grant_connect', 'metric', 'metric_cpu_utilization', 'metric_database_connections', 'metric_free_storage_space', 'metric_freeable_memory', 'metric_read_iops', 'metric_write_iops', 'on_event']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceReadReplica'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_database_instance_attributes']
    ...


    from_database_instance_attributes: typing.Optional[DatabaseInstanceReadReplicaDefFromDatabaseInstanceAttributesParams] = pydantic.Field(None, description='Import an existing database instance.')
    resource_config: typing.Optional[DatabaseInstanceReadReplicaDefConfig] = pydantic.Field(None)


class DatabaseInstanceReadReplicaDefConfig(pydantic.BaseModel):
    add_proxy: typing.Optional[list[DatabaseInstanceReadReplicaDefAddProxyParams]] = pydantic.Field(None, description='Add a new db proxy to this instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_connect: typing.Optional[list[DatabaseInstanceReadReplicaDefGrantConnectParams]] = pydantic.Field(None, description='Grant the given identity connection access to the database.')
    metric: typing.Optional[list[DatabaseInstanceReadReplicaDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this DBInstance.')
    metric_cpu_utilization: typing.Optional[list[DatabaseInstanceReadReplicaDefMetricCpuUtilizationParams]] = pydantic.Field(None, description='The percentage of CPU utilization.\nAverage over 5 minutes')
    metric_database_connections: typing.Optional[list[DatabaseInstanceReadReplicaDefMetricDatabaseConnectionsParams]] = pydantic.Field(None, description='The number of database connections in use.\nAverage over 5 minutes')
    metric_free_storage_space: typing.Optional[list[DatabaseInstanceReadReplicaDefMetricFreeStorageSpaceParams]] = pydantic.Field(None, description='The amount of available storage space.\nAverage over 5 minutes')
    metric_freeable_memory: typing.Optional[list[DatabaseInstanceReadReplicaDefMetricFreeableMemoryParams]] = pydantic.Field(None, description='The amount of available random access memory.\nAverage over 5 minutes')
    metric_read_iops: typing.Optional[list[DatabaseInstanceReadReplicaDefMetricReadIopsParams]] = pydantic.Field(None, description='The average number of disk write I/O operations per second.\nAverage over 5 minutes')
    metric_write_iops: typing.Optional[list[DatabaseInstanceReadReplicaDefMetricWriteIopsParams]] = pydantic.Field(None, description='The average number of disk read I/O operations per second.\nAverage over 5 minutes')
    on_event: typing.Optional[list[DatabaseInstanceReadReplicaDefOnEventParams]] = pydantic.Field(None, description='Defines a CloudWatch event rule which triggers for instance events.\nUse\n``rule.addEventPattern(pattern)`` to specify a filter.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)

class DatabaseInstanceReadReplicaDefAddProxyParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    return_config: typing.Optional[list[models.aws_rds.DatabaseProxyDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseInstanceReadReplicaDefFromDatabaseInstanceAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    instance_endpoint_address: str = pydantic.Field(..., description='The endpoint address.\n')
    instance_identifier: str = pydantic.Field(..., description='The instance identifier.\n')
    port: typing.Union[int, float] = pydantic.Field(..., description='The database port.\n')
    security_groups: typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(..., description='The security groups of the instance.\n')
    engine: typing.Optional[typing.Union[models.aws_rds.DatabaseInstanceEngineDef]] = pydantic.Field(None, description="The engine of the existing database Instance. Default: - the imported Instance's engine is unknown\n")
    instance_resource_id: typing.Optional[str] = pydantic.Field(None, description='The AWS Region-unique, immutable identifier for the DB instance. This identifier is found in AWS CloudTrail log entries whenever the AWS KMS key for the DB instance is accessed.')
    ...

class DatabaseInstanceReadReplicaDefGrantConnectParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    db_user: typing.Optional[str] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefMetricCpuUtilizationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefMetricDatabaseConnectionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefMetricFreeStorageSpaceParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefMetricFreeableMemoryParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefMetricReadIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefMetricWriteIopsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class DatabaseInstanceReadReplicaDefOnEventParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    return_config: typing.Optional[list[models.aws_events.RuleDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseProxy
class DatabaseProxyDef(BaseConstruct, ConnectableMixin):
    proxy_target: models.aws_rds.ProxyTargetDef = pydantic.Field(..., description='DB proxy target: Instance or Cluster.\n')
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.')
    _init_params: typing.ClassVar[list[str]] = ['proxy_target', 'secrets', 'vpc', 'borrow_timeout', 'db_proxy_name', 'debug_logging', 'iam_auth', 'idle_client_timeout', 'init_query', 'max_connections_percent', 'max_idle_connections_percent', 'require_tls', 'role', 'security_groups', 'session_pinning_filters', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy', 'grant_connect']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_database_proxy_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseProxy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_database_proxy_attributes']
    ...


    from_database_proxy_attributes: typing.Optional[DatabaseProxyDefFromDatabaseProxyAttributesParams] = pydantic.Field(None, description='Import an existing database proxy.')
    resource_config: typing.Optional[DatabaseProxyDefConfig] = pydantic.Field(None)


class DatabaseProxyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_connect: typing.Optional[list[DatabaseProxyDefGrantConnectParams]] = pydantic.Field(None, description='Grant the given identity connection access to the proxy.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class DatabaseProxyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseProxyDefFromDatabaseProxyAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    db_proxy_arn: str = pydantic.Field(..., description='DB Proxy ARN.\n')
    db_proxy_name: str = pydantic.Field(..., description='DB Proxy Name.\n')
    endpoint: str = pydantic.Field(..., description='Endpoint.\n')
    security_groups: typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(..., description='The security groups of the instance.')
    ...

class DatabaseProxyDefGrantConnectParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    db_user: typing.Optional[str] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.DatabaseSecret
class DatabaseSecretDef(BaseConstruct):
    username: str = pydantic.Field(..., description='The username.\n')
    dbname: typing.Optional[str] = pydantic.Field(None, description='The database name, if not using the default one. Default: - whatever the secret generates after the attach method is run\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key to use to encrypt the secret. Default: default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Characters to not include in the generated password. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    master_secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The master secret which will be used to rotate this secret. Default: - no master secret information will be included\n')
    replace_on_password_criteria_changes: typing.Optional[bool] = pydantic.Field(None, description='Whether to replace this secret when the criteria for the password change. This is achieved by overriding the logical id of the AWS::SecretsManager::Secret with a hash of the options that influence the password generation. This way a new secret will be created when the password is regenerated and the cluster or instance consuming this secret will have its credentials updated. Default: false\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated\n')
    secret_name: typing.Optional[str] = pydantic.Field(None, description='A name for the secret. Default: - A name is generated by CloudFormation.')
    _init_params: typing.ClassVar[list[str]] = ['username', 'dbname', 'encryption_key', 'exclude_characters', 'master_secret', 'replace_on_password_criteria_changes', 'replica_regions', 'secret_name']
    _method_names: typing.ClassVar[list[str]] = ['add_replica_region', 'add_rotation_schedule', 'add_to_resource_policy', 'apply_removal_policy', 'attach', 'deny_account_root_delete', 'grant_read', 'grant_write', 'secret_value_from_json']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_secret_attributes', 'from_secret_complete_arn', 'from_secret_name_v2', 'from_secret_partial_arn']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseSecret'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_secret_attributes', 'from_secret_complete_arn', 'from_secret_name_v2', 'from_secret_partial_arn']
    ...


    from_secret_attributes: typing.Optional[DatabaseSecretDefFromSecretAttributesParams] = pydantic.Field(None, description='Import an existing secret into the Stack.')
    from_secret_complete_arn: typing.Optional[DatabaseSecretDefFromSecretCompleteArnParams] = pydantic.Field(None, description='Imports a secret by complete ARN.\nThe complete ARN is the ARN with the Secrets Manager-supplied suffix.')
    from_secret_name_v2: typing.Optional[DatabaseSecretDefFromSecretNameV2Params] = pydantic.Field(None, description='Imports a secret by secret name.\nA secret with this name must exist in the same account & region.\nReplaces the deprecated ``fromSecretName``.\nPlease note this method returns ISecret that only contains partial ARN and could lead to AccessDeniedException\nwhen you pass the partial ARN to CLI or SDK to get the secret value. If your secret name ends with a hyphen and\n6 characters, you should always use fromSecretCompleteArn() to avoid potential AccessDeniedException.')
    from_secret_partial_arn: typing.Optional[DatabaseSecretDefFromSecretPartialArnParams] = pydantic.Field(None, description='Imports a secret by partial ARN.\nThe partial ARN is the ARN without the Secrets Manager-supplied suffix.')
    resource_config: typing.Optional[DatabaseSecretDefConfig] = pydantic.Field(None)


class DatabaseSecretDefConfig(pydantic.BaseModel):
    add_replica_region: typing.Optional[list[DatabaseSecretDefAddReplicaRegionParams]] = pydantic.Field(None, description='Adds a replica region for the secret.')
    add_rotation_schedule: typing.Optional[list[DatabaseSecretDefAddRotationScheduleParams]] = pydantic.Field(None, description='Adds a rotation schedule to the secret.')
    add_to_resource_policy: typing.Optional[list[DatabaseSecretDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM resource policy associated with this secret.\nIf this secret was created in this stack, a resource policy will be\nautomatically created upon the first call to ``addToResourcePolicy``. If\nthe secret is imported, then this is a no-op.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    attach: typing.Optional[list[DatabaseSecretDefAttachParams]] = pydantic.Field(None, description='Attach a target to this secret.')
    deny_account_root_delete: typing.Optional[bool] = pydantic.Field(None, description='Denies the ``DeleteSecret`` action to all principals within the current account.')
    grant_read: typing.Optional[list[DatabaseSecretDefGrantReadParams]] = pydantic.Field(None, description='Grants reading the secret value to some role.')
    grant_write: typing.Optional[list[DatabaseSecretDefGrantWriteParams]] = pydantic.Field(None, description='Grants writing and updating the secret value to some role.')
    secret_value_from_json: typing.Optional[list[DatabaseSecretDefSecretValueFromJsonParams]] = pydantic.Field(None, description="Interpret the secret as a JSON object and return a field's value from it as a ``SecretValue``.")
    secret_value_config: typing.Optional[models.core.SecretValueDefConfig] = pydantic.Field(None)

class DatabaseSecretDefAddReplicaRegionParams(pydantic.BaseModel):
    region: str = pydantic.Field(..., description='The name of the region.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The customer-managed encryption key to use for encrypting the secret value.')
    ...

class DatabaseSecretDefAddRotationScheduleParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. A value of zero will disable automatic rotation - ``Duration.days(0)``. Default: Duration.days(30)\n')
    hosted_rotation: typing.Optional[models.aws_secretsmanager.HostedRotationDef] = pydantic.Field(None, description='Hosted rotation. Default: - either ``rotationLambda`` or ``hostedRotation`` must be specified\n')
    rotate_immediately_on_update: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether to rotate the secret immediately or wait until the next scheduled rotation window. Default: - secret is rotated immediately\n')
    rotation_lambda: typing.Optional[typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef]] = pydantic.Field(None, description='A Lambda function that can rotate the secret. Default: - either ``rotationLambda`` or ``hostedRotation`` must be specified')
    return_config: typing.Optional[list[models.aws_secretsmanager.RotationScheduleDefConfig]] = pydantic.Field(None)
    ...

class DatabaseSecretDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class DatabaseSecretDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class DatabaseSecretDefAttachParams(pydantic.BaseModel):
    target: typing.Union[models.aws_rds.DatabaseProxyDef] = pydantic.Field(..., description='The target to attach.\n')
    return_config: typing.Optional[list[models._interface_methods.AwsSecretsmanagerISecretDefConfig]] = pydantic.Field(None)
    ...

class DatabaseSecretDefFromSecretAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='the scope of the import.\n')
    id: str = pydantic.Field(..., description='the ID of the imported Secret in the construct tree.\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The encryption key that is used to encrypt the secret, unless the default SecretsManager key is used.\n')
    secret_complete_arn: typing.Optional[str] = pydantic.Field(None, description='The complete ARN of the secret in SecretsManager. This is the ARN including the Secrets Manager 6-character suffix. Cannot be used with ``secretArn`` or ``secretPartialArn``.\n')
    secret_partial_arn: typing.Optional[str] = pydantic.Field(None, description='The partial ARN of the secret in SecretsManager. This is the ARN without the Secrets Manager 6-character suffix. Cannot be used with ``secretArn`` or ``secretCompleteArn``.')
    ...

class DatabaseSecretDefFromSecretCompleteArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    secret_complete_arn: str = pydantic.Field(..., description='-')
    ...

class DatabaseSecretDefFromSecretNameV2Params(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    secret_name: str = pydantic.Field(..., description='-\n\n:see: https://docs.aws.amazon.com/secretsmanager/latest/userguide/troubleshoot.html#ARN_secretnamehyphen\n')
    ...

class DatabaseSecretDefFromSecretPartialArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    secret_partial_arn: str = pydantic.Field(..., description='-')
    ...

class DatabaseSecretDefGrantReadParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-\n')
    version_stages: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DatabaseSecretDefGrantWriteParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class DatabaseSecretDefSecretValueFromJsonParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.core.SecretValueDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.OptionGroup
class OptionGroupDef(BaseConstruct):
    configurations: typing.Sequence[typing.Union[models.aws_rds.OptionConfigurationDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The configurations for this option group.\n')
    engine: typing.Union[models.aws_rds.DatabaseInstanceEngineDef] = pydantic.Field(..., description='The database engine that this option group is associated with.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the option group. Default: a CDK generated description')
    _init_params: typing.ClassVar[list[str]] = ['configurations', 'engine', 'description']
    _method_names: typing.ClassVar[list[str]] = ['add_configuration', 'apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_option_group_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OptionGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_option_group_name']
    ...


    from_option_group_name: typing.Optional[OptionGroupDefFromOptionGroupNameParams] = pydantic.Field(None, description='Import an existing option group.')
    resource_config: typing.Optional[OptionGroupDefConfig] = pydantic.Field(None)


class OptionGroupDefConfig(pydantic.BaseModel):
    add_configuration: typing.Optional[list[OptionGroupDefAddConfigurationParams]] = pydantic.Field(None, description='Adds a configuration to this OptionGroup.\nThis method is a no-op for an imported OptionGroup.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class OptionGroupDefAddConfigurationParams(pydantic.BaseModel):
    name: str = pydantic.Field(..., description='The name of the option.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number that this option uses. If ``port`` is specified then ``vpc`` must also be specified. Default: - no port\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Optional list of security groups to use for this option, if ``vpc`` is specified. If no groups are provided, a default one will be created. Default: - a default group will be created if ``port`` or ``vpc`` are specified.\n')
    settings: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The settings for the option. Default: - no settings\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version for the option. Default: - no version\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where a security group should be created for this option. If ``vpc`` is specified then ``port`` must also be specified. Default: - no VPC')
    ...

class OptionGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class OptionGroupDefFromOptionGroupNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    option_group_name: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_rds.ParameterGroup
class ParameterGroupDef(BaseConstruct):
    engine: models.UnsupportedResource = pydantic.Field(..., description='The database engine for this parameter group.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for this parameter group. Default: a CDK generated description\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in this parameter group. Default: - None')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'description', 'parameters']
    _method_names: typing.ClassVar[list[str]] = ['add_parameter', 'apply_removal_policy', 'bind_to_cluster', 'bind_to_instance']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_parameter_group_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ParameterGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_parameter_group_name']
    ...


    from_parameter_group_name: typing.Optional[ParameterGroupDefFromParameterGroupNameParams] = pydantic.Field(None, description='Imports a parameter group.')
    resource_config: typing.Optional[ParameterGroupDefConfig] = pydantic.Field(None)


class ParameterGroupDefConfig(pydantic.BaseModel):
    add_parameter: typing.Optional[list[ParameterGroupDefAddParameterParams]] = pydantic.Field(None, description='Add a parameter to this parameter group.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    bind_to_cluster: typing.Optional[list[ParameterGroupDefBindToClusterParams]] = pydantic.Field(None, description='Method called when this Parameter Group is used when defining a database cluster.')
    bind_to_instance: typing.Optional[list[ParameterGroupDefBindToInstanceParams]] = pydantic.Field(None, description='Method called when this Parameter Group is used when defining a database instance.')

class ParameterGroupDefAddParameterParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='The key of the parameter to be added.\n')
    value: str = pydantic.Field(..., description='The value of the parameter to be added.')
    ...

class ParameterGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ParameterGroupDefBindToClusterParams(pydantic.BaseModel):
    ...

class ParameterGroupDefBindToInstanceParams(pydantic.BaseModel):
    ...

class ParameterGroupDefFromParameterGroupNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    parameter_group_name: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_rds.ServerlessCluster
class ServerlessClusterDef(BaseConstruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Automatic backup retention cannot be disabled on serverless clusters. Must be a value from 1 day to 35 days. Default: Duration.days(1)\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="Credentials for the administrative user. Default: - A username of 'admin' and SecretsManager-generated password\n")
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if removalPolicy is RETAIN, false otherwise\n')
    enable_data_api: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the Data API. Default: false\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - no parameter group.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    scaling: typing.Union[models.aws_rds.ServerlessScalingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Scaling configuration of an Aurora Serverless database cluster. Default: - Serverless cluster is automatically paused after 5 minutes of being idle. minimum capacity: 2 ACU maximum capacity: 16 ACU\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: - a new security group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no VPC security groups will be associated with the DB cluster.\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key for storage encryption. Default: - the default master key will be used for storage encryption\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no subnet group will be associated with the DB cluster\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC that this Aurora Serverless cluster has been created in. Default: - the default VPC in the account and region will be used\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. If provided, the ``vpc`` property must also be specified. Default: - the VPC default strategy if not specified.')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'backup_retention', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'enable_data_api', 'parameter_group', 'removal_policy', 'scaling', 'security_groups', 'storage_encryption_key', 'subnet_group', 'vpc', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = ['add_rotation_multi_user', 'add_rotation_single_user', 'apply_removal_policy', 'grant_data_api_access']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_serverless_cluster_attributes']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ServerlessCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_serverless_cluster_attributes']
    ...


    from_serverless_cluster_attributes: typing.Optional[ServerlessClusterDefFromServerlessClusterAttributesParams] = pydantic.Field(None, description='Import an existing DatabaseCluster from properties.')
    resource_config: typing.Optional[ServerlessClusterDefConfig] = pydantic.Field(None)


class ServerlessClusterDefConfig(pydantic.BaseModel):
    add_rotation_multi_user: typing.Optional[list[ServerlessClusterDefAddRotationMultiUserParams]] = pydantic.Field(None, description='Adds the multi user rotation to this cluster.')
    add_rotation_single_user: typing.Optional[list[ServerlessClusterDefAddRotationSingleUserParams]] = pydantic.Field(None, description='Adds the single user rotation of the master password to this cluster.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_data_api_access: typing.Optional[list[ServerlessClusterDefGrantDataApiAccessParams]] = pydantic.Field(None, description='Grant the given identity to access to the Data API, including read access to the secret attached to the cluster if present.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class ServerlessClusterDefAddRotationMultiUserParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret to rotate. It must be a JSON string with the following format:: { "engine": <required: database engine>, "host": <required: instance host name>, "username": <required: username>, "password": <required: password>, "dbname": <optional: database name>, "port": <optional: if not specified, default port will be used>, "masterarn": <required: the arn of the master secret which will be used to create users/change passwords> }\n')
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class ServerlessClusterDefAddRotationSingleUserParams(pydantic.BaseModel):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster')
    ...

class ServerlessClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ServerlessClusterDefFromServerlessClusterAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    cluster_identifier: str = pydantic.Field(..., description='Identifier for the cluster.\n')
    cluster_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Cluster endpoint address. Default: - no endpoint address\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The database port. Default: - none\n')
    reader_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Reader endpoint address. Default: - no reader address\n')
    secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret attached to the database cluster. Default: - no secret\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups of the database cluster. Default: - no security groups')
    ...

class ServerlessClusterDefGrantDataApiAccessParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.ServerlessClusterFromSnapshot
class ServerlessClusterFromSnapshotDef(BaseConstruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    snapshot_identifier: str = pydantic.Field(..., description='The identifier for the DB instance snapshot or DB cluster snapshot to restore from. You can use either the name or the Amazon Resource Name (ARN) to specify a DB cluster snapshot. However, you can use only the ARN to specify a DB instance snapshot.\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Automatic backup retention cannot be disabled on serverless clusters. Must be a value from 1 day to 35 days. Default: Duration.days(1)\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.SnapshotCredentialsDef] = pydantic.Field(None, description='Master user credentials. Note - It is not possible to change the master username for a snapshot; however, it is possible to provide (or generate) a new password. Default: - The existing username and password from the snapshot will be used.\n')
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if removalPolicy is RETAIN, false otherwise\n')
    enable_data_api: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the Data API. Default: false\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - no parameter group.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    scaling: typing.Union[models.aws_rds.ServerlessScalingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Scaling configuration of an Aurora Serverless database cluster. Default: - Serverless cluster is automatically paused after 5 minutes of being idle. minimum capacity: 2 ACU maximum capacity: 16 ACU\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: - a new security group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no VPC security groups will be associated with the DB cluster.\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no subnet group will be associated with the DB cluster\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC that this Aurora Serverless cluster has been created in. Default: - the default VPC in the account and region will be used\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. If provided, the ``vpc`` property must also be specified. Default: - the VPC default strategy if not specified.')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'snapshot_identifier', 'backup_retention', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'enable_data_api', 'parameter_group', 'removal_policy', 'scaling', 'security_groups', 'subnet_group', 'vpc', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy', 'grant_data_api_access']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ServerlessClusterFromSnapshot'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[ServerlessClusterFromSnapshotDefConfig] = pydantic.Field(None)


class ServerlessClusterFromSnapshotDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    grant_data_api_access: typing.Optional[list[ServerlessClusterFromSnapshotDefGrantDataApiAccessParams]] = pydantic.Field(None, description='Grant the given identity to access to the Data API, including read access to the secret attached to the cluster if present.')
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)

class ServerlessClusterFromSnapshotDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class ServerlessClusterFromSnapshotDefGrantDataApiAccessParams(pydantic.BaseModel):
    grantee: typing.Union[models.aws_appsync.BackedDataSourceDef, models.aws_appsync.DynamoDbDataSourceDef, models.aws_appsync.ElasticsearchDataSourceDef, models.aws_appsync.EventBridgeDataSourceDef, models.aws_appsync.HttpDataSourceDef, models.aws_appsync.LambdaDataSourceDef, models.aws_appsync.OpenSearchDataSourceDef, models.aws_appsync.RdsDataSourceDef, models.aws_backup.BackupSelectionDef, models.aws_codebuild.UntrustedCodeBoundaryPolicyDef, models.aws_ec2.LaunchTemplateDef, models.aws_iam.ManagedPolicyDef, models.aws_iam.PolicyDef, models.aws_stepfunctions_tasks.EmrContainersStartJobRunDef, models.aws_stepfunctions_tasks.SageMakerCreateModelDef, models.aws_stepfunctions_tasks.SageMakerCreateTrainingJobDef, models.custom_resources.AwsCustomResourceDef] = pydantic.Field(..., description='The principal to grant access to.')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_rds.SubnetGroup
class SubnetGroupDef(BaseConstruct):
    description: str = pydantic.Field(..., description='Description of the subnet group.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to place the subnet group in.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the subnet group are removed from the stack or replaced during an update. Default: RemovalPolicy.DESTROY\n')
    subnet_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subnet group. Default: - a name is generated\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Which subnets within the VPC to associate with this group. Default: - private subnets')
    _init_params: typing.ClassVar[list[str]] = ['description', 'vpc', 'removal_policy', 'subnet_group_name', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_subnet_group_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SubnetGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_subnet_group_name']
    ...


    from_subnet_group_name: typing.Optional[SubnetGroupDefFromSubnetGroupNameParams] = pydantic.Field(None, description='Imports an existing subnet group by name.')
    resource_config: typing.Optional[SubnetGroupDefConfig] = pydantic.Field(None)


class SubnetGroupDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class SubnetGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class SubnetGroupDefFromSubnetGroupNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    subnet_group_name: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_rds.AuroraClusterEngineProps
class AuroraClusterEnginePropsDef(BaseStruct):
    version: models.aws_rds.AuroraEngineVersionDef = pydantic.Field(..., description='The version of the Aurora cluster engine.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    rds.DatabaseClusterFromSnapshot(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora(version=rds.AuroraEngineVersion.VER_1_22_2),\n        writer=rds.ClusterInstance.provisioned("writer"),\n        vpc=vpc,\n        snapshot_identifier="mySnapshot"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.AuroraClusterEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.AuroraMysqlClusterEngineProps
class AuroraMysqlClusterEnginePropsDef(BaseStruct):
    version: models.aws_rds.AuroraMysqlEngineVersionDef = pydantic.Field(..., description='The version of the Aurora MySQL cluster engine.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    # vpc: ec2.Vpc\n\n    cluster = rds.DatabaseCluster(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora_mysql(version=rds.AuroraMysqlEngineVersion.VER_2_08_1),\n        writer=rds.ClusterInstance.provisioned("writer",\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.R6G, ec2.InstanceSize.XLARGE4)\n        ),\n        serverless_v2_min_capacity=6.5,\n        serverless_v2_max_capacity=64,\n        readers=[\n            # will be put in promotion tier 1 and will scale with the writer\n            rds.ClusterInstance.serverless_v2("reader1", scale_with_writer=True),\n            # will be put in promotion tier 2 and will not scale with the writer\n            rds.ClusterInstance.serverless_v2("reader2")\n        ],\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.AuroraMysqlClusterEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.AuroraPostgresClusterEngineProps
class AuroraPostgresClusterEnginePropsDef(BaseStruct):
    version: models.aws_rds.AuroraPostgresEngineVersionDef = pydantic.Field(..., description='The version of the Aurora PostgreSQL cluster engine.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    # vpc: ec2.Vpc\n\n    cluster = rds.DatabaseCluster(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora_postgres(version=rds.AuroraPostgresEngineVersion.VER_15_2),\n        credentials=rds.Credentials.from_username("adminuser", password=cdk.SecretValue.unsafe_plain_text("7959866cacc02c2d243ecfe177464fe6")),\n        instance_props=rds.InstanceProps(\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.X2G, ec2.InstanceSize.XLARGE),\n            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC),\n            vpc=vpc\n        ),\n        storage_type=rds.DBClusterStorageType.AURORA_IOPT1\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.AuroraPostgresClusterEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.AuroraPostgresEngineFeatures
class AuroraPostgresEngineFeaturesDef(BaseStruct):
    s3_export: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Aurora Postgres cluster engine supports the S3 data export feature. Default: false\n')
    s3_import: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Aurora Postgres cluster engine supports the S3 data import feature. Default: false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    aurora_postgres_engine_features = rds.AuroraPostgresEngineFeatures(\n        s3_export=False,\n        s3_import=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_export', 's3_import']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.AuroraPostgresEngineFeatures'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.BackupProps
class BackupPropsDef(BaseStruct):
    retention: models.DurationDef = pydantic.Field(..., description='How many days to retain the backup.\n')
    preferred_window: typing.Optional[str] = pydantic.Field(None, description='A daily time range in 24-hours UTC format in which backups preferably execute. Must be at least 30 minutes long. Example: \'01:00-02:00\' Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n\n:default:\n\n- The retention period for automated backups is 1 day.\nThe preferred backup window will be a 30-minute window selected at random\nfrom an 8-hour block of time for each AWS Region.\n\n:see: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_rds as rds\n\n    backup_props = rds.BackupProps(\n        retention=cdk.Duration.minutes(30),\n\n        # the properties below are optional\n        preferred_window="preferredWindow"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['retention', 'preferred_window']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.BackupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[BackupPropsDefConfig] = pydantic.Field(None)


class BackupPropsDefConfig(pydantic.BaseModel):
    retention_config: typing.Optional[models.core.DurationDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.CfnDBCluster.DBClusterRoleProperty
class CfnDBCluster_DBClusterRolePropertyDef(BaseStruct):
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the IAM role that is associated with the DB cluster.\n')
    feature_name: typing.Optional[str] = pydantic.Field(None, description='The name of the feature associated with the AWS Identity and Access Management (IAM) role. IAM roles that are associated with a DB cluster grant permission for the DB cluster to access other AWS services on your behalf. For the list of supported feature names, see the ``SupportedFeatureNames`` description in `DBEngineVersion <https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_DBEngineVersion.html>`_ in the *Amazon RDS API Reference* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbcluster-dbclusterrole.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    d_bCluster_role_property = rds.CfnDBCluster.DBClusterRoleProperty(\n        role_arn="roleArn",\n\n        # the properties below are optional\n        feature_name="featureName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['role_arn', 'feature_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBCluster.DBClusterRoleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBCluster.EndpointProperty
class CfnDBCluster_EndpointPropertyDef(BaseStruct):
    address: typing.Optional[str] = pydantic.Field(None, description='Specifies the connection endpoint for the primary instance of the DB cluster.\n')
    port: typing.Optional[str] = pydantic.Field(None, description='Specifies the port that the database engine is listening on.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbcluster-endpoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    endpoint_property = rds.CfnDBCluster.EndpointProperty(\n        address="address",\n        port="port"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['address', 'port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBCluster.EndpointProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBCluster.MasterUserSecretProperty
class CfnDBCluster_MasterUserSecretPropertyDef(BaseStruct):
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The AWS KMS key identifier that is used to encrypt the secret.\n')
    secret_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the secret.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbcluster-masterusersecret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    master_user_secret_property = rds.CfnDBCluster.MasterUserSecretProperty(\n        kms_key_id="kmsKeyId",\n        secret_arn="secretArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['kms_key_id', 'secret_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBCluster.MasterUserSecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBCluster.ReadEndpointProperty
class CfnDBCluster_ReadEndpointPropertyDef(BaseStruct):
    address: typing.Optional[str] = pydantic.Field(None, description='The host address of the reader endpoint.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbcluster-readendpoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    read_endpoint_property = rds.CfnDBCluster.ReadEndpointProperty(\n        address="address"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['address']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBCluster.ReadEndpointProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBCluster.ScalingConfigurationProperty
class CfnDBCluster_ScalingConfigurationPropertyDef(BaseStruct):
    auto_pause: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to allow or disallow automatic pause for an Aurora DB cluster in ``serverless`` DB engine mode. A DB cluster can be paused only when it's idle (it has no connections). .. epigraph:: If a DB cluster is paused for more than seven days, the DB cluster might be backed up with a snapshot. In this case, the DB cluster is restored when there is a request to connect to it.\n")
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum capacity for an Aurora DB cluster in ``serverless`` DB engine mode. For Aurora MySQL, valid capacity values are ``1`` , ``2`` , ``4`` , ``8`` , ``16`` , ``32`` , ``64`` , ``128`` , and ``256`` . For Aurora PostgreSQL, valid capacity values are ``2`` , ``4`` , ``8`` , ``16`` , ``32`` , ``64`` , ``192`` , and ``384`` . The maximum capacity must be greater than or equal to the minimum capacity.\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum capacity for an Aurora DB cluster in ``serverless`` DB engine mode. For Aurora MySQL, valid capacity values are ``1`` , ``2`` , ``4`` , ``8`` , ``16`` , ``32`` , ``64`` , ``128`` , and ``256`` . For Aurora PostgreSQL, valid capacity values are ``2`` , ``4`` , ``8`` , ``16`` , ``32`` , ``64`` , ``192`` , and ``384`` . The minimum capacity must be less than or equal to the maximum capacity.\n')
    seconds_before_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time, in seconds, that Aurora Serverless v1 tries to find a scaling point to perform seamless scaling before enforcing the timeout action. The default is 300. Specify a value between 60 and 600 seconds.\n')
    seconds_until_auto_pause: typing.Union[int, float, None] = pydantic.Field(None, description='The time, in seconds, before an Aurora DB cluster in ``serverless`` mode is paused. Specify a value between 300 and 86,400 seconds.\n')
    timeout_action: typing.Optional[str] = pydantic.Field(None, description='The action to take when the timeout is reached, either ``ForceApplyCapacityChange`` or ``RollbackCapacityChange`` . ``ForceApplyCapacityChange`` sets the capacity to the specified value as soon as possible. ``RollbackCapacityChange`` , the default, ignores the capacity change if a scaling point isn\'t found in the timeout period. .. epigraph:: If you specify ``ForceApplyCapacityChange`` , connections that prevent Aurora Serverless v1 from finding a scaling point might be dropped. For more information, see `Autoscaling for Aurora Serverless v1 <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.how-it-works.html#aurora-serverless.how-it-works.auto-scaling>`_ in the *Amazon Aurora User Guide* .\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbcluster-scalingconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    scaling_configuration_property = rds.CfnDBCluster.ScalingConfigurationProperty(\n        auto_pause=False,\n        max_capacity=123,\n        min_capacity=123,\n        seconds_before_timeout=123,\n        seconds_until_auto_pause=123,\n        timeout_action="timeoutAction"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_pause', 'max_capacity', 'min_capacity', 'seconds_before_timeout', 'seconds_until_auto_pause', 'timeout_action']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBCluster.ScalingConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBCluster.ServerlessV2ScalingConfigurationProperty
class CfnDBCluster_ServerlessV2ScalingConfigurationPropertyDef(BaseStruct):
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 40, 40.5, 41, and so on. The largest value that you can use is 128. The maximum capacity must be higher than 0.5 ACUs. For more information, see `Choosing the maximum Aurora Serverless v2 capacity setting for a cluster <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.setting-capacity.html#aurora-serverless-v2.max_capacity_considerations>`_ in the *Amazon Aurora User Guide* .\n')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 8, 8.5, 9, and so on. The smallest value that you can use is 0.5.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbcluster-serverlessv2scalingconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    serverless_v2_scaling_configuration_property = rds.CfnDBCluster.ServerlessV2ScalingConfigurationProperty(\n        max_capacity=123,\n        min_capacity=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_capacity', 'min_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBCluster.ServerlessV2ScalingConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBInstance.CertificateDetailsProperty
class CfnDBInstance_CertificateDetailsPropertyDef(BaseStruct):
    ca_identifier: typing.Optional[str] = pydantic.Field(None, description="The CA identifier of the CA certificate used for the DB instance's server certificate.\n")
    valid_till: typing.Optional[str] = pydantic.Field(None, description='The expiration date of the DB instances server certificate.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbinstance-certificatedetails.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    certificate_details_property = rds.CfnDBInstance.CertificateDetailsProperty(\n        ca_identifier="caIdentifier",\n        valid_till="validTill"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['ca_identifier', 'valid_till']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBInstance.CertificateDetailsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBInstance.DBInstanceRoleProperty
class CfnDBInstance_DBInstanceRolePropertyDef(BaseStruct):
    feature_name: str = pydantic.Field(..., description='The name of the feature associated with the AWS Identity and Access Management (IAM) role. IAM roles that are associated with a DB instance grant permission for the DB instance to access other AWS services on your behalf. For the list of supported feature names, see the ``SupportedFeatureNames`` description in `DBEngineVersion <https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_DBEngineVersion.html>`_ in the *Amazon RDS API Reference* .\n')
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the IAM role that is associated with the DB instance.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbinstance-dbinstancerole.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    d_bInstance_role_property = rds.CfnDBInstance.DBInstanceRoleProperty(\n        feature_name="featureName",\n        role_arn="roleArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['feature_name', 'role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBInstance.DBInstanceRoleProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBInstance.EndpointProperty
class CfnDBInstance_EndpointPropertyDef(BaseStruct):
    address: typing.Optional[str] = pydantic.Field(None, description='Specifies the DNS address of the DB instance.\n')
    hosted_zone_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the ID that Amazon Route 53 assigns when you create a hosted zone.\n')
    port: typing.Optional[str] = pydantic.Field(None, description='Specifies the port that the database engine is listening on.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbinstance-endpoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    endpoint_property = rds.CfnDBInstance.EndpointProperty(\n        address="address",\n        hosted_zone_id="hostedZoneId",\n        port="port"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['address', 'hosted_zone_id', 'port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBInstance.EndpointProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBInstance.MasterUserSecretProperty
class CfnDBInstance_MasterUserSecretPropertyDef(BaseStruct):
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The AWS KMS key identifier that is used to encrypt the secret.\n')
    secret_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the secret.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbinstance-masterusersecret.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    master_user_secret_property = rds.CfnDBInstance.MasterUserSecretProperty(\n        kms_key_id="kmsKeyId",\n        secret_arn="secretArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['kms_key_id', 'secret_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBInstance.MasterUserSecretProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBInstance.ProcessorFeatureProperty
class CfnDBInstance_ProcessorFeaturePropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the processor feature. Valid names are ``coreCount`` and ``threadsPerCore`` .\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The value of a processor feature name.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbinstance-processorfeature.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    processor_feature_property = rds.CfnDBInstance.ProcessorFeatureProperty(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBInstance.ProcessorFeatureProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBProxy.AuthFormatProperty
class CfnDBProxy_AuthFormatPropertyDef(BaseStruct):
    auth_scheme: typing.Optional[str] = pydantic.Field(None, description='The type of authentication that the proxy uses for connections from the proxy to the underlying database. Valid Values: ``SECRETS``\n')
    client_password_auth_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the details of authentication used by a proxy to log in as a specific database user.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A user-specified description about the authentication used by a proxy to log in as a specific database user.\n')
    iam_auth: typing.Optional[str] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. The ``ENABLED`` value is valid only for proxies with RDS for Microsoft SQL Server. Valid Values: ``ENABLED | DISABLED | REQUIRED``\n')
    secret_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) representing the secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbproxy-authformat.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    auth_format_property = rds.CfnDBProxy.AuthFormatProperty(\n        auth_scheme="authScheme",\n        client_password_auth_type="clientPasswordAuthType",\n        description="description",\n        iam_auth="iamAuth",\n        secret_arn="secretArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth_scheme', 'client_password_auth_type', 'description', 'iam_auth', 'secret_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxy.AuthFormatProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBProxy.TagFormatProperty
class CfnDBProxy_TagFormatPropertyDef(BaseStruct):
    key: typing.Optional[str] = pydantic.Field(None, description='A key is the required name of the tag. The string value can be 1-128 Unicode characters in length and can\'t be prefixed with ``aws:`` . The string can contain only the set of Unicode letters, digits, white-space, \'*\', \'.\', \'/\', \'=\', \'+\', \'-\' (Java regex: "^([\\p{L}\\p{Z}\\p{N}*.:/=+\\-]*)$").\n')
    value: typing.Optional[str] = pydantic.Field(None, description='A value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can\'t be prefixed with ``aws:`` . The string can contain only the set of Unicode letters, digits, white-space, \'*\', \'.\', \'/\', \'=\', \'+\', \'-\' (Java regex: "^([\\p{L}\\p{Z}\\p{N}*.:/=+\\-]*)$").\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbproxy-tagformat.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    tag_format_property = rds.CfnDBProxy.TagFormatProperty(\n        key="key",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxy.TagFormatProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBProxyEndpoint.TagFormatProperty
class CfnDBProxyEndpoint_TagFormatPropertyDef(BaseStruct):
    key: typing.Optional[str] = pydantic.Field(None, description='A value is the optional value of the tag. The string value can be 1-256 Unicode characters in length and can\'t be prefixed with ``aws:`` . The string can contain only the set of Unicode letters, digits, white-space, \'*\', \'.\', \'/\', \'=\', \'+\', \'-\' (Java regex: "^([\\p{L}\\p{Z}\\p{N}*.:/=+\\-]*)$").\n')
    value: typing.Optional[str] = pydantic.Field(None, description='Metadata assigned to a DB instance consisting of a key-value pair.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbproxyendpoint-tagformat.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    tag_format_property = rds.CfnDBProxyEndpoint.TagFormatProperty(\n        key="key",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxyEndpoint.TagFormatProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBProxyTargetGroup.ConnectionPoolConfigurationInfoFormatProperty
class CfnDBProxyTargetGroup_ConnectionPoolConfigurationInfoFormatPropertyDef(BaseStruct):
    connection_borrow_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The number of seconds for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Default: 120 Constraints: between 1 and 3600, or 0 representing unlimited\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with ``SET`` statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single ``SET`` statement, such as ``SET x=1, y=2`` . Default: no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. The value is expressed as a percentage of the ``max_connections`` setting for the RDS DB instance or Aurora DB cluster used by the target group. If you specify ``MaxIdleConnectionsPercent`` , then you must also include a value for this parameter. Default: 10 for RDS for Microsoft SQL Server, and 100 for all other engines Constraints: Must be between 1 and 100.\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description="Controls how actively the proxy closes idle database connections in the connection pool. The value is expressed as a percentage of the ``max_connections`` setting for the RDS DB instance or Aurora DB cluster used by the target group. With a high value, the proxy leaves a high percentage of idle database connections open. A low value causes the proxy to close more idle connections and return them to the database. If you specify this parameter, then you must also include a value for ``MaxConnectionsPercent`` . Default: The default value is half of the value of ``MaxConnectionsPercent`` . For example, if ``MaxConnectionsPercent`` is 80, then the default value of ``MaxIdleConnectionsPercent`` is 40. If the value of ``MaxConnectionsPercent`` isn't specified, then for SQL Server, ``MaxIdleConnectionsPercent`` is 5, and for all other engines, the default is 50. Constraints: Must be between 0 and the value of ``MaxConnectionsPercent`` .\n")
    session_pinning_filters: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: no session pinning filters\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbproxytargetgroup-connectionpoolconfigurationinfoformat.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    connection_pool_configuration_info_format_property = rds.CfnDBProxyTargetGroup.ConnectionPoolConfigurationInfoFormatProperty(\n        connection_borrow_timeout=123,\n        init_query="initQuery",\n        max_connections_percent=123,\n        max_idle_connections_percent=123,\n        session_pinning_filters=["sessionPinningFilters"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['connection_borrow_timeout', 'init_query', 'max_connections_percent', 'max_idle_connections_percent', 'session_pinning_filters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxyTargetGroup.ConnectionPoolConfigurationInfoFormatProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBSecurityGroup.IngressProperty
class CfnDBSecurityGroup_IngressPropertyDef(BaseStruct):
    cidrip: typing.Optional[str] = pydantic.Field(None, description='The IP range to authorize.\n')
    ec2_security_group_id: typing.Optional[str] = pydantic.Field(None, description='Id of the EC2 security group to authorize. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n')
    ec2_security_group_name: typing.Optional[str] = pydantic.Field(None, description='Name of the EC2 security group to authorize. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n')
    ec2_security_group_owner_id: typing.Optional[str] = pydantic.Field(None, description='AWS account number of the owner of the EC2 security group specified in the ``EC2SecurityGroupName`` parameter. The AWS access key ID isn\'t an acceptable value. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-security-group-rule.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    ingress_property = rds.CfnDBSecurityGroup.IngressProperty(\n        cidrip="cidrip",\n        ec2_security_group_id="ec2SecurityGroupId",\n        ec2_security_group_name="ec2SecurityGroupName",\n        ec2_security_group_owner_id="ec2SecurityGroupOwnerId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cidrip', 'ec2_security_group_id', 'ec2_security_group_name', 'ec2_security_group_owner_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBSecurityGroup.IngressProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnOptionGroup.OptionConfigurationProperty
class CfnOptionGroup_OptionConfigurationPropertyDef(BaseStruct):
    option_name: str = pydantic.Field(..., description='The configuration of options to include in a group.\n')
    db_security_group_memberships: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of DBSecurityGroupMembership name strings used for this option.\n')
    option_settings: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnOptionGroup_OptionSettingPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The option settings to include in an option group.\n')
    option_version: typing.Optional[str] = pydantic.Field(None, description='The version for the option.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The optional port for the option.\n')
    vpc_security_group_memberships: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of VpcSecurityGroupMembership name strings used for this option.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-optiongroup-optionconfiguration.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    option_configuration_property = rds.CfnOptionGroup.OptionConfigurationProperty(\n        option_name="optionName",\n\n        # the properties below are optional\n        db_security_group_memberships=["dbSecurityGroupMemberships"],\n        option_settings=[rds.CfnOptionGroup.OptionSettingProperty(\n            name="name",\n            value="value"\n        )],\n        option_version="optionVersion",\n        port=123,\n        vpc_security_group_memberships=["vpcSecurityGroupMemberships"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['option_name', 'db_security_group_memberships', 'option_settings', 'option_version', 'port', 'vpc_security_group_memberships']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnOptionGroup.OptionConfigurationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnOptionGroup.OptionSettingProperty
class CfnOptionGroup_OptionSettingPropertyDef(BaseStruct):
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the option that has settings that you can set.\n')
    value: typing.Optional[str] = pydantic.Field(None, description='The current value of the option setting.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-optiongroup-optionsetting.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    option_setting_property = rds.CfnOptionGroup.OptionSettingProperty(\n        name="name",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnOptionGroup.OptionSettingProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ClusterEngineBindOptions
class ClusterEngineBindOptionsDef(BaseStruct):
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The customer-provided ParameterGroup. Default: - none\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 exporting. Default: - none\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 importing. Default: - none\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_rds as rds\n\n    # parameter_group: rds.ParameterGroup\n    # role: iam.Role\n\n    cluster_engine_bind_options = rds.ClusterEngineBindOptions(\n        parameter_group=parameter_group,\n        s3_export_role=role,\n        s3_import_role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['parameter_group', 's3_export_role', 's3_import_role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterEngineBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ClusterEngineConfig
class ClusterEngineConfigDef(BaseStruct):
    features: typing.Union[models.aws_rds.ClusterEngineFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Features supported by the database engine. Default: - no features\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The ParameterGroup to use for the cluster. Default: - no ParameterGroup will be used\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port to use for this cluster, unless the customer specified the port directly. Default: - use the default port for clusters (3306)\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # parameter_group: rds.ParameterGroup\n\n    cluster_engine_config = rds.ClusterEngineConfig(\n        features=rds.ClusterEngineFeatures(\n            s3_export="s3Export",\n            s3_import="s3Import"\n        ),\n        parameter_group=parameter_group,\n        port=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['features', 'parameter_group', 'port']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterEngineConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ClusterEngineFeatures
class ClusterEngineFeaturesDef(BaseStruct):
    s3_export: typing.Optional[str] = pydantic.Field(None, description='Feature name for the DB instance that the IAM role to export to S3 bucket is to be associated with. Default: - no s3Export feature name\n')
    s3_import: typing.Optional[str] = pydantic.Field(None, description='Feature name for the DB instance that the IAM role to access the S3 bucket for import is to be associated with. Default: - no s3Import feature name\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cluster_engine_features = rds.ClusterEngineFeatures(\n        s3_export="s3Export",\n        s3_import="s3Import"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_export', 's3_import']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterEngineFeatures'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ClusterInstanceBindOptions
class ClusterInstanceBindOptionsDef(BaseStruct):
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instances. Default: no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instances monitoring. Default: - A role is automatically created for you\n')
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description='The promotion tier of the cluster instance. This matters more for serverlessV2 instances. If a serverless instance is in tier 0-1 then it will scale with the writer. For provisioned instances this just determines the failover priority. If multiple instances have the same priority then one will be picked at random Default: 2\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy on the cluster. Default: - RemovalPolicy.DESTROY (cluster snapshot can restore)\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. This is only needed when using the isFromLegacyInstanceProps Default: - cluster subnet group is used\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_rds as rds\n\n    # role: iam.Role\n    # subnet_group: rds.SubnetGroup\n\n    cluster_instance_bind_options = rds.ClusterInstanceBindOptions(\n        monitoring_interval=cdk.Duration.minutes(30),\n        monitoring_role=role,\n        promotion_tier=123,\n        removal_policy=cdk.RemovalPolicy.DESTROY,\n        subnet_group=subnet_group\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['monitoring_interval', 'monitoring_role', 'promotion_tier', 'removal_policy', 'subnet_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterInstanceBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ClusterInstanceOptions
class ClusterInstanceOptionsDef(BaseStruct):
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow upgrade of major version for the DB instance. Default: - false\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable automatic upgrade of minor version for the DB instance. Default: - true\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier for the database instance. Default: - CloudFormation generated identifier\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. This is only needed if you need to configure different parameter groups for each individual instance, otherwise you should not provide this and just use the cluster parameter group Default: the cluster parameter group is used\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - true if the instance is placed in a public subnet\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kms as kms\n    from aws_cdk import aws_rds as rds\n\n    # key: kms.Key\n    # parameter_group: rds.ParameterGroup\n\n    cluster_instance_options = rds.ClusterInstanceOptions(\n        allow_major_version_upgrade=False,\n        auto_minor_version_upgrade=False,\n        enable_performance_insights=False,\n        instance_identifier="instanceIdentifier",\n        parameter_group=parameter_group,\n        parameters={\n            "parameters_key": "parameters"\n        },\n        performance_insight_encryption_key=key,\n        performance_insight_retention=rds.PerformanceInsightRetention.DEFAULT,\n        publicly_accessible=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_major_version_upgrade', 'auto_minor_version_upgrade', 'enable_performance_insights', 'instance_identifier', 'parameter_group', 'parameters', 'performance_insight_encryption_key', 'performance_insight_retention', 'publicly_accessible']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterInstanceOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ClusterInstanceProps
class ClusterInstancePropsDef(BaseStruct):
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow upgrade of major version for the DB instance. Default: - false\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable automatic upgrade of minor version for the DB instance. Default: - true\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier for the database instance. Default: - CloudFormation generated identifier\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. This is only needed if you need to configure different parameter groups for each individual instance, otherwise you should not provide this and just use the cluster parameter group Default: the cluster parameter group is used\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - true if the instance is placed in a public subnet\n')
    instance_type: models.aws_rds.ClusterInstanceTypeDef = pydantic.Field(..., description='The type of cluster instance to create. Can be either provisioned or serverless v2\n')
    is_from_legacy_instance_props: typing.Optional[bool] = pydantic.Field(None, description='Only used for migrating existing clusters from using ``instanceProps`` to ``writer`` and ``readers``. Default: false\n')
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description='The promotion tier of the cluster instance. This matters more for serverlessV2 instances. If a serverless instance is in tier 0-1 then it will scale with the writer. For provisioned instances this just determines the failover priority. If multiple instances have the same priority then one will be picked at random Default: 2\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_kms as kms\n    from aws_cdk import aws_rds as rds\n\n    # cluster_instance_type: rds.ClusterInstanceType\n    # key: kms.Key\n    # parameter_group: rds.ParameterGroup\n\n    cluster_instance_props = rds.ClusterInstanceProps(\n        instance_type=cluster_instance_type,\n\n        # the properties below are optional\n        allow_major_version_upgrade=False,\n        auto_minor_version_upgrade=False,\n        enable_performance_insights=False,\n        instance_identifier="instanceIdentifier",\n        is_from_legacy_instance_props=False,\n        parameter_group=parameter_group,\n        parameters={\n            "parameters_key": "parameters"\n        },\n        performance_insight_encryption_key=key,\n        performance_insight_retention=rds.PerformanceInsightRetention.DEFAULT,\n        promotion_tier=123,\n        publicly_accessible=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_major_version_upgrade', 'auto_minor_version_upgrade', 'enable_performance_insights', 'instance_identifier', 'parameter_group', 'parameters', 'performance_insight_encryption_key', 'performance_insight_retention', 'publicly_accessible', 'instance_type', 'is_from_legacy_instance_props', 'promotion_tier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ClusterInstanceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CommonRotationUserOptions
class CommonRotationUserOptionsDef(BaseStruct):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_rds as rds\n\n    # interface_vpc_endpoint: ec2.InterfaceVpcEndpoint\n    # security_group: ec2.SecurityGroup\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n\n    common_rotation_user_options = rds.CommonRotationUserOptions(\n        automatically_after=cdk.Duration.minutes(30),\n        endpoint=interface_vpc_endpoint,\n        exclude_characters="excludeCharacters",\n        security_group=security_group,\n        vpc_subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['automatically_after', 'endpoint', 'exclude_characters', 'security_group', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CommonRotationUserOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CredentialsBaseOptions
class CredentialsBaseOptionsDef(BaseStruct):
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key to encrypt the generated secret. Default: - default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='The characters to exclude from the generated password. Has no effect if ``password`` has been provided. Default: - the DatabaseSecret default exclude character set (" %+~`#$&*()|[]{}:;<>?!\'/@"\\")\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated\n')
    secret_name: typing.Optional[str] = pydantic.Field(None, description='The name of the secret. Default: - A name is generated by CloudFormation.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    engine = rds.DatabaseInstanceEngine.postgres(version=rds.PostgresEngineVersion.VER_15_2)\n    my_key = kms.Key(self, "MyKey")\n\n    rds.DatabaseInstance(self, "InstanceWithCustomizedSecret",\n        engine=engine,\n        vpc=vpc,\n        credentials=rds.Credentials.from_generated_secret("postgres",\n            secret_name="my-cool-name",\n            encryption_key=my_key,\n            exclude_characters="!&*^#@()",\n            replica_regions=[secretsmanager.ReplicaRegion(region="eu-west-1"), secretsmanager.ReplicaRegion(region="eu-west-2")]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['encryption_key', 'exclude_characters', 'replica_regions', 'secret_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CredentialsBaseOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CredentialsFromUsernameOptions
class CredentialsFromUsernameOptionsDef(BaseStruct):
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key to encrypt the generated secret. Default: - default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='The characters to exclude from the generated password. Has no effect if ``password`` has been provided. Default: - the DatabaseSecret default exclude character set (" %+~`#$&*()|[]{}:;<>?!\'/@"\\")\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated\n')
    secret_name: typing.Optional[str] = pydantic.Field(None, description='The name of the secret. Default: - A name is generated by CloudFormation.\n')
    password: typing.Optional[models.SecretValueDef] = pydantic.Field(None, description='Password. Do not put passwords in your CDK code directly. Default: - a Secrets Manager generated password\n\n:exampleMetadata: infused\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    # vpc: ec2.Vpc\n\n    cluster = rds.DatabaseCluster(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora_postgres(version=rds.AuroraPostgresEngineVersion.VER_15_2),\n        credentials=rds.Credentials.from_username("adminuser", password=cdk.SecretValue.unsafe_plain_text("7959866cacc02c2d243ecfe177464fe6")),\n        instance_props=rds.InstanceProps(\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.X2G, ec2.InstanceSize.XLARGE),\n            vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PUBLIC),\n            vpc=vpc\n        ),\n        storage_type=rds.DBClusterStorageType.AURORA_IOPT1\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['encryption_key', 'exclude_characters', 'replica_regions', 'secret_name', 'password']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CredentialsFromUsernameOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.DatabaseClusterAttributes
class DatabaseClusterAttributesDef(BaseStruct):
    cluster_identifier: str = pydantic.Field(..., description='Identifier for the cluster.\n')
    cluster_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Cluster endpoint address. Default: - no endpoint address\n')
    cluster_resource_identifier: typing.Optional[str] = pydantic.Field(None, description='The immutable identifier for the cluster; for example: cluster-ABCD1234EFGH5678IJKL90MNOP. This AWS Region-unique identifier is used to grant access to the cluster. Default: none\n')
    engine: typing.Optional[typing.Union[models.aws_rds.DatabaseClusterEngineDef]] = pydantic.Field(None, description="The engine of the existing Cluster. Default: - the imported Cluster's engine is unknown\n")
    instance_endpoint_addresses: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Endpoint addresses of individual instances. Default: - no instance endpoints\n')
    instance_identifiers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Identifier for the instances. Default: - no instance identifiers\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The database port. Default: - none\n')
    reader_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Reader endpoint address. Default: - no reader address\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups of the database cluster. Default: - no security groups\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_rds as rds\n\n    # cluster_engine: rds.IClusterEngine\n    # security_group: ec2.SecurityGroup\n\n    database_cluster_attributes = rds.DatabaseClusterAttributes(\n        cluster_identifier="clusterIdentifier",\n\n        # the properties below are optional\n        cluster_endpoint_address="clusterEndpointAddress",\n        cluster_resource_identifier="clusterResourceIdentifier",\n        engine=cluster_engine,\n        instance_endpoint_addresses=["instanceEndpointAddresses"],\n        instance_identifiers=["instanceIdentifiers"],\n        port=123,\n        reader_endpoint_address="readerEndpointAddress",\n        security_groups=[security_group]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster_identifier', 'cluster_endpoint_address', 'cluster_resource_identifier', 'engine', 'instance_endpoint_addresses', 'instance_identifiers', 'port', 'reader_endpoint_address', 'security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseClusterAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.DatabaseClusterFromSnapshotProps
class DatabaseClusterFromSnapshotPropsDef(BaseStruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    snapshot_identifier: str = pydantic.Field(..., description='The identifier for the DB instance snapshot or DB cluster snapshot to restore from. You can use either the name or the Amazon Resource Name (ARN) to specify a DB cluster snapshot. However, you can use only the ARN to specify a DB instance snapshot.\n')
    backtrack_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of seconds to set a cluster's target backtrack window to. This feature is only supported by the Aurora MySQL database engine and cannot be enabled on existing clusters. Default: 0 seconds (no backtrack)\n")
    backup: typing.Union[models.aws_rds.BackupPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Backup settings. Default: - Backup retention period for automated backups is 1 day. Backup preferred window is set to a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="(deprecated) Credentials for the administrative user. Note - using this prop only works with ``Credentials.fromPassword()`` with the username of the snapshot, ``Credentials.fromUsername()`` with the username and password of the snapshot or ``Credentials.fromSecret()`` with a secret containing the username and password of the snapshot. Default: - A username of 'admin' (or 'postgres' for PostgreSQL) and SecretsManager-generated password that **will not be applied** to the cluster, use ``snapshotCredentials`` for the correct behavior.\n")
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, ``undefined`` otherwise, which will not enable deletion protection. To disable deletion protection after it has been enabled, you must explicitly set this value to ``false``.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier_base: typing.Optional[str] = pydantic.Field(None, description='Base identifier for instances. Every replica is named by appending the replica number to this string, 1-based. Default: - clusterIdentifier is used with the word "Instance" appended. If clusterIdentifier is not provided, the identifier is automatically generated.\n')
    instance_props: typing.Union[models.aws_rds.InstancePropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(deprecated) Settings for the individual instances that are launched.\n')
    instances: typing.Union[int, float, None] = pydantic.Field(None, description='(deprecated) How many replicas/instances to create. Has to be at least 1. Default: 2\n')
    instance_update_behaviour: typing.Optional[aws_cdk.aws_rds.InstanceUpdateBehaviour] = pydantic.Field(None, description='The ordering of updates for instances. Default: InstanceUpdateBehaviour.BULK\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instances. Default: no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instances monitoring. Default: - A role is automatically created for you\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - No parameter group.\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBClusterParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBClusterParameterGroup. Default: - None\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='What port to listen on. Default: - The default for the engine is used.\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description="A preferred maintenance window day/time range. Should be specified as a range ddd:hh24:mi-ddd:hh24:mi (24H Clock UTC). Example: 'Sun:23:45-Mon:00:15' Default: - 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n")
    readers: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.ClusterInstanceDef]]] = pydantic.Field(None, description='A list of instances to create as cluster reader instances. Default: - no readers are created. The cluster will have a single writer/reader\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportRole`` is used. For MySQL: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 export. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportRole`` is used. For MySQL: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 import. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: a new security group is created.\n')
    serverless_v2_max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 40, 40.5, 41, and so on. The largest value that you can use is 128 (256GB). The maximum capacity must be higher than 0.5 ACUs. Default: 2\n')
    serverless_v2_min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 8, 8.5, 9, and so on. The smallest value that you can use is 0.5. Default: 0.5\n')
    snapshot_credentials: typing.Optional[models.aws_rds.SnapshotCredentialsDef] = pydantic.Field(None, description='Master user credentials. Note - It is not possible to change the master username for a snapshot; however, it is possible to provide (or generate) a new password. Default: - The existing username and password from the snapshot will be used.\n')
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable storage encryption. Default: - true if storageEncryptionKey is provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key for storage encryption. If specified, ``storageEncrypted`` will be set to ``true``. Default: - if storageEncrypted is true then the default master key, no key otherwise\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.DBClusterStorageType] = pydantic.Field(None, description='The storage type to be associated with the DB cluster. Default: - DBClusterStorageType.AURORA_IOPT1\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group will be created.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='What subnets to run the RDS instances in. Must be at least 2 subnets in two different AZs.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. Default: - the Vpc default strategy if not specified.\n')
    writer: typing.Optional[typing.Union[models.aws_rds.ClusterInstanceDef]] = pydantic.Field(None, description='The instance to use for the cluster writer. Default: required if instanceProps is not provided\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    rds.DatabaseClusterFromSnapshot(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora(version=rds.AuroraEngineVersion.VER_1_22_2),\n        writer=rds.ClusterInstance.provisioned("writer"),\n        vpc=vpc,\n        snapshot_identifier="mySnapshot"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'snapshot_identifier', 'backtrack_window', 'backup', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'iam_authentication', 'instance_identifier_base', 'instance_props', 'instances', 'instance_update_behaviour', 'monitoring_interval', 'monitoring_role', 'network_type', 'parameter_group', 'parameters', 'port', 'preferred_maintenance_window', 'readers', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'serverless_v2_max_capacity', 'serverless_v2_min_capacity', 'snapshot_credentials', 'storage_encrypted', 'storage_encryption_key', 'storage_type', 'subnet_group', 'vpc', 'vpc_subnets', 'writer']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseClusterFromSnapshotProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.DatabaseClusterProps
class DatabaseClusterPropsDef(BaseStruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    backtrack_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The number of seconds to set a cluster's target backtrack window to. This feature is only supported by the Aurora MySQL database engine and cannot be enabled on existing clusters. Default: 0 seconds (no backtrack)\n")
    backup: typing.Union[models.aws_rds.BackupPropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Backup settings. Default: - Backup retention period for automated backups is 1 day. Backup preferred window is set to a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="Credentials for the administrative user. Default: - A username of 'admin' (or 'postgres' for PostgreSQL) and SecretsManager-generated password\n")
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, ``undefined`` otherwise, which will not enable deletion protection. To disable deletion protection after it has been enabled, you must explicitly set this value to ``false``.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier_base: typing.Optional[str] = pydantic.Field(None, description='Base identifier for instances. Every replica is named by appending the replica number to this string, 1-based. Default: - clusterIdentifier is used with the word "Instance" appended. If clusterIdentifier is not provided, the identifier is automatically generated.\n')
    instance_props: typing.Union[models.aws_rds.InstancePropsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='(deprecated) Settings for the individual instances that are launched.\n')
    instances: typing.Union[int, float, None] = pydantic.Field(None, description='(deprecated) How many replicas/instances to create. Has to be at least 1. Default: 2\n')
    instance_update_behaviour: typing.Optional[aws_cdk.aws_rds.InstanceUpdateBehaviour] = pydantic.Field(None, description='The ordering of updates for instances. Default: InstanceUpdateBehaviour.BULK\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instances. Default: no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instances monitoring. Default: - A role is automatically created for you\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - No parameter group.\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBClusterParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBClusterParameterGroup. Default: - None\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='What port to listen on. Default: - The default for the engine is used.\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description="A preferred maintenance window day/time range. Should be specified as a range ddd:hh24:mi-ddd:hh24:mi (24H Clock UTC). Example: 'Sun:23:45-Mon:00:15' Default: - 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week.\n")
    readers: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.ClusterInstanceDef]]] = pydantic.Field(None, description='A list of instances to create as cluster reader instances. Default: - no readers are created. The cluster will have a single writer/reader\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportRole`` is used. For MySQL: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 export. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ExportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportRole`` is used. For MySQL: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB cluster to enable S3 import. This feature is only supported by the Aurora database engine. This property must not be used if ``s3ImportBuckets`` is used. For MySQL: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: a new security group is created.\n')
    serverless_v2_max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 40, 40.5, 41, and so on. The largest value that you can use is 128 (256GB). The maximum capacity must be higher than 0.5 ACUs. Default: 2\n')
    serverless_v2_min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='The minimum number of Aurora capacity units (ACUs) for a DB instance in an Aurora Serverless v2 cluster. You can specify ACU values in half-step increments, such as 8, 8.5, 9, and so on. The smallest value that you can use is 0.5. Default: 0.5\n')
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable storage encryption. Default: - true if storageEncryptionKey is provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key for storage encryption. If specified, ``storageEncrypted`` will be set to ``true``. Default: - if storageEncrypted is true then the default master key, no key otherwise\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.DBClusterStorageType] = pydantic.Field(None, description='The storage type to be associated with the DB cluster. Default: - DBClusterStorageType.AURORA_IOPT1\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group will be created.\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='What subnets to run the RDS instances in. Must be at least 2 subnets in two different AZs.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. Default: - the Vpc default strategy if not specified.\n')
    writer: typing.Optional[typing.Union[models.aws_rds.ClusterInstanceDef]] = pydantic.Field(None, description='The instance to use for the cluster writer. Default: required if instanceProps is not provided\n\n:exampleMetadata: infused\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    # vpc: ec2.Vpc\n\n    cluster = rds.DatabaseCluster(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora_mysql(version=rds.AuroraMysqlEngineVersion.VER_2_08_1),\n        writer=rds.ClusterInstance.provisioned("writer",\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.R6G, ec2.InstanceSize.XLARGE4)\n        ),\n        serverless_v2_min_capacity=6.5,\n        serverless_v2_max_capacity=64,\n        readers=[\n            # will be put in promotion tier 1 and will scale with the writer\n            rds.ClusterInstance.serverless_v2("reader1", scale_with_writer=True),\n            # will be put in promotion tier 2 and will not scale with the writer\n            rds.ClusterInstance.serverless_v2("reader2")\n        ],\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'backtrack_window', 'backup', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'iam_authentication', 'instance_identifier_base', 'instance_props', 'instances', 'instance_update_behaviour', 'monitoring_interval', 'monitoring_role', 'network_type', 'parameter_group', 'parameters', 'port', 'preferred_maintenance_window', 'readers', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'serverless_v2_max_capacity', 'serverless_v2_min_capacity', 'storage_encrypted', 'storage_encryption_key', 'storage_type', 'subnet_group', 'vpc', 'vpc_subnets', 'writer']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceAttributes
class DatabaseInstanceAttributesDef(BaseStruct):
    instance_endpoint_address: str = pydantic.Field(..., description='The endpoint address.\n')
    instance_identifier: str = pydantic.Field(..., description='The instance identifier.\n')
    port: typing.Union[int, float] = pydantic.Field(..., description='The database port.\n')
    security_groups: typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(..., description='The security groups of the instance.\n')
    engine: typing.Optional[typing.Union[models.aws_rds.DatabaseInstanceEngineDef]] = pydantic.Field(None, description="The engine of the existing database Instance. Default: - the imported Instance's engine is unknown\n")
    instance_resource_id: typing.Optional[str] = pydantic.Field(None, description='The AWS Region-unique, immutable identifier for the DB instance. This identifier is found in AWS CloudTrail log entries whenever the AWS KMS key for the DB instance is accessed.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_rds as rds\n\n    # instance_engine: rds.IInstanceEngine\n    # security_group: ec2.SecurityGroup\n\n    database_instance_attributes = rds.DatabaseInstanceAttributes(\n        instance_endpoint_address="instanceEndpointAddress",\n        instance_identifier="instanceIdentifier",\n        port=123,\n        security_groups=[security_group],\n\n        # the properties below are optional\n        engine=instance_engine,\n        instance_resource_id="instanceResourceId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['instance_endpoint_address', 'instance_identifier', 'port', 'security_groups', 'engine', 'instance_resource_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceFromSnapshotProps
class DatabaseInstanceFromSnapshotPropsDef(BaseStruct):
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets\n')
    engine: typing.Union[models.aws_rds.DatabaseInstanceEngineDef] = pydantic.Field(..., description='The database engine.\n')
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The allocated storage size, specified in gibibytes (GiB). Default: 100\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow major version upgrades. Default: false\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of the database. Default: - no name\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The name of the compute and memory capacity for the instance. Default: - m5.large (or, more specifically, db.m5.large)\n')
    license_model: typing.Optional[aws_cdk.aws_rds.LicenseModel] = pydantic.Field(None, description='The license model. Default: - RDS default license model\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The time zone of the instance. This is currently supported only by Microsoft Sql Server. Default: - RDS default timezone\n')
    snapshot_identifier: str = pydantic.Field(..., description="The name or Amazon Resource Name (ARN) of the DB snapshot that's used to restore the DB instance. If you're restoring from a shared manual DB snapshot, you must specify the ARN of the snapshot.\n")
    credentials: typing.Optional[models.aws_rds.SnapshotCredentialsDef] = pydantic.Field(None, description='Master user credentials. Note - It is not possible to change the master username for a snapshot; however, it is possible to provide (or generate) a new password. Default: - The existing username and password from the snapshot will be used.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    # source_instance: rds.DatabaseInstance\n\n    rds.DatabaseInstanceFromSnapshot(self, "Instance",\n        snapshot_identifier="my-snapshot",\n        engine=rds.DatabaseInstanceEngine.postgres(version=rds.PostgresEngineVersion.VER_15_2),\n        # optional, defaults to m5.large\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE2, ec2.InstanceSize.LARGE),\n        vpc=vpc\n    )\n    rds.DatabaseInstanceReadReplica(self, "ReadReplica",\n        source_database_instance=source_instance,\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE2, ec2.InstanceSize.LARGE),\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets', 'engine', 'allocated_storage', 'allow_major_version_upgrade', 'database_name', 'instance_type', 'license_model', 'parameters', 'timezone', 'snapshot_identifier', 'credentials']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceFromSnapshotProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseInstanceFromSnapshotPropsDefConfig] = pydantic.Field(None)


class DatabaseInstanceFromSnapshotPropsDefConfig(pydantic.BaseModel):
    engine_config: typing.Optional[models._interface_methods.AwsRdsIInstanceEngineDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceNewProps
class DatabaseInstanceNewPropsDef(BaseStruct):
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_kms as kms\n    from aws_cdk import aws_logs as logs\n    from aws_cdk import aws_rds as rds\n    from aws_cdk import aws_s3 as s3\n\n    # bucket: s3.Bucket\n    # key: kms.Key\n    # option_group: rds.OptionGroup\n    # parameter_group: rds.ParameterGroup\n    # role: iam.Role\n    # security_group: ec2.SecurityGroup\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # subnet_group: rds.SubnetGroup\n    # vpc: ec2.Vpc\n\n    database_instance_new_props = rds.DatabaseInstanceNewProps(\n        vpc=vpc,\n\n        # the properties below are optional\n        auto_minor_version_upgrade=False,\n        availability_zone="availabilityZone",\n        backup_retention=cdk.Duration.minutes(30),\n        cloudwatch_logs_exports=["cloudwatchLogsExports"],\n        cloudwatch_logs_retention=logs.RetentionDays.ONE_DAY,\n        cloudwatch_logs_retention_role=role,\n        copy_tags_to_snapshot=False,\n        delete_automated_backups=False,\n        deletion_protection=False,\n        domain="domain",\n        domain_role=role,\n        enable_performance_insights=False,\n        iam_authentication=False,\n        instance_identifier="instanceIdentifier",\n        iops=123,\n        max_allocated_storage=123,\n        monitoring_interval=cdk.Duration.minutes(30),\n        monitoring_role=role,\n        multi_az=False,\n        network_type=rds.NetworkType.IPV4,\n        option_group=option_group,\n        parameter_group=parameter_group,\n        performance_insight_encryption_key=key,\n        performance_insight_retention=rds.PerformanceInsightRetention.DEFAULT,\n        port=123,\n        preferred_backup_window="preferredBackupWindow",\n        preferred_maintenance_window="preferredMaintenanceWindow",\n        processor_features=rds.ProcessorFeatures(\n            core_count=123,\n            threads_per_core=123\n        ),\n        publicly_accessible=False,\n        removal_policy=cdk.RemovalPolicy.DESTROY,\n        s3_export_buckets=[bucket],\n        s3_export_role=role,\n        s3_import_buckets=[bucket],\n        s3_import_role=role,\n        security_groups=[security_group],\n        storage_throughput=123,\n        storage_type=rds.StorageType.STANDARD,\n        subnet_group=subnet_group,\n        vpc_subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceNewProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseInstanceNewPropsDefConfig] = pydantic.Field(None)


class DatabaseInstanceNewPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceProps
class DatabaseInstancePropsDef(BaseStruct):
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets\n')
    engine: typing.Union[models.aws_rds.DatabaseInstanceEngineDef] = pydantic.Field(..., description='The database engine.\n')
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The allocated storage size, specified in gibibytes (GiB). Default: 100\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow major version upgrades. Default: false\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of the database. Default: - no name\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The name of the compute and memory capacity for the instance. Default: - m5.large (or, more specifically, db.m5.large)\n')
    license_model: typing.Optional[aws_cdk.aws_rds.LicenseModel] = pydantic.Field(None, description='The license model. Default: - RDS default license model\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The time zone of the instance. This is currently supported only by Microsoft Sql Server. Default: - RDS default timezone\n')
    character_set_name: typing.Optional[str] = pydantic.Field(None, description='For supported engines, specifies the character set to associate with the DB instance. Default: - RDS default character set name\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="Credentials for the administrative user. Default: - A username of 'admin' (or 'postgres' for PostgreSQL) and SecretsManager-generated password\n")
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is encrypted. Default: - true if storageEncryptionKey has been provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key that\'s used to encrypt the DB instance. Default: - default master key if storageEncrypted is true, no key otherwise\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.IVpc\n\n\n    instance1 = rds.DatabaseInstance(self, "PostgresInstance1",\n        engine=rds.DatabaseInstanceEngine.POSTGRES,\n        # Generate the secret with admin username `postgres` and random password\n        credentials=rds.Credentials.from_generated_secret("postgres"),\n        vpc=vpc\n    )\n    # Templated secret with username and password fields\n    templated_secret = secretsmanager.Secret(self, "TemplatedSecret",\n        generate_secret_string=secretsmanager.SecretStringGenerator(\n            secret_string_template=JSON.stringify({"username": "postgres"}),\n            generate_string_key="password",\n            exclude_characters="/@""\n        )\n    )\n    # Using the templated secret as credentials\n    instance2 = rds.DatabaseInstance(self, "PostgresInstance2",\n        engine=rds.DatabaseInstanceEngine.POSTGRES,\n        credentials={\n            "username": templated_secret.secret_value_from_json("username").to_string(),\n            "password": templated_secret.secret_value_from_json("password")\n        },\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets', 'engine', 'allocated_storage', 'allow_major_version_upgrade', 'database_name', 'instance_type', 'license_model', 'parameters', 'timezone', 'character_set_name', 'credentials', 'storage_encrypted', 'storage_encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseInstancePropsDefConfig] = pydantic.Field(None)


class DatabaseInstancePropsDefConfig(pydantic.BaseModel):
    engine_config: typing.Optional[models._interface_methods.AwsRdsIInstanceEngineDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceReadReplicaProps
class DatabaseInstanceReadReplicaPropsDef(BaseStruct):
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets\n')
    instance_type: models.aws_ec2.InstanceTypeDef = pydantic.Field(..., description='The name of the compute and memory capacity classes.\n')
    source_database_instance: typing.Union[models.aws_rds.DatabaseInstanceBaseDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceReadReplicaDef, models.aws_rds.DatabaseInstanceReadReplicaDef] = pydantic.Field(..., description='The source database instance. Each DB instance can have a limited number of read replicas. For more information, see https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/USER_ReadRepl.html.\n')
    storage_encrypted: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is encrypted. Default: - true if storageEncryptionKey has been provided, false otherwise\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key that\'s used to encrypt the DB instance. Default: - default master key if storageEncrypted is true, no key otherwise\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    # source_instance: rds.DatabaseInstance\n\n    rds.DatabaseInstanceFromSnapshot(self, "Instance",\n        snapshot_identifier="my-snapshot",\n        engine=rds.DatabaseInstanceEngine.postgres(version=rds.PostgresEngineVersion.VER_15_2),\n        # optional, defaults to m5.large\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE2, ec2.InstanceSize.LARGE),\n        vpc=vpc\n    )\n    rds.DatabaseInstanceReadReplica(self, "ReadReplica",\n        source_database_instance=source_instance,\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE2, ec2.InstanceSize.LARGE),\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets', 'instance_type', 'source_database_instance', 'storage_encrypted', 'storage_encryption_key']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceReadReplicaProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseInstanceReadReplicaPropsDefConfig] = pydantic.Field(None)


class DatabaseInstanceReadReplicaPropsDefConfig(pydantic.BaseModel):
    instance_type_config: typing.Optional[models.aws_ec2.InstanceTypeDefConfig] = pydantic.Field(None)
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.DatabaseInstanceSourceProps
class DatabaseInstanceSourcePropsDef(BaseStruct):
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC network where the DB subnet group should be created.\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Indicates that minor engine upgrades are applied automatically to the DB instance during the maintenance window. Default: true\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The name of the Availability Zone where the DB instance will be located. Default: - no preference\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Set to zero to disable backups. When creating a read replica, you must enable automatic backups on the source database instance by setting the backup retention to a value other than zero. Default: - Duration.days(1) for source instances, disabled for read replicas\n')
    cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. Default: - no log exports\n')
    cloudwatch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``Infinity``. Default: - logs never expire\n")
    cloudwatch_logs_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - a new role is created.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether to copy all of the user-defined tags from the DB instance to snapshots of the DB instance. Default: true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether automated backups should be deleted or retained when you delete a DB instance. Default: false\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance should have deletion protection enabled. Default: - true if ``removalPolicy`` is RETAIN, false otherwise\n')
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Default: - Do not join domain\n')
    domain_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role to be used when making API calls to the Directory Service. The role needs the AWS-managed policy AmazonRDSDirectoryServiceAccess or equivalent. Default: - The role will be created for you if ``DatabaseInstanceNewProps#domain`` is specified\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    iam_authentication: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. Default: false\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. Default: - a CloudFormation generated name\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. Default: - no provisioned iops if storage type is not specified. For GP3: 3,000 IOPS if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 12,000 IOPS otherwise (except for SQL Server where the default is always 3,000 IOPS).\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='Upper limit to which RDS can scale the storage in GiB(Gibibyte). Default: - No autoscaling of RDS instance\n')
    monitoring_interval: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The interval, in seconds, between points when Amazon RDS collects enhanced monitoring metrics for the DB instance. Default: - no enhanced monitoring\n')
    monitoring_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be used to manage DB instance monitoring. Default: - A role is automatically created for you\n')
    multi_az: typing.Optional[bool] = pydantic.Field(None, description='Specifies if the database instance is a multiple Availability Zone deployment. Default: false\n')
    network_type: typing.Optional[aws_cdk.aws_rds.NetworkType] = pydantic.Field(None, description='The network type of the DB instance. Default: - IPV4\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group to associate with the instance. Default: - no option group\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: - no parameter group\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7 this is the free tier\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port for the instance. Default: - the default port for the chosen engine.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are performed. Constraints: - Must be in the format ``hh24:mi-hh24:mi``. - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range (in UTC) during which system maintenance can occur. Format: ``ddd:hh24:mi-ddd:hh24:mi`` Constraint: Minimum 30-minute window Default: - a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#Concepts.DBMaintenance\n')
    processor_features: typing.Union[models.aws_rds.ProcessorFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The number of CPU cores and the number of threads per core. Default: - the default number of CPU cores and threads per core for the chosen instance class. See https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html#USER_ConfigureProcessor\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The CloudFormation policy to apply when the instance is removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the resource, but retain a snapshot of the data)\n')
    s3_export_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data into. This property must not be used if ``s3ExportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 export. This property must not be used if ``s3ExportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ExportBuckets`` is set, no role is defined otherwise\n')
    s3_import_buckets: typing.Optional[typing.Sequence[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]]] = pydantic.Field(None, description='S3 buckets that you want to load data from. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportRole`` is used. For Microsoft SQL Server: Default: - None\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Role that will be associated with this DB instance to enable S3 import. This feature is only supported by the Microsoft SQL Server, Oracle, and PostgreSQL engines. This property must not be used if ``s3ImportBuckets`` is used. For Microsoft SQL Server: Default: - New role is created if ``s3ImportBuckets`` is set, no role is defined otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups to assign to the DB instance. Default: - a new security group is created\n')
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description='The storage throughput, specified in mebibytes per second (MiBps). Only applicable for GP3. Default: - 125 MiBps if allocated storage is less than 400 GiB for MariaDB, MySQL, and PostgreSQL, less than 200 GiB for Oracle and less than 20 GiB for SQL Server. 500 MiBps otherwise (except for SQL Server where the default is always 125 MiBps).\n')
    storage_type: typing.Optional[aws_cdk.aws_rds.StorageType] = pydantic.Field(None, description='The storage type. Storage types supported are gp2, io1, standard. Default: GP2\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the instance. Default: - a new subnet group will be created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The type of subnets to add to the created DB subnet group. Default: - private subnets\n')
    engine: typing.Union[models.aws_rds.DatabaseInstanceEngineDef] = pydantic.Field(..., description='The database engine.\n')
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The allocated storage size, specified in gibibytes (GiB). Default: 100\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow major version upgrades. Default: false\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The name of the database. Default: - no name\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The name of the compute and memory capacity for the instance. Default: - m5.large (or, more specifically, db.m5.large)\n')
    license_model: typing.Optional[aws_cdk.aws_rds.LicenseModel] = pydantic.Field(None, description='The license model. Default: - RDS default license model\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The time zone of the instance. This is currently supported only by Microsoft Sql Server. Default: - RDS default timezone\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_kms as kms\n    from aws_cdk import aws_logs as logs\n    from aws_cdk import aws_rds as rds\n    from aws_cdk import aws_s3 as s3\n\n    # bucket: s3.Bucket\n    # instance_engine: rds.IInstanceEngine\n    # instance_type: ec2.InstanceType\n    # key: kms.Key\n    # option_group: rds.OptionGroup\n    # parameter_group: rds.ParameterGroup\n    # role: iam.Role\n    # security_group: ec2.SecurityGroup\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # subnet_group: rds.SubnetGroup\n    # vpc: ec2.Vpc\n\n    database_instance_source_props = rds.DatabaseInstanceSourceProps(\n        engine=instance_engine,\n        vpc=vpc,\n\n        # the properties below are optional\n        allocated_storage=123,\n        allow_major_version_upgrade=False,\n        auto_minor_version_upgrade=False,\n        availability_zone="availabilityZone",\n        backup_retention=cdk.Duration.minutes(30),\n        cloudwatch_logs_exports=["cloudwatchLogsExports"],\n        cloudwatch_logs_retention=logs.RetentionDays.ONE_DAY,\n        cloudwatch_logs_retention_role=role,\n        copy_tags_to_snapshot=False,\n        database_name="databaseName",\n        delete_automated_backups=False,\n        deletion_protection=False,\n        domain="domain",\n        domain_role=role,\n        enable_performance_insights=False,\n        iam_authentication=False,\n        instance_identifier="instanceIdentifier",\n        instance_type=instance_type,\n        iops=123,\n        license_model=rds.LicenseModel.LICENSE_INCLUDED,\n        max_allocated_storage=123,\n        monitoring_interval=cdk.Duration.minutes(30),\n        monitoring_role=role,\n        multi_az=False,\n        network_type=rds.NetworkType.IPV4,\n        option_group=option_group,\n        parameter_group=parameter_group,\n        parameters={\n            "parameters_key": "parameters"\n        },\n        performance_insight_encryption_key=key,\n        performance_insight_retention=rds.PerformanceInsightRetention.DEFAULT,\n        port=123,\n        preferred_backup_window="preferredBackupWindow",\n        preferred_maintenance_window="preferredMaintenanceWindow",\n        processor_features=rds.ProcessorFeatures(\n            core_count=123,\n            threads_per_core=123\n        ),\n        publicly_accessible=False,\n        removal_policy=cdk.RemovalPolicy.DESTROY,\n        s3_export_buckets=[bucket],\n        s3_export_role=role,\n        s3_import_buckets=[bucket],\n        s3_import_role=role,\n        security_groups=[security_group],\n        storage_throughput=123,\n        storage_type=rds.StorageType.STANDARD,\n        subnet_group=subnet_group,\n        timezone="timezone",\n        vpc_subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention', 'cloudwatch_logs_exports', 'cloudwatch_logs_retention', 'cloudwatch_logs_retention_role', 'copy_tags_to_snapshot', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_role', 'enable_performance_insights', 'iam_authentication', 'instance_identifier', 'iops', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role', 'multi_az', 'network_type', 'option_group', 'parameter_group', 'performance_insight_encryption_key', 'performance_insight_retention', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'publicly_accessible', 'removal_policy', 's3_export_buckets', 's3_export_role', 's3_import_buckets', 's3_import_role', 'security_groups', 'storage_throughput', 'storage_type', 'subnet_group', 'vpc_subnets', 'engine', 'allocated_storage', 'allow_major_version_upgrade', 'database_name', 'instance_type', 'license_model', 'parameters', 'timezone']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseInstanceSourceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseInstanceSourcePropsDefConfig] = pydantic.Field(None)


class DatabaseInstanceSourcePropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.DatabaseProxyAttributes
class DatabaseProxyAttributesDef(BaseStruct):
    db_proxy_arn: str = pydantic.Field(..., description='DB Proxy ARN.\n')
    db_proxy_name: str = pydantic.Field(..., description='DB Proxy Name.\n')
    endpoint: str = pydantic.Field(..., description='Endpoint.\n')
    security_groups: typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(..., description='The security groups of the instance.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_rds as rds\n\n    # security_group: ec2.SecurityGroup\n\n    database_proxy_attributes = rds.DatabaseProxyAttributes(\n        db_proxy_arn="dbProxyArn",\n        db_proxy_name="dbProxyName",\n        endpoint="endpoint",\n        security_groups=[security_group]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['db_proxy_arn', 'db_proxy_name', 'endpoint', 'security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseProxyAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.DatabaseProxyOptions
class DatabaseProxyOptionsDef(BaseStruct):
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n    # security_group: ec2.SecurityGroup\n    # secrets: List[secretsmanager.Secret[]]\n    # db_instance: rds.DatabaseInstance\n\n\n    proxy = db_instance.add_proxy("proxy",\n        borrow_timeout=Duration.seconds(30),\n        max_connections_percent=50,\n        secrets=secrets,\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['secrets', 'vpc', 'borrow_timeout', 'db_proxy_name', 'debug_logging', 'iam_auth', 'idle_client_timeout', 'init_query', 'max_connections_percent', 'max_idle_connections_percent', 'require_tls', 'role', 'security_groups', 'session_pinning_filters', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseProxyOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseProxyOptionsDefConfig] = pydantic.Field(None)


class DatabaseProxyOptionsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.DatabaseProxyProps
class DatabaseProxyPropsDef(BaseStruct):
    secrets: typing.Sequence[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(..., description='The secret that the proxy uses to authenticate to the RDS DB instance or Aurora DB cluster. These secrets are stored within Amazon Secrets Manager. One or more secrets are required.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to associate with the new proxy.\n')
    borrow_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The duration for a proxy to wait for a connection to become available in the connection pool. Only applies when the proxy has opened its maximum number of connections and all connections are busy with client sessions. Value must be between 1 second and 1 hour, or ``Duration.seconds(0)`` to represent unlimited. Default: cdk.Duration.seconds(120)\n')
    db_proxy_name: typing.Optional[str] = pydantic.Field(None, description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens. Default: - Generated by CloudFormation (recommended)\n")
    debug_logging: typing.Optional[bool] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs. Default: false\n')
    iam_auth: typing.Optional[bool] = pydantic.Field(None, description='Whether to require or disallow AWS Identity and Access Management (IAM) authentication for connections to the proxy. Default: false\n')
    idle_client_timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database. Default: cdk.Duration.minutes(30)\n')
    init_query: typing.Optional[str] = pydantic.Field(None, description='One or more SQL statements for the proxy to run when opening each new database connection. Typically used with SET statements to make sure that each connection has identical settings such as time zone and character set. For multiple statements, use semicolons as the separator. You can also include multiple variables in a single SET statement, such as SET x=1, y=2. not currently supported for PostgreSQL. Default: - no initialization query\n')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the connection pool for each target in a target group. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. 1-100 Default: 100\n')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='Controls how actively the proxy closes idle database connections in the connection pool. A high value enables the proxy to leave a high percentage of idle connections open. A low value causes the proxy to close idle client connections and return the underlying database connections to the connection pool. For Aurora MySQL, it is expressed as a percentage of the max_connections setting for the RDS DB instance or Aurora DB cluster used by the target group. between 0 and MaxConnectionsPercent Default: 50\n')
    require_tls: typing.Optional[bool] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy. Default: true\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='IAM role that the proxy uses to access secrets in AWS Secrets Manager. Default: - A role will automatically be created\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='One or more VPC security groups to associate with the new proxy. Default: - No security groups\n')
    session_pinning_filters: typing.Optional[typing.Sequence[models.aws_rds.SessionPinningFilterDef]] = pydantic.Field(None, description='Each item in the list represents a class of SQL operations that normally cause all later statements in a session using a proxy to be pinned to the same underlying database connection. Including an item in the list exempts that class of SQL operations from the pinning behavior. Default: - no session pinning filters\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The subnets used by the proxy. Default: - the VPC default strategy if not specified.\n')
    proxy_target: models.aws_rds.ProxyTargetDef = pydantic.Field(..., description='DB proxy target: Instance or Cluster.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    cluster = rds.DatabaseCluster(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora_mysql(\n            version=rds.AuroraMysqlEngineVersion.VER_3_03_0\n        ),\n        writer=rds.ClusterInstance.provisioned("writer"),\n        vpc=vpc\n    )\n\n    proxy = rds.DatabaseProxy(self, "Proxy",\n        proxy_target=rds.ProxyTarget.from_cluster(cluster),\n        secrets=[cluster.secret],\n        vpc=vpc\n    )\n\n    role = iam.Role(self, "DBProxyRole", assumed_by=iam.AccountPrincipal(self.account))\n    proxy.grant_connect(role, "admin")\n')
    _init_params: typing.ClassVar[list[str]] = ['secrets', 'vpc', 'borrow_timeout', 'db_proxy_name', 'debug_logging', 'iam_auth', 'idle_client_timeout', 'init_query', 'max_connections_percent', 'max_idle_connections_percent', 'require_tls', 'role', 'security_groups', 'session_pinning_filters', 'vpc_subnets', 'proxy_target']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseProxyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[DatabaseProxyPropsDefConfig] = pydantic.Field(None)


class DatabaseProxyPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.DatabaseSecretProps
class DatabaseSecretPropsDef(BaseStruct):
    username: str = pydantic.Field(..., description='The username.\n')
    dbname: typing.Optional[str] = pydantic.Field(None, description='The database name, if not using the default one. Default: - whatever the secret generates after the attach method is run\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key to use to encrypt the secret. Default: default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Characters to not include in the generated password. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    master_secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The master secret which will be used to rotate this secret. Default: - no master secret information will be included\n')
    replace_on_password_criteria_changes: typing.Optional[bool] = pydantic.Field(None, description='Whether to replace this secret when the criteria for the password change. This is achieved by overriding the logical id of the AWS::SecretsManager::Secret with a hash of the options that influence the password generation. This way a new secret will be created when the password is regenerated and the cluster or instance consuming this secret will have its credentials updated. Default: false\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated\n')
    secret_name: typing.Optional[str] = pydantic.Field(None, description='A name for the secret. Default: - A name is generated by CloudFormation.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Build a data source for AppSync to access the database.\n    # api: appsync.GraphqlApi\n    # Create username and password secret for DB Cluster\n    secret = rds.DatabaseSecret(self, "AuroraSecret",\n        username="clusteradmin"\n    )\n\n    # The VPC to place the cluster in\n    vpc = ec2.Vpc(self, "AuroraVpc")\n\n    # Create the serverless cluster, provide all values needed to customise the database.\n    cluster = rds.ServerlessCluster(self, "AuroraCluster",\n        engine=rds.DatabaseClusterEngine.AURORA_MYSQL,\n        vpc=vpc,\n        credentials={"username": "clusteradmin"},\n        cluster_identifier="db-endpoint-test",\n        default_database_name="demos"\n    )\n    rds_dS = api.add_rds_data_source("rds", cluster, secret, "demos")\n\n    # Set up a resolver for an RDS query.\n    rds_dS.create_resolver("QueryGetDemosRdsResolver",\n        type_name="Query",\n        field_name="getDemosRds",\n        request_mapping_template=appsync.MappingTemplate.from_string("""\n              {\n                "version": "2018-05-29",\n                "statements": [\n                  "SELECT * FROM demos"\n                ]\n              }\n              """),\n        response_mapping_template=appsync.MappingTemplate.from_string("""\n                $utils.toJson($utils.rds.toJsonObject($ctx.result)[0])\n              """)\n    )\n\n    # Set up a resolver for an RDS mutation.\n    rds_dS.create_resolver("MutationAddDemoRdsResolver",\n        type_name="Mutation",\n        field_name="addDemoRds",\n        request_mapping_template=appsync.MappingTemplate.from_string("""\n              {\n                "version": "2018-05-29",\n                "statements": [\n                  "INSERT INTO demos VALUES (:id, :version)",\n                  "SELECT * WHERE id = :id"\n                ],\n                "variableMap": {\n                  ":id": $util.toJson($util.autoId()),\n                  ":version": $util.toJson($ctx.args.version)\n                }\n              }\n              """),\n        response_mapping_template=appsync.MappingTemplate.from_string("""\n                $utils.toJson($utils.rds.toJsonObject($ctx.result)[1][0])\n              """)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['username', 'dbname', 'encryption_key', 'exclude_characters', 'master_secret', 'replace_on_password_criteria_changes', 'replica_regions', 'secret_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.DatabaseSecretProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.EngineVersion
class EngineVersionDef(BaseStruct):
    major_version: str = pydantic.Field(..., description='The major version of the engine, for example, "5.6". Used in specifying the ParameterGroup family and OptionGroup version for this engine.\n')
    full_version: typing.Optional[str] = pydantic.Field(None, description='The full version string of the engine, for example, "5.6.mysql_aurora.1.22.1". It can be undefined, which means RDS should use whatever version it deems appropriate for the given engine type. Default: - no version specified\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    engine_version = rds.EngineVersion(\n        major_version="majorVersion",\n\n        # the properties below are optional\n        full_version="fullVersion"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['major_version', 'full_version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.EngineVersion'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.InstanceEngineBindOptions
class InstanceEngineBindOptionsDef(BaseStruct):
    domain: typing.Optional[str] = pydantic.Field(None, description="The Active Directory directory ID to create the DB instance in. Default: - none (it's an optional field)\n")
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='The option group of the database. Default: - none\n')
    s3_export_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 exporting. Default: - none\n')
    s3_import_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role used for S3 importing. Default: - none\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The timezone of the database, set by the customer. Default: - none (it\'s an optional field)\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_rds as rds\n\n    # option_group: rds.OptionGroup\n    # role: iam.Role\n\n    instance_engine_bind_options = rds.InstanceEngineBindOptions(\n        domain="domain",\n        option_group=option_group,\n        s3_export_role=role,\n        s3_import_role=role,\n        timezone="timezone"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['domain', 'option_group', 's3_export_role', 's3_import_role', 'timezone']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.InstanceEngineBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.InstanceEngineConfig
class InstanceEngineConfigDef(BaseStruct):
    features: typing.Union[models.aws_rds.InstanceEngineFeaturesDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Features supported by the database engine. Default: - no features\n')
    option_group: typing.Optional[typing.Union[models.aws_rds.OptionGroupDef]] = pydantic.Field(None, description='Option group of the database. Default: - none\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # option_group: rds.OptionGroup\n\n    instance_engine_config = rds.InstanceEngineConfig(\n        features=rds.InstanceEngineFeatures(\n            s3_export="s3Export",\n            s3_import="s3Import"\n        ),\n        option_group=option_group\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['features', 'option_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.InstanceEngineConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.InstanceEngineFeatures
class InstanceEngineFeaturesDef(BaseStruct):
    s3_export: typing.Optional[str] = pydantic.Field(None, description='Feature name for the DB instance that the IAM role to export to S3 bucket is to be associated with. Default: - no s3Export feature name\n')
    s3_import: typing.Optional[str] = pydantic.Field(None, description='Feature name for the DB instance that the IAM role to access the S3 bucket for import is to be associated with. Default: - no s3Import feature name\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    instance_engine_features = rds.InstanceEngineFeatures(\n        s3_export="s3Export",\n        s3_import="s3Import"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_export', 's3_import']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.InstanceEngineFeatures'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.InstanceProps
class InstancePropsDef(BaseStruct):
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='What subnets to run the RDS instances in. Must be at least 2 subnets in two different AZs.\n')
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow upgrade of major version for the DB instance. Default: - false\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable automatic upgrade of minor version for the DB instance. Default: - true\n')
    delete_automated_backups: typing.Optional[bool] = pydantic.Field(None, description='Whether to remove automated backups immediately after the DB instance is deleted for the DB instance. Default: - true\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='What type of instance to start for the replicas. Default: - t3.medium (or, more precisely, db.t3.medium)\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. Default: no parameter group\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - ``true`` if ``vpcSubnets`` is ``subnetType: SubnetType.PUBLIC``, ``false`` otherwise\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: a new security group is created.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. Default: - the Vpc default strategy if not specified.\n\n:exampleMetadata: lit=aws-rds/test/integ.cluster-rotation.lit.ts infused\n\nExample::\n\n    cluster = rds.DatabaseCluster(stack, "Database",\n        engine=rds.DatabaseClusterEngine.AURORA,\n        instance_props=cdk.aws_rds.InstanceProps(\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.SMALL),\n            vpc=vpc\n        )\n    )\n\n    cluster.add_rotation_single_user()\n\n    cluster_with_custom_rotation_options = rds.DatabaseCluster(stack, "CustomRotationOptions",\n        engine=rds.DatabaseClusterEngine.AURORA,\n        instance_props=cdk.aws_rds.InstanceProps(\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.SMALL),\n            vpc=vpc\n        )\n    )\n    cluster_with_custom_rotation_options.add_rotation_single_user(\n        automatically_after=cdk.Duration.days(7),\n        exclude_characters="!@#$%^&*",\n        security_group=security_group,\n        vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS),\n        endpoint=endpoint\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['vpc', 'allow_major_version_upgrade', 'auto_minor_version_upgrade', 'delete_automated_backups', 'enable_performance_insights', 'instance_type', 'parameter_group', 'parameters', 'performance_insight_encryption_key', 'performance_insight_retention', 'publicly_accessible', 'security_groups', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.InstanceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[InstancePropsDefConfig] = pydantic.Field(None)


class InstancePropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.MariaDbInstanceEngineProps
class MariaDbInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.MariaDbEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # maria_db_engine_version: rds.MariaDbEngineVersion\n\n    maria_db_instance_engine_props = rds.MariaDbInstanceEngineProps(\n        version=maria_db_engine_version\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.MariaDbInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[MariaDbInstanceEnginePropsDefConfig] = pydantic.Field(None)


class MariaDbInstanceEnginePropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models.aws_rds.MariaDbEngineVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.MySqlInstanceEngineProps
class MySqlInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.MysqlEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n\n    iops_instance = rds.DatabaseInstance(self, "IopsInstance",\n        engine=rds.DatabaseInstanceEngine.mysql(version=rds.MysqlEngineVersion.VER_8_0_30),\n        vpc=vpc,\n        storage_type=rds.StorageType.IO1,\n        iops=5000\n    )\n\n    gp3_instance = rds.DatabaseInstance(self, "Gp3Instance",\n        engine=rds.DatabaseInstanceEngine.mysql(version=rds.MysqlEngineVersion.VER_8_0_30),\n        vpc=vpc,\n        allocated_storage=500,\n        storage_type=rds.StorageType.GP3,\n        storage_throughput=500\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.MySqlInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.OptionConfiguration
class OptionConfigurationDef(BaseStruct):
    name: str = pydantic.Field(..., description='The name of the option.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number that this option uses. If ``port`` is specified then ``vpc`` must also be specified. Default: - no port\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Optional list of security groups to use for this option, if ``vpc`` is specified. If no groups are provided, a default one will be created. Default: - a default group will be created if ``port`` or ``vpc`` are specified.\n')
    settings: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The settings for the option. Default: - no settings\n')
    version: typing.Optional[str] = pydantic.Field(None, description='The version for the option. Default: - no version\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC where a security group should be created for this option. If ``vpc`` is specified then ``port`` must also be specified. Default: - no VPC\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_rds as rds\n\n    # security_group: ec2.SecurityGroup\n    # vpc: ec2.Vpc\n\n    option_configuration = rds.OptionConfiguration(\n        name="name",\n\n        # the properties below are optional\n        port=123,\n        security_groups=[security_group],\n        settings={\n            "settings_key": "settings"\n        },\n        version="version",\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'port', 'security_groups', 'settings', 'version', 'vpc']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OptionConfiguration'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.OptionGroupProps
class OptionGroupPropsDef(BaseStruct):
    configurations: typing.Sequence[typing.Union[models.aws_rds.OptionConfigurationDef, dict[str, typing.Any]]] = pydantic.Field(..., description='The configurations for this option group.\n')
    engine: typing.Union[models.aws_rds.DatabaseInstanceEngineDef] = pydantic.Field(..., description='The database engine that this option group is associated with.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the option group. Default: a CDK generated description\n\n:exampleMetadata: lit=aws-rds/test/integ.instance.lit.ts infused\n\nExample::\n\n    # Set open cursors with parameter group\n    parameter_group = rds.ParameterGroup(self, "ParameterGroup",\n        engine=rds.DatabaseInstanceEngine.oracle_se2(version=rds.OracleEngineVersion.VER_19_0_0_0_2020_04_R1),\n        parameters={\n            "open_cursors": "2500"\n        }\n    )\n\n    option_group = rds.OptionGroup(self, "OptionGroup",\n        engine=rds.DatabaseInstanceEngine.oracle_se2(version=rds.OracleEngineVersion.VER_19_0_0_0_2020_04_R1),\n        configurations=[cdk.aws_rds.OptionConfiguration(\n            name="LOCATOR"\n        ), cdk.aws_rds.OptionConfiguration(\n            name="OEM",\n            port=1158,\n            vpc=vpc\n        )\n        ]\n    )\n\n    # Allow connections to OEM\n    option_group.option_connections.OEM.connections.allow_default_port_from_any_ipv4()\n\n    # Database instance with production values\n    instance = rds.DatabaseInstance(self, "Instance",\n        engine=rds.DatabaseInstanceEngine.oracle_se2(version=rds.OracleEngineVersion.VER_19_0_0_0_2020_04_R1),\n        license_model=rds.LicenseModel.BRING_YOUR_OWN_LICENSE,\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.MEDIUM),\n        multi_az=True,\n        storage_type=rds.StorageType.IO1,\n        credentials=rds.Credentials.from_username("syscdk"),\n        vpc=vpc,\n        database_name="ORCL",\n        storage_encrypted=True,\n        backup_retention=cdk.Duration.days(7),\n        monitoring_interval=cdk.Duration.seconds(60),\n        enable_performance_insights=True,\n        cloudwatch_logs_exports=["trace", "audit", "alert", "listener"\n        ],\n        cloudwatch_logs_retention=logs.RetentionDays.ONE_MONTH,\n        auto_minor_version_upgrade=True,  # required to be true if LOCATOR is used in the option group\n        option_group=option_group,\n        parameter_group=parameter_group,\n        removal_policy=RemovalPolicy.DESTROY\n    )\n\n    # Allow connections on default port from any IPV4\n    instance.connections.allow_default_port_from_any_ipv4()\n\n    # Rotate the master user password every 30 days\n    instance.add_rotation_single_user()\n\n    # Add alarm for high CPU\n    cloudwatch.Alarm(self, "HighCPU",\n        metric=instance.metric_cPUUtilization(),\n        threshold=90,\n        evaluation_periods=1\n    )\n\n    # Trigger Lambda function on instance availability events\n    fn = lambda_.Function(self, "Function",\n        code=lambda_.Code.from_inline("exports.handler = (event) => console.log(event);"),\n        handler="index.handler",\n        runtime=lambda_.Runtime.NODEJS_14_X\n    )\n\n    availability_rule = instance.on_event("Availability", target=targets.LambdaFunction(fn))\n    availability_rule.add_event_pattern(\n        detail={\n            "EventCategories": ["availability"\n            ]\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['configurations', 'engine', 'description']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OptionGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[OptionGroupPropsDefConfig] = pydantic.Field(None)


class OptionGroupPropsDefConfig(pydantic.BaseModel):
    engine_config: typing.Optional[models._interface_methods.AwsRdsIInstanceEngineDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.OracleEeCdbInstanceEngineProps
class OracleEeCdbInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # oracle_engine_version: rds.OracleEngineVersion\n\n    oracle_ee_cdb_instance_engine_props = rds.OracleEeCdbInstanceEngineProps(\n        version=oracle_engine_version\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OracleEeCdbInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.OracleEeInstanceEngineProps
class OracleEeInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # oracle_engine_version: rds.OracleEngineVersion\n\n    oracle_ee_instance_engine_props = rds.OracleEeInstanceEngineProps(\n        version=oracle_engine_version\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OracleEeInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.OracleSe2CdbInstanceEngineProps
class OracleSe2CdbInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # oracle_engine_version: rds.OracleEngineVersion\n\n    oracle_se2_cdb_instance_engine_props = rds.OracleSe2CdbInstanceEngineProps(\n        version=oracle_engine_version\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OracleSe2CdbInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[OracleSe2CdbInstanceEnginePropsDefConfig] = pydantic.Field(None)


class OracleSe2CdbInstanceEnginePropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models.aws_rds.OracleEngineVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.OracleSe2InstanceEngineProps
class OracleSe2InstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.OracleEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    instance = rds.DatabaseInstance(self, "Instance",\n        engine=rds.DatabaseInstanceEngine.oracle_se2(version=rds.OracleEngineVersion.VER_19_0_0_0_2020_04_R1),\n        # optional, defaults to m5.large\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.SMALL),\n        credentials=rds.Credentials.from_generated_secret("syscdk"),  # Optional - will default to \'admin\' username and generated password\n        vpc=vpc,\n        vpc_subnets=ec2.SubnetSelection(\n            subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.OracleSe2InstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[OracleSe2InstanceEnginePropsDefConfig] = pydantic.Field(None)


class OracleSe2InstanceEnginePropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models.aws_rds.OracleEngineVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.ParameterGroupClusterBindOptions
class ParameterGroupClusterBindOptionsDef(BaseStruct):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ParameterGroupClusterBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ParameterGroupClusterConfig
class ParameterGroupClusterConfigDef(BaseStruct):
    parameter_group_name: str = pydantic.Field(..., description='The name of this parameter group.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    parameter_group_cluster_config = rds.ParameterGroupClusterConfig(\n        parameter_group_name="parameterGroupName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['parameter_group_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ParameterGroupClusterConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ParameterGroupInstanceBindOptions
class ParameterGroupInstanceBindOptionsDef(BaseStruct):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ParameterGroupInstanceBindOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ParameterGroupInstanceConfig
class ParameterGroupInstanceConfigDef(BaseStruct):
    parameter_group_name: str = pydantic.Field(..., description='The name of this parameter group.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    parameter_group_instance_config = rds.ParameterGroupInstanceConfig(\n        parameter_group_name="parameterGroupName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['parameter_group_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ParameterGroupInstanceConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ParameterGroupProps
class ParameterGroupPropsDef(BaseStruct):
    engine: models.UnsupportedResource = pydantic.Field(..., description='The database engine for this parameter group.\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for this parameter group. Default: a CDK generated description\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in this parameter group. Default: - None\n\n:exampleMetadata: lit=aws-rds/test/integ.instance.lit.ts infused\n\nExample::\n\n    # Set open cursors with parameter group\n    parameter_group = rds.ParameterGroup(self, "ParameterGroup",\n        engine=rds.DatabaseInstanceEngine.oracle_se2(version=rds.OracleEngineVersion.VER_19_0_0_0_2020_04_R1),\n        parameters={\n            "open_cursors": "2500"\n        }\n    )\n\n    option_group = rds.OptionGroup(self, "OptionGroup",\n        engine=rds.DatabaseInstanceEngine.oracle_se2(version=rds.OracleEngineVersion.VER_19_0_0_0_2020_04_R1),\n        configurations=[cdk.aws_rds.OptionConfiguration(\n            name="LOCATOR"\n        ), cdk.aws_rds.OptionConfiguration(\n            name="OEM",\n            port=1158,\n            vpc=vpc\n        )\n        ]\n    )\n\n    # Allow connections to OEM\n    option_group.option_connections.OEM.connections.allow_default_port_from_any_ipv4()\n\n    # Database instance with production values\n    instance = rds.DatabaseInstance(self, "Instance",\n        engine=rds.DatabaseInstanceEngine.oracle_se2(version=rds.OracleEngineVersion.VER_19_0_0_0_2020_04_R1),\n        license_model=rds.LicenseModel.BRING_YOUR_OWN_LICENSE,\n        instance_type=ec2.InstanceType.of(ec2.InstanceClass.BURSTABLE3, ec2.InstanceSize.MEDIUM),\n        multi_az=True,\n        storage_type=rds.StorageType.IO1,\n        credentials=rds.Credentials.from_username("syscdk"),\n        vpc=vpc,\n        database_name="ORCL",\n        storage_encrypted=True,\n        backup_retention=cdk.Duration.days(7),\n        monitoring_interval=cdk.Duration.seconds(60),\n        enable_performance_insights=True,\n        cloudwatch_logs_exports=["trace", "audit", "alert", "listener"\n        ],\n        cloudwatch_logs_retention=logs.RetentionDays.ONE_MONTH,\n        auto_minor_version_upgrade=True,  # required to be true if LOCATOR is used in the option group\n        option_group=option_group,\n        parameter_group=parameter_group,\n        removal_policy=RemovalPolicy.DESTROY\n    )\n\n    # Allow connections on default port from any IPV4\n    instance.connections.allow_default_port_from_any_ipv4()\n\n    # Rotate the master user password every 30 days\n    instance.add_rotation_single_user()\n\n    # Add alarm for high CPU\n    cloudwatch.Alarm(self, "HighCPU",\n        metric=instance.metric_cPUUtilization(),\n        threshold=90,\n        evaluation_periods=1\n    )\n\n    # Trigger Lambda function on instance availability events\n    fn = lambda_.Function(self, "Function",\n        code=lambda_.Code.from_inline("exports.handler = (event) => console.log(event);"),\n        handler="index.handler",\n        runtime=lambda_.Runtime.NODEJS_14_X\n    )\n\n    availability_rule = instance.on_event("Availability", target=targets.LambdaFunction(fn))\n    availability_rule.add_event_pattern(\n        detail={\n            "EventCategories": ["availability"\n            ]\n        }\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'description', 'parameters']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ParameterGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.PostgresEngineFeatures
class PostgresEngineFeaturesDef(BaseStruct):
    s3_export: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Postgres engine supports the S3 data export feature. Default: false\n')
    s3_import: typing.Optional[bool] = pydantic.Field(None, description='Whether this version of the Postgres engine supports the S3 data import feature. Default: false\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    postgres_engine_features = rds.PostgresEngineFeatures(\n        s3_export=False,\n        s3_import=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['s3_export', 's3_import']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.PostgresEngineFeatures'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.PostgresInstanceEngineProps
class PostgresInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.PostgresEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    engine = rds.DatabaseInstanceEngine.postgres(version=rds.PostgresEngineVersion.VER_15_2)\n    my_key = kms.Key(self, "MyKey")\n\n    rds.DatabaseInstance(self, "InstanceWithCustomizedSecret",\n        engine=engine,\n        vpc=vpc,\n        credentials=rds.Credentials.from_generated_secret("postgres",\n            secret_name="my-cool-name",\n            encryption_key=my_key,\n            exclude_characters="!&*^#@()",\n            replica_regions=[secretsmanager.ReplicaRegion(region="eu-west-1"), secretsmanager.ReplicaRegion(region="eu-west-2")]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.PostgresInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[PostgresInstanceEnginePropsDefConfig] = pydantic.Field(None)


class PostgresInstanceEnginePropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models.aws_rds.PostgresEngineVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.ProcessorFeatures
class ProcessorFeaturesDef(BaseStruct):
    core_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of CPU core. Default: - the default number of CPU cores for the chosen instance class.\n')
    threads_per_core: typing.Union[int, float, None] = pydantic.Field(None, description='The number of threads per core. Default: - the default number of threads per core for the chosen instance class.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    processor_features = rds.ProcessorFeatures(\n        core_count=123,\n        threads_per_core=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['core_count', 'threads_per_core']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ProcessorFeatures'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ProvisionedClusterInstanceProps
class ProvisionedClusterInstancePropsDef(BaseStruct):
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow upgrade of major version for the DB instance. Default: - false\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable automatic upgrade of minor version for the DB instance. Default: - true\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier for the database instance. Default: - CloudFormation generated identifier\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. This is only needed if you need to configure different parameter groups for each individual instance, otherwise you should not provide this and just use the cluster parameter group Default: the cluster parameter group is used\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - true if the instance is placed in a public subnet\n')
    instance_type: typing.Optional[models.aws_ec2.InstanceTypeDef] = pydantic.Field(None, description='The cluster instance type. Default: db.t3.medium\n')
    is_from_legacy_instance_props: typing.Optional[bool] = pydantic.Field(None, description='Only used for migrating existing clusters from using ``instanceProps`` to ``writer`` and ``readers``. Default: false\n')
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description='The promotion tier of the cluster instance. Can be between 0-15 For provisioned instances this just determines the failover priority. If multiple instances have the same priority then one will be picked at random Default: 2\n\n:exampleMetadata: infused\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    # vpc: ec2.Vpc\n\n    cluster = rds.DatabaseCluster(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora_mysql(version=rds.AuroraMysqlEngineVersion.VER_2_08_1),\n        credentials=rds.Credentials.from_generated_secret("clusteradmin"),  # Optional - will default to \'admin\' username and generated password\n        writer=rds.ClusterInstance.provisioned("writer",\n            readers=[\n                rds.ClusterInstance.provisioned("reader1", promotion_tier=1),\n                rds.ClusterInstance.serverless_v2("reader2")\n            ],\n            vpc_subnets={\n                "subnet_type": ec2.SubnetType.PRIVATE_WITH_EGRESS\n            },\n            vpc=vpc\n        )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_major_version_upgrade', 'auto_minor_version_upgrade', 'enable_performance_insights', 'instance_identifier', 'parameter_group', 'parameters', 'performance_insight_encryption_key', 'performance_insight_retention', 'publicly_accessible', 'instance_type', 'is_from_legacy_instance_props', 'promotion_tier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ProvisionedClusterInstanceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ProxyTargetConfig
class ProxyTargetConfigDef(BaseStruct):
    engine_family: str = pydantic.Field(..., description='The engine family of the database instance or cluster this proxy connects with.\n')
    db_clusters: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.DatabaseClusterBaseDef, models.aws_rds.DatabaseClusterDef, models.aws_rds.DatabaseClusterFromSnapshotDef]]] = pydantic.Field(None, description='The database clusters to which this proxy connects. Either this or ``dbInstances`` will be set and the other ``undefined``. Default: - ``undefined`` if ``dbInstances`` is set.\n')
    db_instances: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.DatabaseInstanceBaseDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceFromSnapshotDef, models.aws_rds.DatabaseInstanceReadReplicaDef, models.aws_rds.DatabaseInstanceReadReplicaDef]]] = pydantic.Field(None, description='The database instances to which this proxy connects. Either this or ``dbClusters`` will be set and the other ``undefined``. Default: - ``undefined`` if ``dbClusters`` is set.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # database_cluster: rds.DatabaseCluster\n    # database_instance: rds.DatabaseInstance\n\n    proxy_target_config = rds.ProxyTargetConfig(\n        engine_family="engineFamily",\n\n        # the properties below are optional\n        db_clusters=[database_cluster],\n        db_instances=[database_instance]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine_family', 'db_clusters', 'db_instances']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ProxyTargetConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.RotationMultiUserOptions
class RotationMultiUserOptionsDef(BaseStruct):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster\n')
    secret: typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef] = pydantic.Field(..., description='The secret to rotate. It must be a JSON string with the following format:: { "engine": <required: database engine>, "host": <required: instance host name>, "username": <required: username>, "password": <required: password>, "dbname": <optional: database name>, "port": <optional: if not specified, default port will be used>, "masterarn": <required: the arn of the master secret which will be used to create users/change passwords> }\n\n:exampleMetadata: infused\n\nExample::\n\n    # instance: rds.DatabaseInstance\n    # my_imported_secret: rds.DatabaseSecret\n\n    instance.add_rotation_multi_user("MyUser",\n        secret=my_imported_secret\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['automatically_after', 'endpoint', 'exclude_characters', 'security_group', 'vpc_subnets', 'secret']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.RotationMultiUserOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[RotationMultiUserOptionsDefConfig] = pydantic.Field(None)


class RotationMultiUserOptionsDefConfig(pydantic.BaseModel):
    secret_config: typing.Optional[models._interface_methods.AwsSecretsmanagerISecretDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.RotationSingleUserOptions
class RotationSingleUserOptionsDef(BaseStruct):
    automatically_after: typing.Optional[models.DurationDef] = pydantic.Field(None, description='Specifies the number of days after the previous rotation before Secrets Manager triggers the next automatic rotation. Default: - 30 days\n')
    endpoint: typing.Optional[typing.Union[models.aws_ec2.InterfaceVpcEndpointDef]] = pydantic.Field(None, description="The VPC interface endpoint to use for the Secrets Manager API. If you enable private DNS hostnames for your VPC private endpoint (the default), you don't need to specify an endpoint. The standard Secrets Manager DNS hostname the Secrets Manager CLI and SDKs use by default (https://secretsmanager..amazonaws.com) automatically resolves to your VPC endpoint. Default: https://secretsmanager..amazonaws.com\n")
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='Specifies characters to not include in generated passwords. Default: " %+~`#$&*()|[]{}:;<>?!\'/@"\\"\n')
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group for the Lambda rotation function. Default: - a new security group is created\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the rotation Lambda function. Default: - same placement as instance or cluster\n\n:exampleMetadata: infused\n\nExample::\n\n    # instance: rds.DatabaseInstance\n    # my_endpoint: ec2.InterfaceVpcEndpoint\n\n\n    instance.add_rotation_single_user(\n        vpc_subnets=ec2.SubnetSelection(subnet_type=ec2.SubnetType.PRIVATE_WITH_EGRESS),  # Place rotation Lambda in private subnets\n        endpoint=my_endpoint\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['automatically_after', 'endpoint', 'exclude_characters', 'security_group', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.RotationSingleUserOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ServerlessClusterAttributes
class ServerlessClusterAttributesDef(BaseStruct):
    cluster_identifier: str = pydantic.Field(..., description='Identifier for the cluster.\n')
    cluster_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Cluster endpoint address. Default: - no endpoint address\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The database port. Default: - none\n')
    reader_endpoint_address: typing.Optional[str] = pydantic.Field(None, description='Reader endpoint address. Default: - no reader address\n')
    secret: typing.Optional[typing.Union[models.aws_docdb.DatabaseSecretDef, models.aws_rds.DatabaseSecretDef, models.aws_secretsmanager.SecretDef, models.aws_secretsmanager.SecretTargetAttachmentDef]] = pydantic.Field(None, description='The secret attached to the database cluster. Default: - no secret\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='The security groups of the database cluster. Default: - no security groups\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_rds as rds\n    from aws_cdk import aws_secretsmanager as secretsmanager\n\n    # secret: secretsmanager.Secret\n    # security_group: ec2.SecurityGroup\n\n    serverless_cluster_attributes = rds.ServerlessClusterAttributes(\n        cluster_identifier="clusterIdentifier",\n\n        # the properties below are optional\n        cluster_endpoint_address="clusterEndpointAddress",\n        port=123,\n        reader_endpoint_address="readerEndpointAddress",\n        secret=secret,\n        security_groups=[security_group]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['cluster_identifier', 'cluster_endpoint_address', 'port', 'reader_endpoint_address', 'secret', 'security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ServerlessClusterAttributes'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ServerlessClusterFromSnapshotProps
class ServerlessClusterFromSnapshotPropsDef(BaseStruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    snapshot_identifier: str = pydantic.Field(..., description='The identifier for the DB instance snapshot or DB cluster snapshot to restore from. You can use either the name or the Amazon Resource Name (ARN) to specify a DB cluster snapshot. However, you can use only the ARN to specify a DB instance snapshot.\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Automatic backup retention cannot be disabled on serverless clusters. Must be a value from 1 day to 35 days. Default: Duration.days(1)\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.SnapshotCredentialsDef] = pydantic.Field(None, description='Master user credentials. Note - It is not possible to change the master username for a snapshot; however, it is possible to provide (or generate) a new password. Default: - The existing username and password from the snapshot will be used.\n')
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if removalPolicy is RETAIN, false otherwise\n')
    enable_data_api: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the Data API. Default: false\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - no parameter group.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    scaling: typing.Union[models.aws_rds.ServerlessScalingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Scaling configuration of an Aurora Serverless database cluster. Default: - Serverless cluster is automatically paused after 5 minutes of being idle. minimum capacity: 2 ACU maximum capacity: 16 ACU\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: - a new security group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no VPC security groups will be associated with the DB cluster.\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no subnet group will be associated with the DB cluster\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC that this Aurora Serverless cluster has been created in. Default: - the default VPC in the account and region will be used\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. If provided, the ``vpc`` property must also be specified. Default: - the VPC default strategy if not specified.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    rds.ServerlessClusterFromSnapshot(self, "Cluster",\n        engine=rds.DatabaseClusterEngine.AURORA_MYSQL,\n        vpc=vpc,\n        snapshot_identifier="mySnapshot"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'snapshot_identifier', 'backup_retention', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'enable_data_api', 'parameter_group', 'removal_policy', 'scaling', 'security_groups', 'subnet_group', 'vpc', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ServerlessClusterFromSnapshotProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ServerlessClusterProps
class ServerlessClusterPropsDef(BaseStruct):
    engine: typing.Union[models.aws_rds.DatabaseClusterEngineDef] = pydantic.Field(..., description='What kind of database to start.\n')
    backup_retention: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The number of days during which automatic DB snapshots are retained. Automatic backup retention cannot be disabled on serverless clusters. Must be a value from 1 day to 35 days. Default: Duration.days(1)\n')
    cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='An optional identifier for the cluster. Default: - A name is automatically generated.\n')
    copy_tags_to_snapshot: typing.Optional[bool] = pydantic.Field(None, description='Whether to copy tags to the snapshot when a snapshot is created. Default: - true\n')
    credentials: typing.Optional[models.aws_rds.CredentialsDef] = pydantic.Field(None, description="Credentials for the administrative user. Default: - A username of 'admin' and SecretsManager-generated password\n")
    default_database_name: typing.Optional[str] = pydantic.Field(None, description='Name of a database which is automatically created inside the cluster. Default: - Database is not created in cluster.\n')
    deletion_protection: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB cluster should have deletion protection enabled. Default: - true if removalPolicy is RETAIN, false otherwise\n')
    enable_data_api: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable the Data API. Default: false\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='Additional parameters to pass to the database engine. Default: - no parameter group.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the cluster and its instances are removed from the stack or replaced during an update. Default: - RemovalPolicy.SNAPSHOT (remove the cluster and instances, but retain a snapshot of the data)\n')
    scaling: typing.Union[models.aws_rds.ServerlessScalingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Scaling configuration of an Aurora Serverless database cluster. Default: - Serverless cluster is automatically paused after 5 minutes of being idle. minimum capacity: 2 ACU maximum capacity: 16 ACU\n')
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description='Security group. Default: - a new security group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no VPC security groups will be associated with the DB cluster.\n')
    storage_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS key for storage encryption. Default: - the default master key will be used for storage encryption\n')
    subnet_group: typing.Optional[typing.Union[models.aws_rds.SubnetGroupDef]] = pydantic.Field(None, description='Existing subnet group for the cluster. Default: - a new subnet group is created if ``vpc`` was provided. If the ``vpc`` property was not provided, no subnet group will be associated with the DB cluster\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='The VPC that this Aurora Serverless cluster has been created in. Default: - the default VPC in the account and region will be used\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the instances within the VPC. If provided, the ``vpc`` property must also be specified. Default: - the VPC default strategy if not specified.\n\n:exampleMetadata: infused\n\nExample::\n\n    # Build a data source for AppSync to access the database.\n    # api: appsync.GraphqlApi\n    # Create username and password secret for DB Cluster\n    secret = rds.DatabaseSecret(self, "AuroraSecret",\n        username="clusteradmin"\n    )\n\n    # The VPC to place the cluster in\n    vpc = ec2.Vpc(self, "AuroraVpc")\n\n    # Create the serverless cluster, provide all values needed to customise the database.\n    cluster = rds.ServerlessCluster(self, "AuroraCluster",\n        engine=rds.DatabaseClusterEngine.AURORA_MYSQL,\n        vpc=vpc,\n        credentials={"username": "clusteradmin"},\n        cluster_identifier="db-endpoint-test",\n        default_database_name="demos"\n    )\n    rds_dS = api.add_rds_data_source("rds", cluster, secret, "demos")\n\n    # Set up a resolver for an RDS query.\n    rds_dS.create_resolver("QueryGetDemosRdsResolver",\n        type_name="Query",\n        field_name="getDemosRds",\n        request_mapping_template=appsync.MappingTemplate.from_string("""\n              {\n                "version": "2018-05-29",\n                "statements": [\n                  "SELECT * FROM demos"\n                ]\n              }\n              """),\n        response_mapping_template=appsync.MappingTemplate.from_string("""\n                $utils.toJson($utils.rds.toJsonObject($ctx.result)[0])\n              """)\n    )\n\n    # Set up a resolver for an RDS mutation.\n    rds_dS.create_resolver("MutationAddDemoRdsResolver",\n        type_name="Mutation",\n        field_name="addDemoRds",\n        request_mapping_template=appsync.MappingTemplate.from_string("""\n              {\n                "version": "2018-05-29",\n                "statements": [\n                  "INSERT INTO demos VALUES (:id, :version)",\n                  "SELECT * WHERE id = :id"\n                ],\n                "variableMap": {\n                  ":id": $util.toJson($util.autoId()),\n                  ":version": $util.toJson($ctx.args.version)\n                }\n              }\n              """),\n        response_mapping_template=appsync.MappingTemplate.from_string("""\n                $utils.toJson($utils.rds.toJsonObject($ctx.result)[1][0])\n              """)\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'backup_retention', 'cluster_identifier', 'copy_tags_to_snapshot', 'credentials', 'default_database_name', 'deletion_protection', 'enable_data_api', 'parameter_group', 'removal_policy', 'scaling', 'security_groups', 'storage_encryption_key', 'subnet_group', 'vpc', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ServerlessClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ServerlessScalingOptions
class ServerlessScalingOptionsDef(BaseStruct):
    auto_pause: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The time before an Aurora Serverless database cluster is paused. A database cluster can be paused only when it is idle (it has no connections). Auto pause time must be between 5 minutes and 1 day. If a DB cluster is paused for more than seven days, the DB cluster might be backed up with a snapshot. In this case, the DB cluster is restored when there is a request to connect to it. Set to 0 to disable Default: - automatic pause enabled after 5 minutes\n')
    max_capacity: typing.Optional[aws_cdk.aws_rds.AuroraCapacityUnit] = pydantic.Field(None, description='The maximum capacity for an Aurora Serverless database cluster. Default: - determined by Aurora based on database engine\n')
    min_capacity: typing.Optional[aws_cdk.aws_rds.AuroraCapacityUnit] = pydantic.Field(None, description='The minimum capacity for an Aurora Serverless database cluster. Default: - determined by Aurora based on database engine\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n\n    cluster = rds.ServerlessCluster(self, "AnotherCluster",\n        engine=rds.DatabaseClusterEngine.AURORA_POSTGRESQL,\n        copy_tags_to_snapshot=True,  # whether to save the cluster tags when creating the snapshot. Default is \'true\'\n        parameter_group=rds.ParameterGroup.from_parameter_group_name(self, "ParameterGroup", "default.aurora-postgresql10"),\n        vpc=vpc,\n        scaling=rds.ServerlessScalingOptions(\n            auto_pause=Duration.minutes(10),  # default is to pause after 5 minutes of idle time\n            min_capacity=rds.AuroraCapacityUnit.ACU_8,  # default is 2 Aurora capacity units (ACUs)\n            max_capacity=rds.AuroraCapacityUnit.ACU_32\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auto_pause', 'max_capacity', 'min_capacity']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ServerlessScalingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.ServerlessV2ClusterInstanceProps
class ServerlessV2ClusterInstancePropsDef(BaseStruct):
    allow_major_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow upgrade of major version for the DB instance. Default: - false\n')
    auto_minor_version_upgrade: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable automatic upgrade of minor version for the DB instance. Default: - true\n')
    enable_performance_insights: typing.Optional[bool] = pydantic.Field(None, description='Whether to enable Performance Insights for the DB instance. Default: - false, unless ``performanceInsightRentention`` or ``performanceInsightEncryptionKey`` is set.\n')
    instance_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier for the database instance. Default: - CloudFormation generated identifier\n')
    parameter_group: typing.Optional[typing.Union[models.aws_rds.ParameterGroupDef]] = pydantic.Field(None, description='The DB parameter group to associate with the instance. This is only needed if you need to configure different parameter groups for each individual instance, otherwise you should not provide this and just use the cluster parameter group Default: the cluster parameter group is used\n')
    parameters: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The parameters in the DBParameterGroup to create automatically. You can only specify parameterGroup or parameters but not both. You need to use a versioned engine to auto-generate a DBParameterGroup. Default: - None\n')
    performance_insight_encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS KMS key for encryption of Performance Insights data. Default: - default master key\n')
    performance_insight_retention: typing.Optional[aws_cdk.aws_rds.PerformanceInsightRetention] = pydantic.Field(None, description='The amount of time, in days, to retain Performance Insights data. Default: 7\n')
    publicly_accessible: typing.Optional[bool] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. Default: - true if the instance is placed in a public subnet\n')
    scale_with_writer: typing.Optional[bool] = pydantic.Field(None, description='Only applicable to reader instances. If this is true then the instance will be placed in promotion tier 1, otherwise it will be placed in promotion tier 2. For serverless v2 instances this means: - true: The serverless v2 reader will scale to match the writer instance (provisioned or serverless) - false: The serverless v2 reader will scale with the read workfload on the instance Default: false\n\n:exampleMetadata: infused\n\nExample::\n\n    # Example automatically generated from non-compiling source. May contain errors.\n    # vpc: ec2.Vpc\n\n    cluster = rds.DatabaseCluster(self, "Database",\n        engine=rds.DatabaseClusterEngine.aurora_mysql(version=rds.AuroraMysqlEngineVersion.VER_2_08_1),\n        writer=rds.ClusterInstance.provisioned("writer",\n            instance_type=ec2.InstanceType.of(ec2.InstanceClass.R6G, ec2.InstanceSize.XLARGE4)\n        ),\n        serverless_v2_min_capacity=6.5,\n        serverless_v2_max_capacity=64,\n        readers=[\n            # will be put in promotion tier 1 and will scale with the writer\n            rds.ClusterInstance.serverless_v2("reader1", scale_with_writer=True),\n            # will be put in promotion tier 2 and will not scale with the writer\n            rds.ClusterInstance.serverless_v2("reader2")\n        ],\n        vpc=vpc\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allow_major_version_upgrade', 'auto_minor_version_upgrade', 'enable_performance_insights', 'instance_identifier', 'parameter_group', 'parameters', 'performance_insight_encryption_key', 'performance_insight_retention', 'publicly_accessible', 'scale_with_writer']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.ServerlessV2ClusterInstanceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.SnapshotCredentialsFromGeneratedPasswordOptions
class SnapshotCredentialsFromGeneratedPasswordOptionsDef(BaseStruct):
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='KMS encryption key to encrypt the generated secret. Default: - default master key\n')
    exclude_characters: typing.Optional[str] = pydantic.Field(None, description='The characters to exclude from the generated password. Default: - the DatabaseSecret default exclude character set (" %+~`#$&*()|[]{}:;<>?!\'/@"\\")\n')
    replica_regions: typing.Optional[typing.Sequence[typing.Union[models.aws_secretsmanager.ReplicaRegionDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of regions where to replicate this secret. Default: - Secret is not replicated\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n    engine = rds.DatabaseInstanceEngine.postgres(version=rds.PostgresEngineVersion.VER_15_2)\n    my_key = kms.Key(self, "MyKey")\n\n    rds.DatabaseInstanceFromSnapshot(self, "InstanceFromSnapshotWithCustomizedSecret",\n        engine=engine,\n        vpc=vpc,\n        snapshot_identifier="mySnapshot",\n        credentials=rds.SnapshotCredentials.from_generated_secret("username",\n            encryption_key=my_key,\n            exclude_characters="!&*^#@()",\n            replica_regions=[secretsmanager.ReplicaRegion(region="eu-west-1"), secretsmanager.ReplicaRegion(region="eu-west-2")]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['encryption_key', 'exclude_characters', 'replica_regions']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SnapshotCredentialsFromGeneratedPasswordOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.SqlServerEeInstanceEngineProps
class SqlServerEeInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: infused\n\nExample::\n\n    # vpc: ec2.Vpc\n\n\n    parameter_group = rds.ParameterGroup(self, "ParameterGroup",\n        engine=rds.DatabaseInstanceEngine.sql_server_ee(\n            version=rds.SqlServerEngineVersion.VER_11\n        ),\n        parameters={\n            "locks": "100"\n        }\n    )\n\n    rds.DatabaseInstance(self, "Database",\n        engine=rds.DatabaseInstanceEngine.SQL_SERVER_EE,\n        vpc=vpc,\n        parameter_group=parameter_group\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SqlServerEeInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.SqlServerExInstanceEngineProps
class SqlServerExInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # sql_server_engine_version: rds.SqlServerEngineVersion\n\n    sql_server_ex_instance_engine_props = rds.SqlServerExInstanceEngineProps(\n        version=sql_server_engine_version\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SqlServerExInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqlServerExInstanceEnginePropsDefConfig] = pydantic.Field(None)


class SqlServerExInstanceEnginePropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models.aws_rds.SqlServerEngineVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.SqlServerSeInstanceEngineProps
class SqlServerSeInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # sql_server_engine_version: rds.SqlServerEngineVersion\n\n    sql_server_se_instance_engine_props = rds.SqlServerSeInstanceEngineProps(\n        version=sql_server_engine_version\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SqlServerSeInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqlServerSeInstanceEnginePropsDefConfig] = pydantic.Field(None)


class SqlServerSeInstanceEnginePropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models.aws_rds.SqlServerEngineVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.SqlServerWebInstanceEngineProps
class SqlServerWebInstanceEnginePropsDef(BaseStruct):
    version: models.aws_rds.SqlServerEngineVersionDef = pydantic.Field(..., description='The exact version of the engine to use.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # sql_server_engine_version: rds.SqlServerEngineVersion\n\n    sql_server_web_instance_engine_props = rds.SqlServerWebInstanceEngineProps(\n        version=sql_server_engine_version\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SqlServerWebInstanceEngineProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SqlServerWebInstanceEnginePropsDefConfig] = pydantic.Field(None)


class SqlServerWebInstanceEnginePropsDefConfig(pydantic.BaseModel):
    version_config: typing.Optional[models.aws_rds.SqlServerEngineVersionDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.SubnetGroupProps
class SubnetGroupPropsDef(BaseStruct):
    description: str = pydantic.Field(..., description='Description of the subnet group.\n')
    vpc: typing.Union[models.aws_ec2.VpcDef] = pydantic.Field(..., description='The VPC to place the subnet group in.\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removal policy to apply when the subnet group are removed from the stack or replaced during an update. Default: RemovalPolicy.DESTROY\n')
    subnet_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subnet group. Default: - a name is generated\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Which subnets within the VPC to associate with this group. Default: - private subnets\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_ec2 as ec2\n    from aws_cdk import aws_rds as rds\n\n    # subnet: ec2.Subnet\n    # subnet_filter: ec2.SubnetFilter\n    # vpc: ec2.Vpc\n\n    subnet_group_props = rds.SubnetGroupProps(\n        description="description",\n        vpc=vpc,\n\n        # the properties below are optional\n        removal_policy=cdk.RemovalPolicy.DESTROY,\n        subnet_group_name="subnetGroupName",\n        vpc_subnets=ec2.SubnetSelection(\n            availability_zones=["availabilityZones"],\n            one_per_az=False,\n            subnet_filters=[subnet_filter],\n            subnet_group_name="subnetGroupName",\n            subnets=[subnet],\n            subnet_type=ec2.SubnetType.PRIVATE_ISOLATED\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['description', 'vpc', 'removal_policy', 'subnet_group_name', 'vpc_subnets']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.SubnetGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[SubnetGroupPropsDefConfig] = pydantic.Field(None)


class SubnetGroupPropsDefConfig(pydantic.BaseModel):
    vpc_config: typing.Optional[models._interface_methods.AwsEc2IVpcDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_rds.AuroraCapacityUnit
# skipping emum

#  autogenerated from aws_cdk.aws_rds.DBClusterStorageType
# skipping emum

#  autogenerated from aws_cdk.aws_rds.InstanceType
# skipping emum

#  autogenerated from aws_cdk.aws_rds.InstanceUpdateBehaviour
# skipping emum

#  autogenerated from aws_cdk.aws_rds.LicenseModel
# skipping emum

#  autogenerated from aws_cdk.aws_rds.NetworkType
# skipping emum

#  autogenerated from aws_cdk.aws_rds.PerformanceInsightRetention
# skipping emum

#  autogenerated from aws_cdk.aws_rds.StorageType
# skipping emum

#  autogenerated from aws_cdk.aws_rds.IAuroraClusterInstance
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IClusterEngine
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IClusterInstance
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IDatabaseCluster
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IDatabaseInstance
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IDatabaseProxy
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IEngine
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IInstanceEngine
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IOptionGroup
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IParameterGroup
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.IServerlessCluster
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.ISubnetGroup
#  skipping Interface

#  autogenerated from aws_cdk.aws_rds.CfnDBCluster
class CfnDBClusterDef(BaseCfnResource):
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of storage in gibibytes (GiB) to allocate to each DB instance in the Multi-AZ DB cluster. This setting is required to create a Multi-AZ DB cluster. Valid for: Multi-AZ DB clusters only\n')
    associated_roles: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_DBClusterRolePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Provides a list of the AWS Identity and Access Management (IAM) roles that are associated with the DB cluster. IAM roles that are associated with a DB cluster grant permission for the DB cluster to access other Amazon Web Services on your behalf. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    auto_minor_version_upgrade: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether minor engine upgrades are applied automatically to the DB cluster during the maintenance window. By default, minor engine upgrades are applied automatically. Valid for: Multi-AZ DB clusters only\n')
    availability_zones: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of Availability Zones (AZs) where instances in the DB cluster can be created. For information on AWS Regions and Availability Zones, see `Choosing the Regions and Availability Zones <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.RegionsAndAvailabilityZones.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n')
    backtrack_window: typing.Union[int, float, None] = pydantic.Field(None, description='The target backtrack window, in seconds. To disable backtracking, set this value to 0. .. epigraph:: Currently, Backtrack is only supported for Aurora MySQL DB clusters. Default: 0 Constraints: - If specified, this value must be set to a number from 0 to 259,200 (72 hours). Valid for: Aurora MySQL DB clusters only\n')
    backup_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days for which automated backups are retained. Default: 1 Constraints: - Must be a value from 1 to 35 Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    copy_tags_to_snapshot: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to copy all tags from the DB cluster to snapshots of the DB cluster. The default is not to copy them. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description="The name of your database. If you don't provide a name, then Amazon RDS won't create a database in this DB cluster. For naming constraints, see `Naming Constraints <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Limits.html#RDS_Limits.Constraints>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="The DB cluster identifier. This parameter is stored as a lowercase string. Constraints: - Must contain from 1 to 63 letters, numbers, or hyphens. - First character must be a letter. - Can't end with a hyphen or contain two consecutive hyphens. Example: ``my-cluster1`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    db_cluster_instance_class: typing.Optional[str] = pydantic.Field(None, description='The compute and memory capacity of each DB instance in the Multi-AZ DB cluster, for example db.m6gd.xlarge. Not all DB instance classes are available in all AWS Regions , or for all database engines. For the full list of DB instance classes and availability for your engine, see `DB instance class <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html>`_ in the *Amazon RDS User Guide* . This setting is required to create a Multi-AZ DB cluster. Valid for: Multi-AZ DB clusters only\n')
    db_cluster_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the DB cluster parameter group to associate with this DB cluster. .. epigraph:: If you apply a parameter group to an existing DB cluster, then its DB instances might need to reboot. This can result in an outage while the DB instances are rebooting. If you apply a change to parameter group associated with a stopped DB cluster, then the update stack waits until the DB cluster is started. To list all of the available DB cluster parameter group names, use the following command: ``aws rds describe-db-cluster-parameter-groups --query "DBClusterParameterGroups[].DBClusterParameterGroupName" --output text`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    db_instance_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the DB parameter group to apply to all instances of the DB cluster. .. epigraph:: When you apply a parameter group using the ``DBInstanceParameterGroupName`` parameter, the DB cluster isn't rebooted automatically. Also, parameter changes are applied immediately rather than during the next maintenance window. Default: The existing name setting Constraints: - The DB parameter group must be in the same DB parameter group family as this DB cluster.\n")
    db_subnet_group_name: typing.Optional[str] = pydantic.Field(None, description="A DB subnet group that you want to associate with this DB cluster. If you are restoring a DB cluster to a point in time with ``RestoreType`` set to ``copy-on-write`` , and don't specify a DB subnet group name, then the DB cluster is restored with a default DB subnet group. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    db_system_id: typing.Optional[str] = pydantic.Field(None, description='Reserved for future use.\n')
    deletion_protection: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB cluster has deletion protection enabled. The database can't be deleted when deletion protection is enabled. By default, deletion protection is disabled. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    domain: typing.Optional[str] = pydantic.Field(None, description='Indicates the directory ID of the Active Directory to create the DB cluster. For Amazon Aurora DB clusters, Amazon RDS can use Kerberos authentication to authenticate users that connect to the DB cluster. For more information, see `Kerberos authentication <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/kerberos-authentication.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n')
    domain_iam_role_name: typing.Optional[str] = pydantic.Field(None, description='Specifies the name of the IAM role to use when making API calls to the Directory Service. Valid for: Aurora DB clusters only\n')
    enable_cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. The values in the list depend on the DB engine being used. For more information, see `Publishing Database Logs to Amazon CloudWatch Logs <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.html#USER_LogAccess.Procedural.UploadtoCloudWatch>`_ in the *Amazon Aurora User Guide* . *Aurora MySQL* Valid values: ``audit`` , ``error`` , ``general`` , ``slowquery`` *Aurora PostgreSQL* Valid values: ``postgresql`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    enable_http_endpoint: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to enable the HTTP endpoint for an Aurora Serverless DB cluster. By default, the HTTP endpoint is disabled. When enabled, the HTTP endpoint provides a connectionless web service API for running SQL queries on the Aurora Serverless DB cluster. You can also query your database from inside the RDS console with the query editor. For more information, see `Using the Data API for Aurora Serverless <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n')
    enable_iam_database_authentication: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. By default, mapping is disabled. For more information, see `IAM Database Authentication <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html>`_ in the *Amazon Aurora User Guide.* Valid for: Aurora DB clusters only\n')
    engine: typing.Optional[str] = pydantic.Field(None, description='The name of the database engine to be used for this DB cluster. Valid Values: - ``aurora-mysql`` - ``aurora-postgresql`` - ``mysql`` - ``postgres`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    engine_mode: typing.Optional[str] = pydantic.Field(None, description="The DB engine mode of the DB cluster, either ``provisioned`` or ``serverless`` . The ``serverless`` engine mode only supports Aurora Serverless v1. Currently, AWS CloudFormation doesn't support Aurora Serverless v2. Limitations and requirements apply to some DB engine modes. For more information, see the following sections in the *Amazon Aurora User Guide* : - `Limitations of Aurora Serverless v1 <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations>`_ - `Requirements for Aurora Serverless v2 <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.requirements.html>`_ - `Limitations of parallel query <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-mysql-parallel-query.html#aurora-mysql-parallel-query-limitations>`_ - `Limitations of Aurora global databases <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database.limitations>`_ Valid for: Aurora DB clusters only\n")
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The version number of the database engine to use. To list all of the available engine versions for Aurora MySQL version 2 (5.7-compatible) and version 3 (8.0-compatible), use the following command: ``aws rds describe-db-engine-versions --engine aurora-mysql --query "DBEngineVersions[].EngineVersion"`` You can supply either ``5.7`` or ``8.0`` to use the default engine version for Aurora MySQL version 2 or version 3, respectively. To list all of the available engine versions for Aurora PostgreSQL, use the following command: ``aws rds describe-db-engine-versions --engine aurora-postgresql --query "DBEngineVersions[].EngineVersion"`` To list all of the available engine versions for RDS for MySQL, use the following command: ``aws rds describe-db-engine-versions --engine mysql --query "DBEngineVersions[].EngineVersion"`` To list all of the available engine versions for RDS for PostgreSQL, use the following command: ``aws rds describe-db-engine-versions --engine postgres --query "DBEngineVersions[].EngineVersion"`` *Aurora MySQL* For information, see `Database engine updates for Amazon Aurora MySQL <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.html>`_ in the *Amazon Aurora User Guide* . *Aurora PostgreSQL* For information, see `Amazon Aurora PostgreSQL releases and engine versions <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Updates.20180305.html>`_ in the *Amazon Aurora User Guide* . *MySQL* For information, see `Amazon RDS for MySQL <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.VersionMgmt>`_ in the *Amazon RDS User Guide* . *PostgreSQL* For information, see `Amazon RDS for PostgreSQL <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts>`_ in the *Amazon RDS User Guide* . Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    global_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="If you are configuring an Aurora global database cluster and want your Aurora DB cluster to be a secondary member in the global database cluster, specify the global cluster ID of the global database cluster. To define the primary database cluster of the global cluster, use the `AWS::RDS::GlobalCluster <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-globalcluster.html>`_ resource. If you aren't configuring a global database cluster, don't specify this property. .. epigraph:: To remove the DB cluster from a global database cluster, specify an empty value for the ``GlobalClusterIdentifier`` property. For information about Aurora global databases, see `Working with Amazon Aurora Global Databases <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n")
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of Provisioned IOPS (input/output operations per second) to be initially allocated for each DB instance in the Multi-AZ DB cluster. For information about valid IOPS values, see `Provisioned IOPS storage <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html#USER_PIOPS>`_ in the *Amazon RDS User Guide* . This setting is required to create a Multi-AZ DB cluster. Constraints: Must be a multiple between .5 and 50 of the storage amount for the DB cluster. Valid for: Multi-AZ DB clusters only\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the AWS KMS key that is used to encrypt the database instances in the DB cluster, such as ``arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef`` . If you enable the ``StorageEncrypted`` property but don't specify this property, the default KMS key is used. If you specify this property, you must set the ``StorageEncrypted`` property to ``true`` . If you specify the ``SnapshotIdentifier`` property, the ``StorageEncrypted`` property value is inherited from the snapshot, and if the DB cluster is encrypted, the specified ``KmsKeyId`` property is used. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    manage_master_user_password: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to manage the master user password with AWS Secrets Manager. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide* and `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-secrets-manager.html>`_ in the *Amazon Aurora User Guide.* Constraints: - Can't manage the master user password with AWS Secrets Manager if ``MasterUserPassword`` is specified. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    master_username: typing.Optional[str] = pydantic.Field(None, description="The name of the master user for the DB cluster. .. epigraph:: If you specify the ``SourceDBClusterIdentifier`` , ``SnapshotIdentifier`` , or ``GlobalClusterIdentifier`` property, don't specify this property. The value is inherited from the source DB cluster, the snapshot, or the primary DB cluster for the global database cluster, respectively. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    master_user_password: typing.Optional[str] = pydantic.Field(None, description="The master password for the DB instance. .. epigraph:: If you specify the ``SourceDBClusterIdentifier`` , ``SnapshotIdentifier`` , or ``GlobalClusterIdentifier`` property, don't specify this property. The value is inherited from the source DB cluster, the snapshot, or the primary DB cluster for the global database cluster, respectively. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    master_user_secret: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_MasterUserSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Contains the secret managed by RDS in AWS Secrets Manager for the master user password. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide* and `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-secrets-manager.html>`_ in the *Amazon Aurora User Guide.*\n')
    monitoring_interval: typing.Union[int, float, None] = pydantic.Field(None, description='The interval, in seconds, between points when Enhanced Monitoring metrics are collected for the DB cluster. To turn off collecting Enhanced Monitoring metrics, specify 0. The default is 0. If ``MonitoringRoleArn`` is specified, also set ``MonitoringInterval`` to a value other than 0. Valid Values: ``0, 1, 5, 10, 15, 30, 60`` Valid for: Multi-AZ DB clusters only\n')
    monitoring_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the IAM role that permits RDS to send Enhanced Monitoring metrics to Amazon CloudWatch Logs. An example is ``arn:aws:iam:123456789012:role/emaccess`` . For information on creating a monitoring role, see `Setting up and enabling Enhanced Monitoring <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.Enabling>`_ in the *Amazon RDS User Guide* . If ``MonitoringInterval`` is set to a value other than 0, supply a ``MonitoringRoleArn`` value. Valid for: Multi-AZ DB clusters only\n')
    network_type: typing.Optional[str] = pydantic.Field(None, description='The network type of the DB cluster. Valid values: - ``IPV4`` - ``DUAL`` The network type is determined by the ``DBSubnetGroup`` specified for the DB cluster. A ``DBSubnetGroup`` can support only the IPv4 protocol or the IPv4 and IPv6 protocols ( ``DUAL`` ). For more information, see `Working with a DB instance in a VPC <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html>`_ in the *Amazon Aurora User Guide.* Valid for: Aurora DB clusters only\n')
    performance_insights_enabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to turn on Performance Insights for the DB cluster. For more information, see `Using Amazon Performance Insights <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.html>`_ in the *Amazon RDS User Guide* . Valid for: Multi-AZ DB clusters only\n')
    performance_insights_kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The AWS KMS key identifier for encryption of Performance Insights data. The AWS KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key. If you don't specify a value for ``PerformanceInsightsKMSKeyId`` , then Amazon RDS uses your default KMS key. There is a default KMS key for your AWS account . Your AWS account has a different default KMS key for each AWS Region . Valid for: Multi-AZ DB clusters only\n")
    performance_insights_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description="The number of days to retain Performance Insights data. The default is 7 days. The following values are valid:. - 7 - *month* * 31, where *month* is a number of months from 1-23 - 731 For example, the following values are valid: - 93 (3 months * 31) - 341 (11 months * 31) - 589 (19 months * 31) - 731 If you specify a retention period such as 94, which isn't a valid value, RDS issues an error. Valid for: Multi-AZ DB clusters only\n")
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number on which the DB instances in the DB cluster accept connections. Default: - When ``EngineMode`` is ``provisioned`` , ``3306`` (for both Aurora MySQL and Aurora PostgreSQL) - When ``EngineMode`` is ``serverless`` : - ``3306`` when ``Engine`` is ``aurora`` or ``aurora-mysql`` - ``5432`` when ``Engine`` is ``aurora-postgresql`` .. epigraph:: The ``No interruption`` on update behavior only applies to DB clusters. If you are updating a DB instance, see `Port <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-port>`_ for the AWS::RDS::DBInstance resource. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are created. For more information, see `Backup Window <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.Backups.BackupWindow>`_ in the *Amazon Aurora User Guide.* Constraints: - Must be in the format ``hh24:mi-hh24:mi`` . - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range during which system maintenance can occur, in Universal Coordinated Time (UTC). Format: ``ddd:hh24:mi-ddd:hh24:mi`` The default is a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see `Adjusting the Preferred DB Cluster Maintenance Window <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_UpgradeDBInstance.Maintenance.html#AdjustingTheMaintenanceWindow.Aurora>`_ in the *Amazon Aurora User Guide.* Valid Days: Mon, Tue, Wed, Thu, Fri, Sat, Sun. Constraints: Minimum 30-minute window. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    publicly_accessible: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB cluster is publicly accessible. When the DB cluster is publicly accessible, its Domain Name System (DNS) endpoint resolves to the private IP address from within the DB cluster's virtual private cloud (VPC). It resolves to the public IP address from outside of the DB cluster's VPC. Access to the DB cluster is ultimately controlled by the security group it uses. That public access isn't permitted if the security group assigned to the DB cluster doesn't permit it. When the DB cluster isn't publicly accessible, it is an internal DB cluster with a DNS name that resolves to a private IP address. Default: The default behavior varies depending on whether ``DBSubnetGroupName`` is specified. If ``DBSubnetGroupName`` isn't specified, and ``PubliclyAccessible`` isn't specified, the following applies: - If the default VPC in the target Region doesnt have an internet gateway attached to it, the DB cluster is private. - If the default VPC in the target Region has an internet gateway attached to it, the DB cluster is public. If ``DBSubnetGroupName`` is specified, and ``PubliclyAccessible`` isn't specified, the following applies: - If the subnets are part of a VPC that doesnt have an internet gateway attached to it, the DB cluster is private. - If the subnets are part of a VPC that has an internet gateway attached to it, the DB cluster is public. Valid for: Multi-AZ DB clusters only\n")
    replication_source_identifier: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the source DB instance or DB cluster if this DB cluster is created as a read replica. Valid for: Aurora DB clusters only\n')
    restore_to_time: typing.Optional[str] = pydantic.Field(None, description="The date and time to restore the DB cluster to. Valid Values: Value must be a time in Universal Coordinated Time (UTC) format Constraints: - Must be before the latest restorable time for the DB instance - Must be specified if ``UseLatestRestorableTime`` parameter isn't provided - Can't be specified if the ``UseLatestRestorableTime`` parameter is enabled - Can't be specified if the ``RestoreType`` parameter is ``copy-on-write`` Example: ``2015-03-07T23:45:00Z`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    restore_type: typing.Optional[str] = pydantic.Field(None, description="The type of restore to be performed. You can specify one of the following values:. - ``full-copy`` - The new DB cluster is restored as a full copy of the source DB cluster. - ``copy-on-write`` - The new DB cluster is restored as a clone of the source DB cluster. If you don't specify a ``RestoreType`` value, then the new DB cluster is restored as a full copy of the source DB cluster. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    scaling_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_ScalingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ``ScalingConfiguration`` property type specifies the scaling configuration of an Aurora Serverless DB cluster. This property is only supported for Aurora Serverless v1. For Aurora Serverless v2, use ``ServerlessV2ScalingConfiguration`` property. Valid for: Aurora DB clusters only\n')
    serverless_v2_scaling_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_ServerlessV2ScalingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ``ServerlessV2ScalingConfiguration`` property type specifies the scaling configuration of an Aurora Serverless V2 DB cluster. This property is only supported for Aurora Serverless v2. For Aurora Serverless v1, use ``ScalingConfiguration`` property. Valid for: Aurora DB clusters only\n')
    snapshot_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier for the DB snapshot or DB cluster snapshot to restore from. You can use either the name or the Amazon Resource Name (ARN) to specify a DB cluster snapshot. However, you can use only the ARN to specify a DB snapshot. After you restore a DB cluster with a ``SnapshotIdentifier`` property, you must specify the same ``SnapshotIdentifier`` property for any future updates to the DB cluster. When you specify this property for an update, the DB cluster is not restored from the snapshot again, and the data in the database is not changed. However, if you don't specify the ``SnapshotIdentifier`` property, an empty DB cluster is created, and the original DB cluster is deleted. If you specify a property that is different from the previous snapshot restore property, a new DB cluster is restored from the specified ``SnapshotIdentifier`` property, and the original DB cluster is deleted. If you specify the ``SnapshotIdentifier`` property to restore a DB cluster (as opposed to specifying it for DB cluster updates), then don't specify the following properties: - ``GlobalClusterIdentifier`` - ``MasterUsername`` - ``MasterUserPassword`` - ``ReplicationSourceIdentifier`` - ``RestoreType`` - ``SourceDBClusterIdentifier`` - ``SourceRegion`` - ``StorageEncrypted`` (for an encrypted snapshot) - ``UseLatestRestorableTime`` Constraints: - Must match the identifier of an existing Snapshot. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    source_db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='When restoring a DB cluster to a point in time, the identifier of the source DB cluster from which to restore. Constraints: - Must match the identifier of an existing DBCluster. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    source_region: typing.Optional[str] = pydantic.Field(None, description='The AWS Region which contains the source DB cluster when replicating a DB cluster. For example, ``us-east-1`` . Valid for: Aurora DB clusters only\n')
    storage_encrypted: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Indicates whether the DB cluster is encrypted. If you specify the ``KmsKeyId`` property, then you must enable encryption. If you specify the ``SourceDBClusterIdentifier`` property, don't specify this property. The value is inherited from the source DB cluster, and if the DB cluster is encrypted, the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot is encrypted, don't specify this property. The value is inherited from the snapshot, and the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot isn't encrypted, you can use this property to specify that the restored DB cluster is encrypted. Specify the ``KmsKeyId`` property for the KMS key to use for encryption. If you don't want the restored DB cluster to be encrypted, then don't set this property or set it to ``false`` . Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    storage_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the storage type to be associated with the DB cluster. This setting is required to create a Multi-AZ DB cluster. When specified for a Multi-AZ DB cluster, a value for the ``Iops`` parameter is required. Valid values: ``aurora`` , ``aurora-iopt1`` (Aurora DB clusters); ``io1`` (Multi-AZ DB clusters) Default: ``aurora`` (Aurora DB clusters); ``io1`` (Multi-AZ DB clusters) Valid for: Aurora DB clusters and Multi-AZ DB clusters For more information on storage types for Aurora DB clusters, see `Storage configurations for Amazon Aurora DB clusters <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#aurora-storage-type>`_ . For more information on storage types for Multi-AZ DB clusters, see `Settings for creating Multi-AZ DB clusters <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/create-multi-az-db-cluster.html#create-multi-az-db-cluster-settings>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB cluster. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    use_latest_restorable_time: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to restore the DB cluster to the latest restorable backup time. By default, the DB cluster is not restored to the latest restorable backup time. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of EC2 VPC security groups to associate with this DB cluster. If you plan to update the resource, don't specify VPC security groups in a shared VPC. Valid for: Aurora DB clusters and Multi-AZ DB clusters")
    _init_params: typing.ClassVar[list[str]] = ['allocated_storage', 'associated_roles', 'auto_minor_version_upgrade', 'availability_zones', 'backtrack_window', 'backup_retention_period', 'copy_tags_to_snapshot', 'database_name', 'db_cluster_identifier', 'db_cluster_instance_class', 'db_cluster_parameter_group_name', 'db_instance_parameter_group_name', 'db_subnet_group_name', 'db_system_id', 'deletion_protection', 'domain', 'domain_iam_role_name', 'enable_cloudwatch_logs_exports', 'enable_http_endpoint', 'enable_iam_database_authentication', 'engine', 'engine_mode', 'engine_version', 'global_cluster_identifier', 'iops', 'kms_key_id', 'manage_master_user_password', 'master_username', 'master_user_password', 'master_user_secret', 'monitoring_interval', 'monitoring_role_arn', 'network_type', 'performance_insights_enabled', 'performance_insights_kms_key_id', 'performance_insights_retention_period', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'publicly_accessible', 'replication_source_identifier', 'restore_to_time', 'restore_type', 'scaling_configuration', 'serverless_v2_scaling_configuration', 'snapshot_identifier', 'source_db_cluster_identifier', 'source_region', 'storage_encrypted', 'storage_type', 'tags', 'use_latest_restorable_time', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = ['DBClusterRoleProperty', 'EndpointProperty', 'MasterUserSecretProperty', 'ReadEndpointProperty', 'ScalingConfigurationProperty', 'ServerlessV2ScalingConfigurationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBClusterDefConfig] = pydantic.Field(None)


class CfnDBClusterDefConfig(pydantic.BaseModel):
    DBClusterRoleProperty: typing.Optional[list[CfnDBClusterDefDbclusterrolepropertyParams]] = pydantic.Field(None, description='')
    EndpointProperty: typing.Optional[list[CfnDBClusterDefEndpointpropertyParams]] = pydantic.Field(None, description='')
    MasterUserSecretProperty: typing.Optional[list[CfnDBClusterDefMasterusersecretpropertyParams]] = pydantic.Field(None, description='')
    ReadEndpointProperty: typing.Optional[list[CfnDBClusterDefReadendpointpropertyParams]] = pydantic.Field(None, description='')
    ScalingConfigurationProperty: typing.Optional[list[CfnDBClusterDefScalingconfigurationpropertyParams]] = pydantic.Field(None, description='')
    ServerlessV2ScalingConfigurationProperty: typing.Optional[list[CfnDBClusterDefServerlessv2ScalingconfigurationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnDBClusterDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBClusterDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBClusterDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBClusterDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBClusterDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBClusterDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBClusterDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBClusterDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBClusterDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBClusterDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBClusterDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBClusterDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBClusterDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDBClusterDefDbclusterrolepropertyParams(pydantic.BaseModel):
    role_arn: str = pydantic.Field(..., description='')
    feature_name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBClusterDefEndpointpropertyParams(pydantic.BaseModel):
    address: typing.Optional[str] = pydantic.Field(None, description='')
    port: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBClusterDefMasterusersecretpropertyParams(pydantic.BaseModel):
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    secret_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBClusterDefReadendpointpropertyParams(pydantic.BaseModel):
    address: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBClusterDefScalingconfigurationpropertyParams(pydantic.BaseModel):
    auto_pause: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='')
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    seconds_before_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    seconds_until_auto_pause: typing.Union[int, float, None] = pydantic.Field(None, description='')
    timeout_action: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBClusterDefServerlessv2ScalingconfigurationpropertyParams(pydantic.BaseModel):
    max_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    min_capacity: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnDBClusterDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBClusterDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBClusterDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBClusterDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBClusterDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBClusterDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBClusterDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBClusterDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBClusterDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBClusterDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBClusterDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBClusterDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBClusterDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBClusterParameterGroup
class CfnDBClusterParameterGroupDef(BaseCfnResource):
    description: str = pydantic.Field(..., description='A friendly description for this DB cluster parameter group.\n')
    family: str = pydantic.Field(..., description='The DB cluster parameter group family name. A DB cluster parameter group can be associated with one and only one DB cluster parameter group family, and can be applied only to a DB cluster running a DB engine and engine version compatible with that DB cluster parameter group family. .. epigraph:: The DB cluster parameter group family can\'t be changed when updating a DB cluster parameter group. To list all of the available parameter group families, use the following command: ``aws rds describe-db-engine-versions --query "DBEngineVersions[].DBParameterGroupFamily"`` The output contains duplicates. For more information, see ``[CreateDBClusterParameterGroup](https://docs.aws.amazon.com//AmazonRDS/latest/APIReference/API_CreateDBClusterParameterGroup.html)`` .\n')
    parameters: typing.Any = pydantic.Field(..., description='Provides a list of parameters for the DB cluster parameter group.\n')
    db_cluster_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the DB cluster parameter group. Constraints: - Must not match the name of an existing DB cluster parameter group. If you don't specify a value for ``DBClusterParameterGroupName`` property, a name is automatically created for the DB cluster parameter group. .. epigraph:: This value is stored as a lowercase string.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB cluster parameter group.')
    _init_params: typing.ClassVar[list[str]] = ['description', 'family', 'parameters', 'db_cluster_parameter_group_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBClusterParameterGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBClusterParameterGroupDefConfig] = pydantic.Field(None)


class CfnDBClusterParameterGroupDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnDBClusterParameterGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBClusterParameterGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBClusterParameterGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBClusterParameterGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBClusterParameterGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBClusterParameterGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBClusterParameterGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBClusterParameterGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBClusterParameterGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBClusterParameterGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBClusterParameterGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBClusterParameterGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBClusterParameterGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDBClusterParameterGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBClusterParameterGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBClusterParameterGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBClusterParameterGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBClusterParameterGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBClusterParameterGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBClusterParameterGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBClusterParameterGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBClusterParameterGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBClusterParameterGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBClusterParameterGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBClusterParameterGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBClusterParameterGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBClusterParameterGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBInstance
class CfnDBInstanceDef(BaseCfnResource):
    allocated_storage: typing.Optional[str] = pydantic.Field(None, description='The amount of storage in gibibytes (GiB) to be initially allocated for the database instance. .. epigraph:: If any value is set in the ``Iops`` parameter, ``AllocatedStorage`` must be at least 100 GiB, which corresponds to the minimum Iops value of 1,000. If you increase the ``Iops`` value (in 1,000 IOPS increments), then you must also increase the ``AllocatedStorage`` value (in 100-GiB increments). *Amazon Aurora* Not applicable. Aurora cluster volumes automatically grow as the amount of data in your database increases, though you are only charged for the space that you use in an Aurora cluster volume. *MySQL* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 5 to 3072. *MariaDB* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 5 to 3072. *PostgreSQL* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 5 to 3072. *Oracle* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 10 to 3072. *SQL Server* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): - Enterprise and Standard editions: Must be an integer from 20 to 16384. - Web and Express editions: Must be an integer from 20 to 16384. - Provisioned IOPS storage (io1): - Enterprise and Standard editions: Must be an integer from 20 to 16384. - Web and Express editions: Must be an integer from 20 to 16384. - Magnetic storage (standard): - Enterprise and Standard editions: Must be an integer from 20 to 1024. - Web and Express editions: Must be an integer from 20 to 1024.\n')
    allow_major_version_upgrade: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether major version upgrades are allowed. Changing this parameter doesn't result in an outage and the change is asynchronously applied as soon as possible. Constraints: Major version upgrades must be allowed when specifying a value for the ``EngineVersion`` parameter that is a different major version than the DB instance's current version.\n")
    associated_roles: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_DBInstanceRolePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The AWS Identity and Access Management (IAM) roles associated with the DB instance. *Amazon Aurora* Not applicable. The associated roles are managed by the DB cluster.\n')
    auto_minor_version_upgrade: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether minor engine upgrades are applied automatically to the DB instance during the maintenance window. By default, minor engine upgrades are applied automatically.\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description="The Availability Zone (AZ) where the database will be created. For information on AWS Regions and Availability Zones, see `Regions and Availability Zones <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html>`_ . *Amazon Aurora* Each Aurora DB cluster hosts copies of its storage in three separate Availability Zones. Specify one of these Availability Zones. Aurora automatically chooses an appropriate Availability Zone if you don't specify one. Default: A random, system-chosen Availability Zone in the endpoint's AWS Region . Example: ``us-east-1d`` Constraint: The ``AvailabilityZone`` parameter can't be specified if the DB instance is a Multi-AZ deployment. The specified Availability Zone must be in the same AWS Region as the current endpoint.\n")
    backup_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description="The number of days for which automated backups are retained. Setting this parameter to a positive number enables backups. Setting this parameter to 0 disables automated backups. *Amazon Aurora* Not applicable. The retention period for automated backups is managed by the DB cluster. Default: 1 Constraints: - Must be a value from 0 to 35 - Can't be set to 0 if the DB instance is a source to read replicas\n")
    ca_certificate_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the CA certificate for this DB instance. .. epigraph:: Specifying or updating this property triggers a reboot. For more information about CA certificate identifiers for RDS DB engines, see `Rotating Your SSL/TLS Certificate <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon RDS User Guide* . For more information about CA certificate identifiers for Aurora DB engines, see `Rotating Your SSL/TLS Certificate <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon Aurora User Guide* .\n')
    certificate_details: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_CertificateDetailsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The details of the DB instance's server certificate.\n")
    certificate_rotation_restart: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance is restarted when you rotate your SSL/TLS certificate. By default, the DB instance is restarted when you rotate your SSL/TLS certificate. The certificate is not updated until the DB instance is restarted. .. epigraph:: Set this parameter only if you are *not* using SSL/TLS to connect to the DB instance. If you are using SSL/TLS to connect to the DB instance, follow the appropriate instructions for your DB engine to rotate your SSL/TLS certificate: - For more information about rotating your SSL/TLS certificate for RDS DB engines, see `Rotating Your SSL/TLS Certificate. <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon RDS User Guide.* - For more information about rotating your SSL/TLS certificate for Aurora DB engines, see `Rotating Your SSL/TLS Certificate <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon Aurora User Guide* . This setting doesn't apply to RDS Custom.\n")
    character_set_name: typing.Optional[str] = pydantic.Field(None, description='For supported engines, indicates that the DB instance should be associated with the specified character set. *Amazon Aurora* Not applicable. The character set is managed by the DB cluster. For more information, see `AWS::RDS::DBCluster <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbcluster.html>`_ .\n')
    copy_tags_to_snapshot: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to copy tags from the DB instance to snapshots of the DB instance. By default, tags are not copied. *Amazon Aurora* Not applicable. Copying tags to snapshots is managed by the DB cluster. Setting this value for an Aurora DB instance has no effect on the DB cluster setting.\n')
    custom_iam_instance_profile: typing.Optional[str] = pydantic.Field(None, description='The instance profile associated with the underlying Amazon EC2 instance of an RDS Custom DB instance. The instance profile must meet the following requirements: - The profile must exist in your account. - The profile must have an IAM role that Amazon EC2 has permissions to assume. - The instance profile name and the associated IAM role name must start with the prefix ``AWSRDSCustom`` . For the list of permissions required for the IAM role, see `Configure IAM and your VPC <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-setup-orcl.html#custom-setup-orcl.iam-vpc>`_ in the *Amazon RDS User Guide* . This setting is required for RDS Custom.\n')
    db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the DB cluster that the instance will belong to.\n')
    db_cluster_snapshot_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier for the RDS for MySQL Multi-AZ DB cluster snapshot to restore from. For more information on Multi-AZ DB clusters, see `Multi-AZ DB cluster deployments <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html>`_ in the *Amazon RDS User Guide* . Constraints: - Must match the identifier of an existing Multi-AZ DB cluster snapshot. - Can't be specified when ``DBSnapshotIdentifier`` is specified. - Must be specified when ``DBSnapshotIdentifier`` isn't specified. - If you are restoring from a shared manual Multi-AZ DB cluster snapshot, the ``DBClusterSnapshotIdentifier`` must be the ARN of the shared snapshot. - Can't be the identifier of an Aurora DB cluster snapshot. - Can't be the identifier of an RDS for PostgreSQL Multi-AZ DB cluster snapshot.\n")
    db_instance_class: typing.Optional[str] = pydantic.Field(None, description='The compute and memory capacity of the DB instance, for example, ``db.m4.large`` . Not all DB instance classes are available in all AWS Regions, or for all database engines. For the full list of DB instance classes, and availability for your engine, see `DB Instance Class <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html>`_ in the *Amazon RDS User Guide.* For more information about DB instance class pricing and AWS Region support for DB instance classes, see `Amazon RDS Pricing <https://docs.aws.amazon.com/rds/pricing/>`_ .\n')
    db_instance_identifier: typing.Optional[str] = pydantic.Field(None, description="A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. If you don't specify a name, AWS CloudFormation generates a unique physical ID and uses that ID for the DB instance. For more information, see `Name Type <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-name.html>`_ . For information about constraints that apply to DB instance identifiers, see `Naming constraints in Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.Constraints>`_ in the *Amazon RDS User Guide* . .. epigraph:: If you specify a name, you can't perform updates that require replacement of this resource. You can perform updates that require no or some interruption. If you must replace the resource, specify a new name.\n")
    db_name: typing.Optional[str] = pydantic.Field(None, description="The meaning of this parameter differs according to the database engine you use. .. epigraph:: If you specify the ``[DBSnapshotIdentifier](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-dbsnapshotidentifier)`` property, this property only applies to RDS for Oracle. *Amazon Aurora* Not applicable. The database name is managed by the DB cluster. *MySQL* The name of the database to create when the DB instance is created. If this parameter is not specified, no database is created in the DB instance. Constraints: - Must contain 1 to 64 letters or numbers. - Can't be a word reserved by the specified database engine *MariaDB* The name of the database to create when the DB instance is created. If this parameter is not specified, no database is created in the DB instance. Constraints: - Must contain 1 to 64 letters or numbers. - Can't be a word reserved by the specified database engine *PostgreSQL* The name of the database to create when the DB instance is created. If this parameter is not specified, the default ``postgres`` database is created in the DB instance. Constraints: - Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9). - Must contain 1 to 63 characters. - Can't be a word reserved by the specified database engine *Oracle* The Oracle System ID (SID) of the created DB instance. If you specify ``null`` , the default value ``ORCL`` is used. You can't specify the string NULL, or any other reserved word, for ``DBName`` . Default: ``ORCL`` Constraints: - Can't be longer than 8 characters *SQL Server* Not applicable. Must be null.\n")
    db_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of an existing DB parameter group or a reference to an `AWS::RDS::DBParameterGroup <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbparametergroup.html>`_ resource created in the template. To list all of the available DB parameter group names, use the following command: ``aws rds describe-db-parameter-groups --query "DBParameterGroups[].DBParameterGroupName" --output text`` .. epigraph:: If any of the data members of the referenced parameter group are changed during an update, the DB instance might need to be restarted, which causes some interruption. If the parameter group contains static parameters, whether they were changed or not, an update triggers a reboot. If you don\'t specify a value for ``DBParameterGroupName`` property, the default DB parameter group for the specified engine and engine version is used.\n')
    db_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of the DB security groups to assign to the DB instance. The list can include both the name of existing DB security groups or references to AWS::RDS::DBSecurityGroup resources created in the template. If you set DBSecurityGroups, you must not set VPCSecurityGroups, and vice versa. Also, note that the DBSecurityGroups property exists only for backwards compatibility with older regions and is no longer recommended for providing security information to an RDS DB instance. Instead, use VPCSecurityGroups. .. epigraph:: If you specify this property, AWS CloudFormation sends only the following properties (if specified) to Amazon RDS during create operations: - ``AllocatedStorage`` - ``AutoMinorVersionUpgrade`` - ``AvailabilityZone`` - ``BackupRetentionPeriod`` - ``CharacterSetName`` - ``DBInstanceClass`` - ``DBName`` - ``DBParameterGroupName`` - ``DBSecurityGroups`` - ``DBSubnetGroupName`` - ``Engine`` - ``EngineVersion`` - ``Iops`` - ``LicenseModel`` - ``MasterUsername`` - ``MasterUserPassword`` - ``MultiAZ`` - ``OptionGroupName`` - ``PreferredBackupWindow`` - ``PreferredMaintenanceWindow`` All other properties are ignored. Specify a virtual private cloud (VPC) security group if you want to submit other properties, such as ``StorageType`` , ``StorageEncrypted`` , or ``KmsKeyId`` . If you're already using the ``DBSecurityGroups`` property, you can't use these other properties by updating your DB instance to use a VPC security group. You must recreate the DB instance.\n")
    db_snapshot_identifier: typing.Optional[str] = pydantic.Field(None, description="The name or Amazon Resource Name (ARN) of the DB snapshot that's used to restore the DB instance. If you're restoring from a shared manual DB snapshot, you must specify the ARN of the snapshot. By specifying this property, you can create a DB instance from the specified DB snapshot. If the ``DBSnapshotIdentifier`` property is an empty string or the ``AWS::RDS::DBInstance`` declaration has no ``DBSnapshotIdentifier`` property, AWS CloudFormation creates a new database. If the property contains a value (other than an empty string), AWS CloudFormation creates a database from the specified snapshot. If a snapshot with the specified name doesn't exist, AWS CloudFormation can't create the database and it rolls back the stack. Some DB instance properties aren't valid when you restore from a snapshot, such as the ``MasterUsername`` and ``MasterUserPassword`` properties. For information about the properties that you can specify, see the ``RestoreDBInstanceFromDBSnapshot`` action in the *Amazon RDS API Reference* . After you restore a DB instance with a ``DBSnapshotIdentifier`` property, you must specify the same ``DBSnapshotIdentifier`` property for any future updates to the DB instance. When you specify this property for an update, the DB instance is not restored from the DB snapshot again, and the data in the database is not changed. However, if you don't specify the ``DBSnapshotIdentifier`` property, an empty DB instance is created, and the original DB instance is deleted. If you specify a property that is different from the previous snapshot restore property, a new DB instance is restored from the specified ``DBSnapshotIdentifier`` property, and the original DB instance is deleted. If you specify the ``DBSnapshotIdentifier`` property to restore a DB instance (as opposed to specifying it for DB instance updates), then don't specify the following properties: - ``CharacterSetName`` - ``DBClusterIdentifier`` - ``DBName`` - ``DeleteAutomatedBackups`` - ``EnablePerformanceInsights`` - ``KmsKeyId`` - ``MasterUsername`` - ``MasterUserPassword`` - ``PerformanceInsightsKMSKeyId`` - ``PerformanceInsightsRetentionPeriod`` - ``PromotionTier`` - ``SourceDBInstanceIdentifier`` - ``SourceRegion`` - ``StorageEncrypted`` (for an encrypted snapshot) - ``Timezone`` *Amazon Aurora* Not applicable. Snapshot restore is managed by the DB cluster.\n")
    db_subnet_group_name: typing.Optional[str] = pydantic.Field(None, description="A DB subnet group to associate with the DB instance. If you update this value, the new subnet group must be a subnet group in a new VPC. If there's no DB subnet group, then the DB instance isn't a VPC DB instance. For more information about using Amazon RDS in a VPC, see `Using Amazon RDS with Amazon Virtual Private Cloud (VPC) <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. The DB subnet group is managed by the DB cluster. If specified, the setting must match the DB cluster setting.\n")
    delete_automated_backups: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to remove automated backups immediately after the DB instance is deleted. This parameter isn't case-sensitive. The default is to remove automated backups immediately after the DB instance is deleted. *Amazon Aurora* Not applicable. When you delete a DB cluster, all automated backups for that DB cluster are deleted and can't be recovered. Manual DB cluster snapshots of the DB cluster are not deleted.\n")
    deletion_protection: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance has deletion protection enabled. The database can't be deleted when deletion protection is enabled. By default, deletion protection is disabled. For more information, see `Deleting a DB Instance <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_DeleteInstance.html>`_ . *Amazon Aurora* Not applicable. You can enable or disable deletion protection for the DB cluster. For more information, see ``CreateDBCluster`` . DB instances in a DB cluster can be deleted even when deletion protection is enabled for the DB cluster.\n")
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Currently, only Microsoft SQL Server, Oracle, and PostgreSQL DB instances can be created in an Active Directory Domain. For more information, see `Kerberos Authentication <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/kerberos-authentication.html>`_ in the *Amazon RDS User Guide* .\n')
    domain_iam_role_name: typing.Optional[str] = pydantic.Field(None, description="Specify the name of the IAM role to be used when making API calls to the Directory Service. This setting doesn't apply to RDS Custom. *Amazon Aurora* Not applicable. The domain is managed by the DB cluster.\n")
    enable_cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. The values in the list depend on the DB engine being used. For more information, see `Publishing Database Logs to Amazon CloudWatch Logs <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.html#USER_LogAccess.Procedural.UploadtoCloudWatch>`_ in the *Amazon Relational Database Service User Guide* . *Amazon Aurora* Not applicable. CloudWatch Logs exports are managed by the DB cluster. *MariaDB* Valid values: ``audit`` , ``error`` , ``general`` , ``slowquery`` *Microsoft SQL Server* Valid values: ``agent`` , ``error`` *MySQL* Valid values: ``audit`` , ``error`` , ``general`` , ``slowquery`` *Oracle* Valid values: ``alert`` , ``audit`` , ``listener`` , ``trace`` *PostgreSQL* Valid values: ``postgresql`` , ``upgrade``\n')
    enable_iam_database_authentication: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. By default, mapping is disabled. This property is supported for RDS for MariaDB, RDS for MySQL, and RDS for PostgreSQL. For more information, see `IAM Database Authentication for MariaDB, MySQL, and PostgreSQL <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html>`_ in the *Amazon RDS User Guide.* *Amazon Aurora* Not applicable. Mapping AWS IAM accounts to database accounts is managed by the DB cluster.\n')
    enable_performance_insights: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to enable Performance Insights for the DB instance. For more information, see `Using Amazon Performance Insights <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.html>`_ in the *Amazon RDS User Guide* . This setting doesn't apply to RDS Custom.\n")
    endpoint: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_EndpointPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the connection endpoint. .. epigraph:: The endpoint might not be shown for instances whose status is ``creating`` .\n')
    engine: typing.Optional[str] = pydantic.Field(None, description='The name of the database engine that you want to use for this DB instance. .. epigraph:: When you are creating a DB instance, the ``Engine`` property is required. Valid Values: - ``aurora-mysql`` (for Aurora MySQL DB instances) - ``aurora-postgresql`` (for Aurora PostgreSQL DB instances) - ``custom-oracle-ee`` (for RDS Custom for Oracle DB instances) - ``custom-oracle-ee-cdb`` (for RDS Custom for Oracle DB instances) - ``custom-sqlserver-ee`` (for RDS Custom for SQL Server DB instances) - ``custom-sqlserver-se`` (for RDS Custom for SQL Server DB instances) - ``custom-sqlserver-web`` (for RDS Custom for SQL Server DB instances) - ``mariadb`` - ``mysql`` - ``oracle-ee`` - ``oracle-ee-cdb`` - ``oracle-se2`` - ``oracle-se2-cdb`` - ``postgres`` - ``sqlserver-ee`` - ``sqlserver-se`` - ``sqlserver-ex`` - ``sqlserver-web``\n')
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The version number of the database engine to use. For a list of valid engine versions, use the ``DescribeDBEngineVersions`` action. The following are the database engines and links to information about the major and minor versions that are available with Amazon RDS. Not every database engine is available for every AWS Region. *Amazon Aurora* Not applicable. The version number of the database engine to be used by the DB instance is managed by the DB cluster. *MariaDB* See `MariaDB on Amazon RDS Versions <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MariaDB.html#MariaDB.Concepts.VersionMgmt>`_ in the *Amazon RDS User Guide.* *Microsoft SQL Server* See `Microsoft SQL Server Versions on Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.General.VersionSupport>`_ in the *Amazon RDS User Guide.* *MySQL* See `MySQL on Amazon RDS Versions <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.VersionMgmt>`_ in the *Amazon RDS User Guide.* *Oracle* See `Oracle Database Engine Release Notes <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.PatchComposition.html>`_ in the *Amazon RDS User Guide.* *PostgreSQL* See `Supported PostgreSQL Database Versions <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.DBVersions>`_ in the *Amazon RDS User Guide.*\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. If you specify this property, you must follow the range of allowed ratios of your requested IOPS rate to the amount of storage that you allocate (IOPS to allocated storage). For example, you can provision an Oracle database instance with 1000 IOPS and 200 GiB of storage (a ratio of 5:1), or specify 2000 IOPS with 200 GiB of storage (a ratio of 10:1). For more information, see `Amazon RDS Provisioned IOPS Storage to Improve Performance <https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/CHAP_Storage.html#USER_PIOPS>`_ in the *Amazon RDS User Guide* . .. epigraph:: If you specify ``io1`` for the ``StorageType`` property, then you must also specify the ``Iops`` property.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The ARN of the AWS KMS key that's used to encrypt the DB instance, such as ``arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef`` . If you enable the StorageEncrypted property but don't specify this property, AWS CloudFormation uses the default KMS key. If you specify this property, you must set the StorageEncrypted property to true. If you specify the ``SourceDBInstanceIdentifier`` property, the value is inherited from the source DB instance if the read replica is created in the same region. If you create an encrypted read replica in a different AWS Region, then you must specify a KMS key for the destination AWS Region. KMS encryption keys are specific to the region that they're created in, and you can't use encryption keys from one region in another region. If you specify the ``SnapshotIdentifier`` property, the ``StorageEncrypted`` property value is inherited from the snapshot, and if the DB instance is encrypted, the specified ``KmsKeyId`` property is used. If you specify ``DBSecurityGroups`` , AWS CloudFormation ignores this property. To specify both a security group and this property, you must use a VPC security group. For more information about Amazon RDS and VPC, see `Using Amazon RDS with Amazon VPC <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. The KMS key identifier is managed by the DB cluster.\n")
    license_model: typing.Optional[str] = pydantic.Field(None, description="License model information for this DB instance. Valid values: - Aurora MySQL - ``general-public-license`` - Aurora PostgreSQL - ``postgresql-license`` - MariaDB - ``general-public-license`` - Microsoft SQL Server - ``license-included`` - MySQL - ``general-public-license`` - Oracle - ``bring-your-own-license`` or ``license-included`` - PostgreSQL - ``postgresql-license`` .. epigraph:: If you've specified ``DBSecurityGroups`` and then you update the license model, AWS CloudFormation replaces the underlying DB instance. This will incur some interruptions to database availability.\n")
    manage_master_user_password: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to manage the master user password with AWS Secrets Manager. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide.* Constraints: - Can't manage the master user password with AWS Secrets Manager if ``MasterUserPassword`` is specified.\n")
    master_username: typing.Optional[str] = pydantic.Field(None, description="The master user name for the DB instance. .. epigraph:: If you specify the ``SourceDBInstanceIdentifier`` or ``DBSnapshotIdentifier`` property, don't specify this property. The value is inherited from the source DB instance or snapshot. *Amazon Aurora* Not applicable. The name for the master user is managed by the DB cluster. *MariaDB* Constraints: - Required for MariaDB. - Must be 1 to 16 letters or numbers. - Can't be a reserved word for the chosen database engine. *Microsoft SQL Server* Constraints: - Required for SQL Server. - Must be 1 to 128 letters or numbers. - The first character must be a letter. - Can't be a reserved word for the chosen database engine. *MySQL* Constraints: - Required for MySQL. - Must be 1 to 16 letters or numbers. - First character must be a letter. - Can't be a reserved word for the chosen database engine. *Oracle* Constraints: - Required for Oracle. - Must be 1 to 30 letters or numbers. - First character must be a letter. - Can't be a reserved word for the chosen database engine. *PostgreSQL* Constraints: - Required for PostgreSQL. - Must be 1 to 63 letters or numbers. - First character must be a letter. - Can't be a reserved word for the chosen database engine.\n")
    master_user_password: typing.Optional[str] = pydantic.Field(None, description='The password for the master user. The password can include any printable ASCII character except "/", """, or "@". *Amazon Aurora* Not applicable. The password for the master user is managed by the DB cluster. *MariaDB* Constraints: Must contain from 8 to 41 characters. *Microsoft SQL Server* Constraints: Must contain from 8 to 128 characters. *MySQL* Constraints: Must contain from 8 to 41 characters. *Oracle* Constraints: Must contain from 8 to 30 characters. *PostgreSQL* Constraints: Must contain from 8 to 128 characters.\n')
    master_user_secret: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_MasterUserSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Contains the secret managed by RDS in AWS Secrets Manager for the master user password. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide.*\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description="The upper limit in gibibytes (GiB) to which Amazon RDS can automatically scale the storage of the DB instance. For more information about this setting, including limitations that apply to it, see `Managing capacity automatically with Amazon RDS storage autoscaling <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling>`_ in the *Amazon RDS User Guide* . This setting doesn't apply to RDS Custom. *Amazon Aurora* Not applicable. Storage is managed by the DB cluster.\n")
    monitoring_interval: typing.Union[int, float, None] = pydantic.Field(None, description="The interval, in seconds, between points when Enhanced Monitoring metrics are collected for the DB instance. To disable collection of Enhanced Monitoring metrics, specify 0. The default is 0. If ``MonitoringRoleArn`` is specified, then you must set ``MonitoringInterval`` to a value other than 0. This setting doesn't apply to RDS Custom. Valid Values: ``0, 1, 5, 10, 15, 30, 60``\n")
    monitoring_role_arn: typing.Optional[str] = pydantic.Field(None, description="The ARN for the IAM role that permits RDS to send enhanced monitoring metrics to Amazon CloudWatch Logs. For example, ``arn:aws:iam:123456789012:role/emaccess`` . For information on creating a monitoring role, see `Setting Up and Enabling Enhanced Monitoring <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.Enabling>`_ in the *Amazon RDS User Guide* . If ``MonitoringInterval`` is set to a value other than 0, then you must supply a ``MonitoringRoleArn`` value. This setting doesn't apply to RDS Custom.\n")
    multi_az: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Specifies whether the database instance is a Multi-AZ DB instance deployment. You can't set the ``AvailabilityZone`` parameter if the ``MultiAZ`` parameter is set to true. For more information, see `Multi-AZ deployments for high availability <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. Amazon Aurora storage is replicated across all of the Availability Zones and doesn't require the ``MultiAZ`` option to be set.\n")
    nchar_character_set_name: typing.Optional[str] = pydantic.Field(None, description="The name of the NCHAR character set for the Oracle DB instance. This parameter doesn't apply to RDS Custom.\n")
    network_type: typing.Optional[str] = pydantic.Field(None, description='The network type of the DB instance. Valid values: - ``IPV4`` - ``DUAL`` The network type is determined by the ``DBSubnetGroup`` specified for the DB instance. A ``DBSubnetGroup`` can support only the IPv4 protocol or the IPv4 and IPv6 protocols ( ``DUAL`` ). For more information, see `Working with a DB instance in a VPC <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html>`_ in the *Amazon RDS User Guide.*\n')
    option_group_name: typing.Optional[str] = pydantic.Field(None, description="Indicates that the DB instance should be associated with the specified option group. Permanent options, such as the TDE option for Oracle Advanced Security TDE, can't be removed from an option group. Also, that option group can't be removed from a DB instance once it is associated with a DB instance.\n")
    performance_insights_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The AWS KMS key identifier for encryption of Performance Insights data. The KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key. If you do not specify a value for ``PerformanceInsightsKMSKeyId`` , then Amazon RDS uses your default KMS key. There is a default KMS key for your AWS account. Your AWS account has a different default KMS key for each AWS Region. For information about enabling Performance Insights, see `EnablePerformanceInsights <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-enableperformanceinsights>`_ .\n')
    performance_insights_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description="The number of days to retain Performance Insights data. The default is 7 days. The following values are valid:. - 7 - *month* * 31, where *month* is a number of months from 1-23 - 731 For example, the following values are valid: - 93 (3 months * 31) - 341 (11 months * 31) - 589 (19 months * 31) - 731 If you specify a retention period such as 94, which isn't a valid value, RDS issues an error. This setting doesn't apply to RDS Custom.\n")
    port: typing.Optional[str] = pydantic.Field(None, description='The port number on which the database accepts connections. *Amazon Aurora* Not applicable. The port number is managed by the DB cluster.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are created if automated backups are enabled, using the ``BackupRetentionPeriod`` parameter. For more information, see `Backup Window <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow>`_ in the *Amazon RDS User Guide.* Constraints: - Must be in the format ``hh24:mi-hh24:mi`` . - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. *Amazon Aurora* Not applicable. The daily time range for creating automated backups is managed by the DB cluster.\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range during which system maintenance can occur, in Universal Coordinated Time (UTC). Format: ``ddd:hh24:mi-ddd:hh24:mi`` The default is a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see `Adjusting the Preferred DB Instance Maintenance Window <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#AdjustingTheMaintenanceWindow>`_ in the *Amazon RDS User Guide.* .. epigraph:: This property applies when AWS CloudFormation initially creates the DB instance. If you use AWS CloudFormation to update the DB instance, those updates are applied immediately. Constraints: Minimum 30-minute window.\n')
    processor_features: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_ProcessorFeaturePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The number of CPU cores and the number of threads per core for the DB instance class of the DB instance. This setting doesn't apply to RDS Custom. *Amazon Aurora* Not applicable.\n")
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description="A value that specifies the order in which an Aurora Replica is promoted to the primary instance after a failure of the existing primary instance. For more information, see `Fault Tolerance for an Aurora DB Cluster <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance>`_ in the *Amazon Aurora User Guide* . This setting doesn't apply to RDS Custom. Default: 1 Valid Values: 0 - 15\n")
    publicly_accessible: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. If you specify true, AWS CloudFormation creates an instance with a publicly resolvable DNS name, which resolves to a public IP address. If you specify false, AWS CloudFormation creates an internal instance with a DNS name that resolves to a private IP address. The default behavior value depends on your VPC setup and the database subnet group. For more information, see the ``PubliclyAccessible`` parameter in the `CreateDBInstance <https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_CreateDBInstance.html>`_ in the *Amazon RDS API Reference* .\n')
    replica_mode: typing.Optional[str] = pydantic.Field(None, description='The open mode of an Oracle read replica. For more information, see `Working with Oracle Read Replicas for Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/oracle-read-replicas.html>`_ in the *Amazon RDS User Guide* . This setting is only supported in RDS for Oracle. Default: ``open-read-only`` Valid Values: ``open-read-only`` or ``mounted``\n')
    restore_time: typing.Optional[str] = pydantic.Field(None, description="The date and time to restore from. Valid Values: Value must be a time in Universal Coordinated Time (UTC) format Constraints: - Must be before the latest restorable time for the DB instance - Can't be specified if the ``UseLatestRestorableTime`` parameter is enabled Example: ``2009-09-07T23:45:00Z``\n")
    source_db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the Multi-AZ DB cluster that will act as the source for the read replica. Each DB cluster can have up to 15 read replicas. Constraints: - Must be the identifier of an existing Multi-AZ DB cluster. - Can't be specified if the ``SourceDBInstanceIdentifier`` parameter is also specified. - The specified DB cluster must have automatic backups enabled, that is, its backup retention period must be greater than 0. - The source DB cluster must be in the same AWS Region as the read replica. Cross-Region replication isn't supported.\n")
    source_db_instance_automated_backups_arn: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the replicated automated backups from which to restore, for example, ``arn:aws:rds:useast-1:123456789012:auto-backup:ab-L2IJCEXJP7XQ7HOJ4SIEXAMPLE`` . This setting doesn't apply to RDS Custom.\n")
    source_db_instance_identifier: typing.Optional[str] = pydantic.Field(None, description="If you want to create a read replica DB instance, specify the ID of the source DB instance. Each DB instance can have a limited number of read replicas. For more information, see `Working with Read Replicas <https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/USER_ReadRepl.html>`_ in the *Amazon RDS User Guide* . For information about constraints that apply to DB instance identifiers, see `Naming constraints in Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.Constraints>`_ in the *Amazon RDS User Guide* . The ``SourceDBInstanceIdentifier`` property determines whether a DB instance is a read replica. If you remove the ``SourceDBInstanceIdentifier`` property from your template and then update your stack, AWS CloudFormation promotes the Read Replica to a standalone DB instance. .. epigraph:: - If you specify a source DB instance that uses VPC security groups, we recommend that you specify the ``VPCSecurityGroups`` property. If you don't specify the property, the read replica inherits the value of the ``VPCSecurityGroups`` property from the source DB when you create the replica. However, if you update the stack, AWS CloudFormation reverts the replica's ``VPCSecurityGroups`` property to the default value because it's not defined in the stack's template. This change might cause unexpected issues. - Read replicas don't support deletion policies. AWS CloudFormation ignores any deletion policy that's associated with a read replica. - If you specify ``SourceDBInstanceIdentifier`` , don't specify the ``DBSnapshotIdentifier`` property. You can't create a read replica from a snapshot. - Don't set the ``BackupRetentionPeriod`` , ``DBName`` , ``MasterUsername`` , ``MasterUserPassword`` , and ``PreferredBackupWindow`` properties. The database attributes are inherited from the source DB instance, and backups are disabled for read replicas. - If the source DB instance is in a different region than the read replica, specify the source region in ``SourceRegion`` , and specify an ARN for a valid DB instance in ``SourceDBInstanceIdentifier`` . For more information, see `Constructing a Amazon RDS Amazon Resource Name (ARN) <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Tagging.html#USER_Tagging.ARN>`_ in the *Amazon RDS User Guide* . - For DB instances in Amazon Aurora clusters, don't specify this property. Amazon RDS automatically assigns writer and reader DB instances.\n")
    source_dbi_resource_id: typing.Optional[str] = pydantic.Field(None, description='The resource ID of the source DB instance from which to restore.\n')
    source_region: typing.Optional[str] = pydantic.Field(None, description='The ID of the region that contains the source DB instance for the read replica.\n')
    storage_encrypted: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance is encrypted. By default, it isn't encrypted. If you specify the ``KmsKeyId`` property, then you must enable encryption. If you specify the ``SourceDBInstanceIdentifier`` property, don't specify this property. The value is inherited from the source DB instance, and if the DB instance is encrypted, the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot is encrypted, don't specify this property. The value is inherited from the snapshot, and the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot isn't encrypted, you can use this property to specify that the restored DB instance is encrypted. Specify the ``KmsKeyId`` property for the KMS key to use for encryption. If you don't want the restored DB instance to be encrypted, then don't set this property or set it to ``false`` . *Amazon Aurora* Not applicable. The encryption for DB instances is managed by the DB cluster.\n")
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies the storage throughput value for the DB instance. This setting applies only to the ``gp3`` storage type. This setting doesn't apply to RDS Custom or Amazon Aurora.\n")
    storage_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the storage type to be associated with the DB instance. Valid values: ``gp2 | gp3 | io1 | standard`` The ``standard`` value is also known as magnetic. If you specify ``io1`` or ``gp3`` , you must also include a value for the ``Iops`` parameter. Default: ``io1`` if the ``Iops`` parameter is specified, otherwise ``gp2`` For more information, see `Amazon RDS DB Instance Storage <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. Aurora data is stored in the cluster volume, which is a single, virtual volume that uses solid state drives (SSDs).\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB instance.\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The time zone of the DB instance. The time zone parameter is currently supported only by `Microsoft SQL Server <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.General.TimeZone>`_ .\n')
    use_default_processor_features: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance class of the DB instance uses its default processor features. This setting doesn't apply to RDS Custom.\n")
    use_latest_restorable_time: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance is restored from the latest backup time. By default, the DB instance isn't restored from the latest backup time. Constraints: Can't be specified if the ``RestoreTime`` parameter is provided.\n")
    vpc_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of the VPC security group IDs to assign to the DB instance. The list can include both the physical IDs of existing VPC security groups and references to `AWS::EC2::SecurityGroup <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-security-group.html>`_ resources created in the template. If you plan to update the resource, don't specify VPC security groups in a shared VPC. If you set ``VPCSecurityGroups`` , you must not set ```DBSecurityGroups`` <https://docs.aws.amazon.com//AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-dbsecuritygroups>`_ , and vice versa. .. epigraph:: You can migrate a DB instance in your stack from an RDS DB security group to a VPC security group, but keep the following in mind: - You can't revert to using an RDS security group after you establish a VPC security group membership. - When you migrate your DB instance to VPC security groups, if your stack update rolls back because the DB instance update fails or because an update fails in another AWS CloudFormation resource, the rollback fails because it can't revert to an RDS security group. - To use the properties that are available when you use a VPC security group, you must recreate the DB instance. If you don't, AWS CloudFormation submits only the property values that are listed in the ```DBSecurityGroups`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-dbsecuritygroups>`_ property. To avoid this situation, migrate your DB instance to using VPC security groups only when that is the only change in your stack template. *Amazon Aurora* Not applicable. The associated list of EC2 VPC security groups is managed by the DB cluster. If specified, the setting must match the DB cluster setting.")
    _init_params: typing.ClassVar[list[str]] = ['allocated_storage', 'allow_major_version_upgrade', 'associated_roles', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention_period', 'ca_certificate_identifier', 'certificate_details', 'certificate_rotation_restart', 'character_set_name', 'copy_tags_to_snapshot', 'custom_iam_instance_profile', 'db_cluster_identifier', 'db_cluster_snapshot_identifier', 'db_instance_class', 'db_instance_identifier', 'db_name', 'db_parameter_group_name', 'db_security_groups', 'db_snapshot_identifier', 'db_subnet_group_name', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_iam_role_name', 'enable_cloudwatch_logs_exports', 'enable_iam_database_authentication', 'enable_performance_insights', 'endpoint', 'engine', 'engine_version', 'iops', 'kms_key_id', 'license_model', 'manage_master_user_password', 'master_username', 'master_user_password', 'master_user_secret', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role_arn', 'multi_az', 'nchar_character_set_name', 'network_type', 'option_group_name', 'performance_insights_kms_key_id', 'performance_insights_retention_period', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'promotion_tier', 'publicly_accessible', 'replica_mode', 'restore_time', 'source_db_cluster_identifier', 'source_db_instance_automated_backups_arn', 'source_db_instance_identifier', 'source_dbi_resource_id', 'source_region', 'storage_encrypted', 'storage_throughput', 'storage_type', 'tags', 'timezone', 'use_default_processor_features', 'use_latest_restorable_time', 'vpc_security_groups']
    _method_names: typing.ClassVar[list[str]] = ['CertificateDetailsProperty', 'DBInstanceRoleProperty', 'EndpointProperty', 'MasterUserSecretProperty', 'ProcessorFeatureProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBInstance'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBInstanceDefConfig] = pydantic.Field(None)


class CfnDBInstanceDefConfig(pydantic.BaseModel):
    CertificateDetailsProperty: typing.Optional[list[CfnDBInstanceDefCertificatedetailspropertyParams]] = pydantic.Field(None, description='')
    DBInstanceRoleProperty: typing.Optional[list[CfnDBInstanceDefDbinstancerolepropertyParams]] = pydantic.Field(None, description='')
    EndpointProperty: typing.Optional[list[CfnDBInstanceDefEndpointpropertyParams]] = pydantic.Field(None, description='')
    MasterUserSecretProperty: typing.Optional[list[CfnDBInstanceDefMasterusersecretpropertyParams]] = pydantic.Field(None, description='')
    ProcessorFeatureProperty: typing.Optional[list[CfnDBInstanceDefProcessorfeaturepropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnDBInstanceDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBInstanceDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBInstanceDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBInstanceDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBInstanceDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBInstanceDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBInstanceDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBInstanceDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBInstanceDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBInstanceDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBInstanceDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBInstanceDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBInstanceDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDBInstanceDefCertificatedetailspropertyParams(pydantic.BaseModel):
    ca_identifier: typing.Optional[str] = pydantic.Field(None, description='')
    valid_till: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBInstanceDefDbinstancerolepropertyParams(pydantic.BaseModel):
    feature_name: str = pydantic.Field(..., description='')
    role_arn: str = pydantic.Field(..., description='')
    ...

class CfnDBInstanceDefEndpointpropertyParams(pydantic.BaseModel):
    address: typing.Optional[str] = pydantic.Field(None, description='')
    hosted_zone_id: typing.Optional[str] = pydantic.Field(None, description='')
    port: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBInstanceDefMasterusersecretpropertyParams(pydantic.BaseModel):
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    secret_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBInstanceDefProcessorfeaturepropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBInstanceDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBInstanceDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBInstanceDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBInstanceDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBInstanceDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBInstanceDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBInstanceDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBInstanceDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBInstanceDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBInstanceDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBInstanceDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBInstanceDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBInstanceDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBParameterGroup
class CfnDBParameterGroupDef(BaseCfnResource):
    description: str = pydantic.Field(..., description='Provides the customer-specified description for this DB parameter group.\n')
    family: str = pydantic.Field(..., description='The DB parameter group family name. A DB parameter group can be associated with one and only one DB parameter group family, and can be applied only to a DB instance running a DB engine and engine version compatible with that DB parameter group family. .. epigraph:: The DB parameter group family can\'t be changed when updating a DB parameter group. To list all of the available parameter group families, use the following command: ``aws rds describe-db-engine-versions --query "DBEngineVersions[].DBParameterGroupFamily"`` The output contains duplicates. For more information, see ``[CreateDBParameterGroup](https://docs.aws.amazon.com//AmazonRDS/latest/APIReference/API_CreateDBParameterGroup.html)`` .\n')
    db_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the DB parameter group. Constraints: - Must be 1 to 255 letters, numbers, or hyphens. - First character must be a letter - Can't end with a hyphen or contain two consecutive hyphens If you don't specify a value for ``DBParameterGroupName`` property, a name is automatically created for the DB parameter group. .. epigraph:: This value is stored as a lowercase string.\n")
    parameters: typing.Any = pydantic.Field(None, description="An array of parameter names and values for the parameter update. At least one parameter name and value must be supplied. Subsequent arguments are optional. For more information about DB parameters and DB parameter groups for Amazon RDS DB engines, see `Working with DB Parameter Groups <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html>`_ in the *Amazon RDS User Guide* . For more information about DB cluster and DB instance parameters and parameter groups for Amazon Aurora DB engines, see `Working with DB Parameter Groups and DB Cluster Parameter Groups <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html>`_ in the *Amazon Aurora User Guide* . .. epigraph:: AWS CloudFormation doesn't support specifying an apply method for each individual parameter. The default apply method for each parameter is used.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB parameter group. .. epigraph:: Currently, this is the only property that supports drift detection.')
    _init_params: typing.ClassVar[list[str]] = ['description', 'family', 'db_parameter_group_name', 'parameters', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBParameterGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBParameterGroupDefConfig] = pydantic.Field(None)


class CfnDBParameterGroupDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnDBParameterGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBParameterGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBParameterGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBParameterGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBParameterGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBParameterGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBParameterGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBParameterGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBParameterGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBParameterGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBParameterGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBParameterGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBParameterGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDBParameterGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBParameterGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBParameterGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBParameterGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBParameterGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBParameterGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBParameterGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBParameterGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBParameterGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBParameterGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBParameterGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBParameterGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBParameterGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBParameterGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBProxy
class CfnDBProxyDef(BaseCfnResource):
    auth: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBProxy_AuthFormatPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='The authorization mechanism that the proxy uses.\n')
    db_proxy_name: str = pydantic.Field(..., description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region . An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens.\n")
    engine_family: str = pydantic.Field(..., description='The kinds of databases that the proxy can connect to. This value determines which database network protocol the proxy recognizes when it interprets network traffic to and from the database. For Aurora MySQL, RDS for MariaDB, and RDS for MySQL databases, specify ``MYSQL`` . For Aurora PostgreSQL and RDS for PostgreSQL databases, specify ``POSTGRESQL`` . For RDS for Microsoft SQL Server, specify ``SQLSERVER`` . *Valid values* : ``MYSQL`` | ``POSTGRESQL`` | ``SQLSERVER``\n')
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the IAM role that the proxy uses to access secrets in AWS Secrets Manager.\n')
    vpc_subnet_ids: typing.Sequence[str] = pydantic.Field(..., description='One or more VPC subnet IDs to associate with the new proxy.\n')
    debug_logging: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs.\n')
    idle_client_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database.\n')
    require_tls: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.CfnDBProxy_TagFormatPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional set of key-value pairs to associate arbitrary data of your choosing with the proxy.\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="One or more VPC security group IDs to associate with the new proxy. If you plan to update the resource, don't specify VPC security groups in a shared VPC.")
    _init_params: typing.ClassVar[list[str]] = ['auth', 'db_proxy_name', 'engine_family', 'role_arn', 'vpc_subnet_ids', 'debug_logging', 'idle_client_timeout', 'require_tls', 'tags', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = ['AuthFormatProperty', 'TagFormatProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBProxyDefConfig] = pydantic.Field(None)


class CfnDBProxyDefConfig(pydantic.BaseModel):
    AuthFormatProperty: typing.Optional[list[CfnDBProxyDefAuthformatpropertyParams]] = pydantic.Field(None, description='')
    TagFormatProperty: typing.Optional[list[CfnDBProxyDefTagformatpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnDBProxyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBProxyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBProxyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBProxyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBProxyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBProxyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBProxyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBProxyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBProxyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBProxyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBProxyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBProxyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBProxyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnDBProxyDefAuthformatpropertyParams(pydantic.BaseModel):
    auth_scheme: typing.Optional[str] = pydantic.Field(None, description='')
    client_password_auth_type: typing.Optional[str] = pydantic.Field(None, description='')
    description: typing.Optional[str] = pydantic.Field(None, description='')
    iam_auth: typing.Optional[str] = pydantic.Field(None, description='')
    secret_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBProxyDefTagformatpropertyParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBProxyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBProxyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBProxyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBProxyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBProxyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBProxyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBProxyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBProxyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBProxyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBProxyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBProxyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBProxyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBProxyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBProxyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBProxyEndpoint
class CfnDBProxyEndpointDef(BaseCfnResource):
    db_proxy_endpoint_name: str = pydantic.Field(..., description='The name of the DB proxy endpoint to create.\n')
    db_proxy_name: str = pydantic.Field(..., description='The name of the DB proxy associated with the DB proxy endpoint that you create.\n')
    vpc_subnet_ids: typing.Sequence[str] = pydantic.Field(..., description='The VPC subnet IDs for the DB proxy endpoint that you create. You can specify a different set of subnet IDs than for the original DB proxy.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.CfnDBProxyEndpoint_TagFormatPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional set of key-value pairs to associate arbitrary data of your choosing with the proxy.\n')
    target_role: typing.Optional[str] = pydantic.Field(None, description='A value that indicates whether the DB proxy endpoint can be used for read/write or read-only operations. Valid Values: ``READ_WRITE | READ_ONLY``\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The VPC security group IDs for the DB proxy endpoint that you create. You can specify a different set of security group IDs than for the original DB proxy. The default is the default security group for the VPC.')
    _init_params: typing.ClassVar[list[str]] = ['db_proxy_endpoint_name', 'db_proxy_name', 'vpc_subnet_ids', 'tags', 'target_role', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = ['TagFormatProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxyEndpoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBProxyEndpointDefConfig] = pydantic.Field(None)


class CfnDBProxyEndpointDefConfig(pydantic.BaseModel):
    TagFormatProperty: typing.Optional[list[CfnDBProxyEndpointDefTagformatpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnDBProxyEndpointDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBProxyEndpointDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBProxyEndpointDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBProxyEndpointDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBProxyEndpointDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBProxyEndpointDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBProxyEndpointDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBProxyEndpointDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBProxyEndpointDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBProxyEndpointDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBProxyEndpointDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBProxyEndpointDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBProxyEndpointDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    attr_is_default_config: typing.Optional[models._interface_methods.CoreIResolvableDefConfig] = pydantic.Field(None)

class CfnDBProxyEndpointDefTagformatpropertyParams(pydantic.BaseModel):
    key: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBProxyEndpointDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBProxyEndpointDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBProxyEndpointDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBProxyEndpointDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBProxyEndpointDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBProxyEndpointDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBProxyEndpointDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBProxyEndpointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBProxyEndpointDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBProxyEndpointDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBProxyEndpointDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBProxyEndpointDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBProxyEndpointDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBProxyEndpointDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBProxyTargetGroup
class CfnDBProxyTargetGroupDef(BaseCfnResource):
    db_proxy_name: str = pydantic.Field(..., description='The identifier of the ``DBProxy`` that is associated with the ``DBProxyTargetGroup`` .\n')
    target_group_name: str = pydantic.Field(..., description='The identifier for the target group. .. epigraph:: Currently, this property must be set to ``default`` .\n')
    connection_pool_configuration_info: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBProxyTargetGroup_ConnectionPoolConfigurationInfoFormatPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings that control the size and behavior of the connection pool associated with a ``DBProxyTargetGroup`` .\n')
    db_cluster_identifiers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more DB cluster identifiers.\n')
    db_instance_identifiers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more DB instance identifiers.')
    _init_params: typing.ClassVar[list[str]] = ['db_proxy_name', 'target_group_name', 'connection_pool_configuration_info', 'db_cluster_identifiers', 'db_instance_identifiers']
    _method_names: typing.ClassVar[list[str]] = ['ConnectionPoolConfigurationInfoFormatProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxyTargetGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBProxyTargetGroupDefConfig] = pydantic.Field(None)


class CfnDBProxyTargetGroupDefConfig(pydantic.BaseModel):
    ConnectionPoolConfigurationInfoFormatProperty: typing.Optional[list[CfnDBProxyTargetGroupDefConnectionpoolconfigurationinfoformatpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnDBProxyTargetGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBProxyTargetGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBProxyTargetGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBProxyTargetGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBProxyTargetGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBProxyTargetGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBProxyTargetGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBProxyTargetGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBProxyTargetGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBProxyTargetGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBProxyTargetGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBProxyTargetGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBProxyTargetGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnDBProxyTargetGroupDefConnectionpoolconfigurationinfoformatpropertyParams(pydantic.BaseModel):
    connection_borrow_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    init_query: typing.Optional[str] = pydantic.Field(None, description='')
    max_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='')
    max_idle_connections_percent: typing.Union[int, float, None] = pydantic.Field(None, description='')
    session_pinning_filters: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnDBProxyTargetGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBProxyTargetGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBProxyTargetGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBProxyTargetGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBProxyTargetGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBProxyTargetGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBProxyTargetGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBProxyTargetGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBProxyTargetGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBProxyTargetGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBProxyTargetGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBProxyTargetGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBProxyTargetGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBProxyTargetGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBSecurityGroup
class CfnDBSecurityGroupDef(BaseCfnResource):
    db_security_group_ingress: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBSecurityGroup_IngressPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='Ingress rules to be applied to the DB security group.\n')
    group_description: str = pydantic.Field(..., description='Provides the description of the DB security group.\n')
    ec2_vpc_id: typing.Optional[str] = pydantic.Field(None, description='The identifier of an Amazon VPC. This property indicates the VPC that this DB security group belongs to. .. epigraph:: The ``EC2VpcId`` property is for backward compatibility with older regions, and is no longer recommended for providing security information to an RDS DB instance.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB security group.')
    _init_params: typing.ClassVar[list[str]] = ['db_security_group_ingress', 'group_description', 'ec2_vpc_id', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['IngressProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBSecurityGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBSecurityGroupDefConfig] = pydantic.Field(None)


class CfnDBSecurityGroupDefConfig(pydantic.BaseModel):
    IngressProperty: typing.Optional[list[CfnDBSecurityGroupDefIngresspropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnDBSecurityGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBSecurityGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBSecurityGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBSecurityGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBSecurityGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBSecurityGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBSecurityGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBSecurityGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBSecurityGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBSecurityGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBSecurityGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBSecurityGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBSecurityGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDBSecurityGroupDefIngresspropertyParams(pydantic.BaseModel):
    cidrip: typing.Optional[str] = pydantic.Field(None, description='')
    ec2_security_group_id: typing.Optional[str] = pydantic.Field(None, description='')
    ec2_security_group_name: typing.Optional[str] = pydantic.Field(None, description='')
    ec2_security_group_owner_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnDBSecurityGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBSecurityGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBSecurityGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBSecurityGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBSecurityGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBSecurityGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBSecurityGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBSecurityGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBSecurityGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBSecurityGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBSecurityGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBSecurityGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBSecurityGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBSecurityGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBSecurityGroupIngress
class CfnDBSecurityGroupIngressDef(BaseCfnResource):
    db_security_group_name: str = pydantic.Field(..., description='The name of the DB security group to add authorization to.\n')
    cidrip: typing.Optional[str] = pydantic.Field(None, description='The IP range to authorize.\n')
    ec2_security_group_id: typing.Optional[str] = pydantic.Field(None, description='Id of the EC2 security group to authorize. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n')
    ec2_security_group_name: typing.Optional[str] = pydantic.Field(None, description='Name of the EC2 security group to authorize. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n')
    ec2_security_group_owner_id: typing.Optional[str] = pydantic.Field(None, description="AWS account number of the owner of the EC2 security group specified in the ``EC2SecurityGroupName`` parameter. The AWS access key ID isn't an acceptable value. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.")
    _init_params: typing.ClassVar[list[str]] = ['db_security_group_name', 'cidrip', 'ec2_security_group_id', 'ec2_security_group_name', 'ec2_security_group_owner_id']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBSecurityGroupIngress'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBSecurityGroupIngressDefConfig] = pydantic.Field(None)


class CfnDBSecurityGroupIngressDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnDBSecurityGroupIngressDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBSecurityGroupIngressDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBSecurityGroupIngressDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBSecurityGroupIngressDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBSecurityGroupIngressDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBSecurityGroupIngressDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBSecurityGroupIngressDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBSecurityGroupIngressDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBSecurityGroupIngressDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBSecurityGroupIngressDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBSecurityGroupIngressDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBSecurityGroupIngressDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBSecurityGroupIngressDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnDBSecurityGroupIngressDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBSecurityGroupIngressDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBSecurityGroupIngressDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBSecurityGroupIngressDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBSecurityGroupIngressDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBSecurityGroupIngressDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBSecurityGroupIngressDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBSecurityGroupIngressDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBSecurityGroupIngressDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBSecurityGroupIngressDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBSecurityGroupIngressDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBSecurityGroupIngressDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBSecurityGroupIngressDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBSecurityGroupIngressDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBSubnetGroup
class CfnDBSubnetGroupDef(BaseCfnResource):
    db_subnet_group_description: str = pydantic.Field(..., description='The description for the DB subnet group.\n')
    subnet_ids: typing.Sequence[str] = pydantic.Field(..., description='The EC2 Subnet IDs for the DB subnet group.\n')
    db_subnet_group_name: typing.Optional[str] = pydantic.Field(None, description='The name for the DB subnet group. This value is stored as a lowercase string. Constraints: Must contain no more than 255 lowercase alphanumeric characters or hyphens. Must not be "Default". Example: ``mysubnetgroup``\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB subnet group.')
    _init_params: typing.ClassVar[list[str]] = ['db_subnet_group_description', 'subnet_ids', 'db_subnet_group_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBSubnetGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnDBSubnetGroupDefConfig] = pydantic.Field(None)


class CfnDBSubnetGroupDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnDBSubnetGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnDBSubnetGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnDBSubnetGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnDBSubnetGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnDBSubnetGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnDBSubnetGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnDBSubnetGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnDBSubnetGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnDBSubnetGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnDBSubnetGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnDBSubnetGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnDBSubnetGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnDBSubnetGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDBSubnetGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDBSubnetGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBSubnetGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDBSubnetGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBSubnetGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDBSubnetGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDBSubnetGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDBSubnetGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDBSubnetGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDBSubnetGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDBSubnetGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnDBSubnetGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDBSubnetGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDBSubnetGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnEventSubscription
class CfnEventSubscriptionDef(BaseCfnResource):
    sns_topic_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the SNS topic created for event notification. The ARN is created by Amazon SNS when you create a topic and subscribe to it.\n')
    enabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to activate the subscription. If the event notification subscription isn't activated, the subscription is created but not active.\n")
    event_categories: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of event categories for a particular source type ( ``SourceType`` ) that you want to subscribe to. You can see a list of the categories for a given source type in the "Amazon RDS event categories and event messages" section of the `*Amazon RDS User Guide* <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html>`_ or the `*Amazon Aurora User Guide* <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_Events.Messages.html>`_ . You can also see this list by using the ``DescribeEventCategories`` operation.\n')
    source_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The list of identifiers of the event sources for which events are returned. If not specified, then all sources are included in the response. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens. It can't end with a hyphen or contain two consecutive hyphens. Constraints: - If a ``SourceIds`` value is supplied, ``SourceType`` must also be provided. - If the source type is a DB instance, a ``DBInstanceIdentifier`` value must be supplied. - If the source type is a DB cluster, a ``DBClusterIdentifier`` value must be supplied. - If the source type is a DB parameter group, a ``DBParameterGroupName`` value must be supplied. - If the source type is a DB security group, a ``DBSecurityGroupName`` value must be supplied. - If the source type is a DB snapshot, a ``DBSnapshotIdentifier`` value must be supplied. - If the source type is a DB cluster snapshot, a ``DBClusterSnapshotIdentifier`` value must be supplied.\n")
    source_type: typing.Optional[str] = pydantic.Field(None, description="The type of source that is generating the events. For example, if you want to be notified of events generated by a DB instance, set this parameter to ``db-instance`` . If this value isn't specified, all events are returned. Valid values: ``db-instance`` | ``db-cluster`` | ``db-parameter-group`` | ``db-security-group`` | ``db-snapshot`` | ``db-cluster-snapshot``\n")
    subscription_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription. Constraints: The name must be less than 255 characters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this subscription.')
    _init_params: typing.ClassVar[list[str]] = ['sns_topic_arn', 'enabled', 'event_categories', 'source_ids', 'source_type', 'subscription_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnEventSubscription'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnEventSubscriptionDefConfig] = pydantic.Field(None)


class CfnEventSubscriptionDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnEventSubscriptionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnEventSubscriptionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnEventSubscriptionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnEventSubscriptionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnEventSubscriptionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnEventSubscriptionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnEventSubscriptionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnEventSubscriptionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnEventSubscriptionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnEventSubscriptionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnEventSubscriptionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnEventSubscriptionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnEventSubscriptionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnEventSubscriptionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnEventSubscriptionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventSubscriptionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnEventSubscriptionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventSubscriptionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnEventSubscriptionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnEventSubscriptionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnEventSubscriptionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnEventSubscriptionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnEventSubscriptionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventSubscriptionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnEventSubscriptionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnEventSubscriptionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventSubscriptionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnGlobalCluster
class CfnGlobalClusterDef(BaseCfnResource):
    deletion_protection: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="The deletion protection setting for the new global database. The global database can't be deleted when deletion protection is enabled.\n")
    engine: typing.Optional[str] = pydantic.Field(None, description="The name of the database engine to be used for this DB cluster. If this property isn't specified, the database engine is derived from the source DB cluster specified by the ``SourceDBClusterIdentifier`` property. .. epigraph:: If the ``SourceDBClusterIdentifier`` property isn't specified, this property is required. If the ``SourceDBClusterIdentifier`` property is specified, make sure this property isn't specified.\n")
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The engine version of the Aurora global database.\n')
    global_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='The cluster identifier of the global database cluster.\n')
    source_db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="The DB cluster identifier or Amazon Resource Name (ARN) to use as the primary cluster of the global database. .. epigraph:: If the ``Engine`` property isn't specified, this property is required. If the ``Engine`` property is specified, make sure this property isn't specified.\n")
    storage_encrypted: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='The storage encryption setting for the global database cluster.')
    _init_params: typing.ClassVar[list[str]] = ['deletion_protection', 'engine', 'engine_version', 'global_cluster_identifier', 'source_db_cluster_identifier', 'storage_encrypted']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnGlobalCluster'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnGlobalClusterDefConfig] = pydantic.Field(None)


class CfnGlobalClusterDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnGlobalClusterDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnGlobalClusterDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnGlobalClusterDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnGlobalClusterDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnGlobalClusterDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnGlobalClusterDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnGlobalClusterDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnGlobalClusterDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnGlobalClusterDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnGlobalClusterDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnGlobalClusterDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnGlobalClusterDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnGlobalClusterDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnGlobalClusterDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnGlobalClusterDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnGlobalClusterDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnGlobalClusterDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnGlobalClusterDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnGlobalClusterDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnGlobalClusterDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnGlobalClusterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnGlobalClusterDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnGlobalClusterDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnGlobalClusterDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnGlobalClusterDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnGlobalClusterDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnGlobalClusterDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnOptionGroup
class CfnOptionGroupDef(BaseCfnResource):
    engine_name: str = pydantic.Field(..., description='Specifies the name of the engine that this option group should be associated with. Valid Values: - ``mariadb`` - ``mysql`` - ``oracle-ee`` - ``oracle-ee-cdb`` - ``oracle-se2`` - ``oracle-se2-cdb`` - ``postgres`` - ``sqlserver-ee`` - ``sqlserver-se`` - ``sqlserver-ex`` - ``sqlserver-web``\n')
    major_engine_version: str = pydantic.Field(..., description='Specifies the major version of the engine that this option group should be associated with.\n')
    option_group_description: str = pydantic.Field(..., description='The description of the option group.\n')
    option_configurations: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnOptionGroup_OptionConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of options and the settings for each option.\n')
    option_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the option group to be created. Constraints: - Must be 1 to 255 letters, numbers, or hyphens - First character must be a letter - Can't end with a hyphen or contain two consecutive hyphens Example: ``myoptiongroup`` If you don't specify a value for ``OptionGroupName`` property, a name is automatically created for the option group. .. epigraph:: This value is stored as a lowercase string.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this option group.')
    _init_params: typing.ClassVar[list[str]] = ['engine_name', 'major_engine_version', 'option_group_description', 'option_configurations', 'option_group_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['OptionConfigurationProperty', 'OptionSettingProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnOptionGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnOptionGroupDefConfig] = pydantic.Field(None)


class CfnOptionGroupDefConfig(pydantic.BaseModel):
    OptionConfigurationProperty: typing.Optional[list[CfnOptionGroupDefOptionconfigurationpropertyParams]] = pydantic.Field(None, description='')
    OptionSettingProperty: typing.Optional[list[CfnOptionGroupDefOptionsettingpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnOptionGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnOptionGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnOptionGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnOptionGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnOptionGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnOptionGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnOptionGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnOptionGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnOptionGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnOptionGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnOptionGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnOptionGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnOptionGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnOptionGroupDefOptionconfigurationpropertyParams(pydantic.BaseModel):
    option_name: str = pydantic.Field(..., description='')
    db_security_group_memberships: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    option_settings: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnOptionGroup_OptionSettingPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    option_version: typing.Optional[str] = pydantic.Field(None, description='')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    vpc_security_group_memberships: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnOptionGroupDefOptionsettingpropertyParams(pydantic.BaseModel):
    name: typing.Optional[str] = pydantic.Field(None, description='')
    value: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnOptionGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnOptionGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnOptionGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnOptionGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnOptionGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnOptionGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnOptionGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnOptionGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnOptionGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnOptionGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnOptionGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='- tree inspector to collect and process attributes.')
    ...

class CfnOptionGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnOptionGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnOptionGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_rds.CfnDBClusterParameterGroupProps
class CfnDBClusterParameterGroupPropsDef(BaseCfnProperty):
    description: str = pydantic.Field(..., description='A friendly description for this DB cluster parameter group.\n')
    family: str = pydantic.Field(..., description='The DB cluster parameter group family name. A DB cluster parameter group can be associated with one and only one DB cluster parameter group family, and can be applied only to a DB cluster running a DB engine and engine version compatible with that DB cluster parameter group family. .. epigraph:: The DB cluster parameter group family can\'t be changed when updating a DB cluster parameter group. To list all of the available parameter group families, use the following command: ``aws rds describe-db-engine-versions --query "DBEngineVersions[].DBParameterGroupFamily"`` The output contains duplicates. For more information, see ``[CreateDBClusterParameterGroup](https://docs.aws.amazon.com//AmazonRDS/latest/APIReference/API_CreateDBClusterParameterGroup.html)`` .\n')
    parameters: typing.Any = pydantic.Field(..., description='Provides a list of parameters for the DB cluster parameter group.\n')
    db_cluster_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the DB cluster parameter group. Constraints: - Must not match the name of an existing DB cluster parameter group. If you don't specify a value for ``DBClusterParameterGroupName`` property, a name is automatically created for the DB cluster parameter group. .. epigraph:: This value is stored as a lowercase string.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB cluster parameter group.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbclusterparametergroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # parameters: Any\n\n    cfn_dBCluster_parameter_group_props = rds.CfnDBClusterParameterGroupProps(\n        description="description",\n        family="family",\n        parameters=parameters,\n\n        # the properties below are optional\n        db_cluster_parameter_group_name="dbClusterParameterGroupName",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['description', 'family', 'parameters', 'db_cluster_parameter_group_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBClusterParameterGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBClusterProps
class CfnDBClusterPropsDef(BaseCfnProperty):
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of storage in gibibytes (GiB) to allocate to each DB instance in the Multi-AZ DB cluster. This setting is required to create a Multi-AZ DB cluster. Valid for: Multi-AZ DB clusters only\n')
    associated_roles: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_DBClusterRolePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Provides a list of the AWS Identity and Access Management (IAM) roles that are associated with the DB cluster. IAM roles that are associated with a DB cluster grant permission for the DB cluster to access other Amazon Web Services on your behalf. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    auto_minor_version_upgrade: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether minor engine upgrades are applied automatically to the DB cluster during the maintenance window. By default, minor engine upgrades are applied automatically. Valid for: Multi-AZ DB clusters only\n')
    availability_zones: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of Availability Zones (AZs) where instances in the DB cluster can be created. For information on AWS Regions and Availability Zones, see `Choosing the Regions and Availability Zones <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.RegionsAndAvailabilityZones.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n')
    backtrack_window: typing.Union[int, float, None] = pydantic.Field(None, description='The target backtrack window, in seconds. To disable backtracking, set this value to 0. .. epigraph:: Currently, Backtrack is only supported for Aurora MySQL DB clusters. Default: 0 Constraints: - If specified, this value must be set to a number from 0 to 259,200 (72 hours). Valid for: Aurora MySQL DB clusters only\n')
    backup_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days for which automated backups are retained. Default: 1 Constraints: - Must be a value from 1 to 35 Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    copy_tags_to_snapshot: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to copy all tags from the DB cluster to snapshots of the DB cluster. The default is not to copy them. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description="The name of your database. If you don't provide a name, then Amazon RDS won't create a database in this DB cluster. For naming constraints, see `Naming Constraints <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/CHAP_Limits.html#RDS_Limits.Constraints>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="The DB cluster identifier. This parameter is stored as a lowercase string. Constraints: - Must contain from 1 to 63 letters, numbers, or hyphens. - First character must be a letter. - Can't end with a hyphen or contain two consecutive hyphens. Example: ``my-cluster1`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    db_cluster_instance_class: typing.Optional[str] = pydantic.Field(None, description='The compute and memory capacity of each DB instance in the Multi-AZ DB cluster, for example db.m6gd.xlarge. Not all DB instance classes are available in all AWS Regions , or for all database engines. For the full list of DB instance classes and availability for your engine, see `DB instance class <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html>`_ in the *Amazon RDS User Guide* . This setting is required to create a Multi-AZ DB cluster. Valid for: Multi-AZ DB clusters only\n')
    db_cluster_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of the DB cluster parameter group to associate with this DB cluster. .. epigraph:: If you apply a parameter group to an existing DB cluster, then its DB instances might need to reboot. This can result in an outage while the DB instances are rebooting. If you apply a change to parameter group associated with a stopped DB cluster, then the update stack waits until the DB cluster is started. To list all of the available DB cluster parameter group names, use the following command: ``aws rds describe-db-cluster-parameter-groups --query "DBClusterParameterGroups[].DBClusterParameterGroupName" --output text`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    db_instance_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the DB parameter group to apply to all instances of the DB cluster. .. epigraph:: When you apply a parameter group using the ``DBInstanceParameterGroupName`` parameter, the DB cluster isn't rebooted automatically. Also, parameter changes are applied immediately rather than during the next maintenance window. Default: The existing name setting Constraints: - The DB parameter group must be in the same DB parameter group family as this DB cluster.\n")
    db_subnet_group_name: typing.Optional[str] = pydantic.Field(None, description="A DB subnet group that you want to associate with this DB cluster. If you are restoring a DB cluster to a point in time with ``RestoreType`` set to ``copy-on-write`` , and don't specify a DB subnet group name, then the DB cluster is restored with a default DB subnet group. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    db_system_id: typing.Optional[str] = pydantic.Field(None, description='Reserved for future use.\n')
    deletion_protection: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB cluster has deletion protection enabled. The database can't be deleted when deletion protection is enabled. By default, deletion protection is disabled. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    domain: typing.Optional[str] = pydantic.Field(None, description='Indicates the directory ID of the Active Directory to create the DB cluster. For Amazon Aurora DB clusters, Amazon RDS can use Kerberos authentication to authenticate users that connect to the DB cluster. For more information, see `Kerberos authentication <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/kerberos-authentication.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n')
    domain_iam_role_name: typing.Optional[str] = pydantic.Field(None, description='Specifies the name of the IAM role to use when making API calls to the Directory Service. Valid for: Aurora DB clusters only\n')
    enable_cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. The values in the list depend on the DB engine being used. For more information, see `Publishing Database Logs to Amazon CloudWatch Logs <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_LogAccess.html#USER_LogAccess.Procedural.UploadtoCloudWatch>`_ in the *Amazon Aurora User Guide* . *Aurora MySQL* Valid values: ``audit`` , ``error`` , ``general`` , ``slowquery`` *Aurora PostgreSQL* Valid values: ``postgresql`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    enable_http_endpoint: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to enable the HTTP endpoint for an Aurora Serverless DB cluster. By default, the HTTP endpoint is disabled. When enabled, the HTTP endpoint provides a connectionless web service API for running SQL queries on the Aurora Serverless DB cluster. You can also query your database from inside the RDS console with the query editor. For more information, see `Using the Data API for Aurora Serverless <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/data-api.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n')
    enable_iam_database_authentication: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. By default, mapping is disabled. For more information, see `IAM Database Authentication <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBAuth.html>`_ in the *Amazon Aurora User Guide.* Valid for: Aurora DB clusters only\n')
    engine: typing.Optional[str] = pydantic.Field(None, description='The name of the database engine to be used for this DB cluster. Valid Values: - ``aurora-mysql`` - ``aurora-postgresql`` - ``mysql`` - ``postgres`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    engine_mode: typing.Optional[str] = pydantic.Field(None, description="The DB engine mode of the DB cluster, either ``provisioned`` or ``serverless`` . The ``serverless`` engine mode only supports Aurora Serverless v1. Currently, AWS CloudFormation doesn't support Aurora Serverless v2. Limitations and requirements apply to some DB engine modes. For more information, see the following sections in the *Amazon Aurora User Guide* : - `Limitations of Aurora Serverless v1 <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless.html#aurora-serverless.limitations>`_ - `Requirements for Aurora Serverless v2 <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.requirements.html>`_ - `Limitations of parallel query <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-mysql-parallel-query.html#aurora-mysql-parallel-query-limitations>`_ - `Limitations of Aurora global databases <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html#aurora-global-database.limitations>`_ Valid for: Aurora DB clusters only\n")
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The version number of the database engine to use. To list all of the available engine versions for Aurora MySQL version 2 (5.7-compatible) and version 3 (8.0-compatible), use the following command: ``aws rds describe-db-engine-versions --engine aurora-mysql --query "DBEngineVersions[].EngineVersion"`` You can supply either ``5.7`` or ``8.0`` to use the default engine version for Aurora MySQL version 2 or version 3, respectively. To list all of the available engine versions for Aurora PostgreSQL, use the following command: ``aws rds describe-db-engine-versions --engine aurora-postgresql --query "DBEngineVersions[].EngineVersion"`` To list all of the available engine versions for RDS for MySQL, use the following command: ``aws rds describe-db-engine-versions --engine mysql --query "DBEngineVersions[].EngineVersion"`` To list all of the available engine versions for RDS for PostgreSQL, use the following command: ``aws rds describe-db-engine-versions --engine postgres --query "DBEngineVersions[].EngineVersion"`` *Aurora MySQL* For information, see `Database engine updates for Amazon Aurora MySQL <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Updates.html>`_ in the *Amazon Aurora User Guide* . *Aurora PostgreSQL* For information, see `Amazon Aurora PostgreSQL releases and engine versions <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraPostgreSQL.Updates.20180305.html>`_ in the *Amazon Aurora User Guide* . *MySQL* For information, see `Amazon RDS for MySQL <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.VersionMgmt>`_ in the *Amazon RDS User Guide* . *PostgreSQL* For information, see `Amazon RDS for PostgreSQL <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts>`_ in the *Amazon RDS User Guide* . Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    global_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="If you are configuring an Aurora global database cluster and want your Aurora DB cluster to be a secondary member in the global database cluster, specify the global cluster ID of the global database cluster. To define the primary database cluster of the global cluster, use the `AWS::RDS::GlobalCluster <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-globalcluster.html>`_ resource. If you aren't configuring a global database cluster, don't specify this property. .. epigraph:: To remove the DB cluster from a global database cluster, specify an empty value for the ``GlobalClusterIdentifier`` property. For information about Aurora global databases, see `Working with Amazon Aurora Global Databases <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html>`_ in the *Amazon Aurora User Guide* . Valid for: Aurora DB clusters only\n")
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of Provisioned IOPS (input/output operations per second) to be initially allocated for each DB instance in the Multi-AZ DB cluster. For information about valid IOPS values, see `Provisioned IOPS storage <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html#USER_PIOPS>`_ in the *Amazon RDS User Guide* . This setting is required to create a Multi-AZ DB cluster. Constraints: Must be a multiple between .5 and 50 of the storage amount for the DB cluster. Valid for: Multi-AZ DB clusters only\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the AWS KMS key that is used to encrypt the database instances in the DB cluster, such as ``arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef`` . If you enable the ``StorageEncrypted`` property but don't specify this property, the default KMS key is used. If you specify this property, you must set the ``StorageEncrypted`` property to ``true`` . If you specify the ``SnapshotIdentifier`` property, the ``StorageEncrypted`` property value is inherited from the snapshot, and if the DB cluster is encrypted, the specified ``KmsKeyId`` property is used. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    manage_master_user_password: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to manage the master user password with AWS Secrets Manager. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide* and `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-secrets-manager.html>`_ in the *Amazon Aurora User Guide.* Constraints: - Can't manage the master user password with AWS Secrets Manager if ``MasterUserPassword`` is specified. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    master_username: typing.Optional[str] = pydantic.Field(None, description="The name of the master user for the DB cluster. .. epigraph:: If you specify the ``SourceDBClusterIdentifier`` , ``SnapshotIdentifier`` , or ``GlobalClusterIdentifier`` property, don't specify this property. The value is inherited from the source DB cluster, the snapshot, or the primary DB cluster for the global database cluster, respectively. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    master_user_password: typing.Optional[str] = pydantic.Field(None, description="The master password for the DB instance. .. epigraph:: If you specify the ``SourceDBClusterIdentifier`` , ``SnapshotIdentifier`` , or ``GlobalClusterIdentifier`` property, don't specify this property. The value is inherited from the source DB cluster, the snapshot, or the primary DB cluster for the global database cluster, respectively. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    master_user_secret: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_MasterUserSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Contains the secret managed by RDS in AWS Secrets Manager for the master user password. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide* and `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-secrets-manager.html>`_ in the *Amazon Aurora User Guide.*\n')
    monitoring_interval: typing.Union[int, float, None] = pydantic.Field(None, description='The interval, in seconds, between points when Enhanced Monitoring metrics are collected for the DB cluster. To turn off collecting Enhanced Monitoring metrics, specify 0. The default is 0. If ``MonitoringRoleArn`` is specified, also set ``MonitoringInterval`` to a value other than 0. Valid Values: ``0, 1, 5, 10, 15, 30, 60`` Valid for: Multi-AZ DB clusters only\n')
    monitoring_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the IAM role that permits RDS to send Enhanced Monitoring metrics to Amazon CloudWatch Logs. An example is ``arn:aws:iam:123456789012:role/emaccess`` . For information on creating a monitoring role, see `Setting up and enabling Enhanced Monitoring <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.Enabling>`_ in the *Amazon RDS User Guide* . If ``MonitoringInterval`` is set to a value other than 0, supply a ``MonitoringRoleArn`` value. Valid for: Multi-AZ DB clusters only\n')
    network_type: typing.Optional[str] = pydantic.Field(None, description='The network type of the DB cluster. Valid values: - ``IPV4`` - ``DUAL`` The network type is determined by the ``DBSubnetGroup`` specified for the DB cluster. A ``DBSubnetGroup`` can support only the IPv4 protocol or the IPv4 and IPv6 protocols ( ``DUAL`` ). For more information, see `Working with a DB instance in a VPC <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html>`_ in the *Amazon Aurora User Guide.* Valid for: Aurora DB clusters only\n')
    performance_insights_enabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to turn on Performance Insights for the DB cluster. For more information, see `Using Amazon Performance Insights <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.html>`_ in the *Amazon RDS User Guide* . Valid for: Multi-AZ DB clusters only\n')
    performance_insights_kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The AWS KMS key identifier for encryption of Performance Insights data. The AWS KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key. If you don't specify a value for ``PerformanceInsightsKMSKeyId`` , then Amazon RDS uses your default KMS key. There is a default KMS key for your AWS account . Your AWS account has a different default KMS key for each AWS Region . Valid for: Multi-AZ DB clusters only\n")
    performance_insights_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description="The number of days to retain Performance Insights data. The default is 7 days. The following values are valid:. - 7 - *month* * 31, where *month* is a number of months from 1-23 - 731 For example, the following values are valid: - 93 (3 months * 31) - 341 (11 months * 31) - 589 (19 months * 31) - 731 If you specify a retention period such as 94, which isn't a valid value, RDS issues an error. Valid for: Multi-AZ DB clusters only\n")
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port number on which the DB instances in the DB cluster accept connections. Default: - When ``EngineMode`` is ``provisioned`` , ``3306`` (for both Aurora MySQL and Aurora PostgreSQL) - When ``EngineMode`` is ``serverless`` : - ``3306`` when ``Engine`` is ``aurora`` or ``aurora-mysql`` - ``5432`` when ``Engine`` is ``aurora-postgresql`` .. epigraph:: The ``No interruption`` on update behavior only applies to DB clusters. If you are updating a DB instance, see `Port <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-port>`_ for the AWS::RDS::DBInstance resource. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are created. For more information, see `Backup Window <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.Backups.BackupWindow>`_ in the *Amazon Aurora User Guide.* Constraints: - Must be in the format ``hh24:mi-hh24:mi`` . - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range during which system maintenance can occur, in Universal Coordinated Time (UTC). Format: ``ddd:hh24:mi-ddd:hh24:mi`` The default is a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see `Adjusting the Preferred DB Cluster Maintenance Window <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_UpgradeDBInstance.Maintenance.html#AdjustingTheMaintenanceWindow.Aurora>`_ in the *Amazon Aurora User Guide.* Valid Days: Mon, Tue, Wed, Thu, Fri, Sat, Sun. Constraints: Minimum 30-minute window. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    publicly_accessible: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB cluster is publicly accessible. When the DB cluster is publicly accessible, its Domain Name System (DNS) endpoint resolves to the private IP address from within the DB cluster's virtual private cloud (VPC). It resolves to the public IP address from outside of the DB cluster's VPC. Access to the DB cluster is ultimately controlled by the security group it uses. That public access isn't permitted if the security group assigned to the DB cluster doesn't permit it. When the DB cluster isn't publicly accessible, it is an internal DB cluster with a DNS name that resolves to a private IP address. Default: The default behavior varies depending on whether ``DBSubnetGroupName`` is specified. If ``DBSubnetGroupName`` isn't specified, and ``PubliclyAccessible`` isn't specified, the following applies: - If the default VPC in the target Region doesnt have an internet gateway attached to it, the DB cluster is private. - If the default VPC in the target Region has an internet gateway attached to it, the DB cluster is public. If ``DBSubnetGroupName`` is specified, and ``PubliclyAccessible`` isn't specified, the following applies: - If the subnets are part of a VPC that doesnt have an internet gateway attached to it, the DB cluster is private. - If the subnets are part of a VPC that has an internet gateway attached to it, the DB cluster is public. Valid for: Multi-AZ DB clusters only\n")
    replication_source_identifier: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the source DB instance or DB cluster if this DB cluster is created as a read replica. Valid for: Aurora DB clusters only\n')
    restore_to_time: typing.Optional[str] = pydantic.Field(None, description="The date and time to restore the DB cluster to. Valid Values: Value must be a time in Universal Coordinated Time (UTC) format Constraints: - Must be before the latest restorable time for the DB instance - Must be specified if ``UseLatestRestorableTime`` parameter isn't provided - Can't be specified if the ``UseLatestRestorableTime`` parameter is enabled - Can't be specified if the ``RestoreType`` parameter is ``copy-on-write`` Example: ``2015-03-07T23:45:00Z`` Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    restore_type: typing.Optional[str] = pydantic.Field(None, description="The type of restore to be performed. You can specify one of the following values:. - ``full-copy`` - The new DB cluster is restored as a full copy of the source DB cluster. - ``copy-on-write`` - The new DB cluster is restored as a clone of the source DB cluster. If you don't specify a ``RestoreType`` value, then the new DB cluster is restored as a full copy of the source DB cluster. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    scaling_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_ScalingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ``ScalingConfiguration`` property type specifies the scaling configuration of an Aurora Serverless DB cluster. This property is only supported for Aurora Serverless v1. For Aurora Serverless v2, use ``ServerlessV2ScalingConfiguration`` property. Valid for: Aurora DB clusters only\n')
    serverless_v2_scaling_configuration: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBCluster_ServerlessV2ScalingConfigurationPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The ``ServerlessV2ScalingConfiguration`` property type specifies the scaling configuration of an Aurora Serverless V2 DB cluster. This property is only supported for Aurora Serverless v2. For Aurora Serverless v1, use ``ScalingConfiguration`` property. Valid for: Aurora DB clusters only\n')
    snapshot_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier for the DB snapshot or DB cluster snapshot to restore from. You can use either the name or the Amazon Resource Name (ARN) to specify a DB cluster snapshot. However, you can use only the ARN to specify a DB snapshot. After you restore a DB cluster with a ``SnapshotIdentifier`` property, you must specify the same ``SnapshotIdentifier`` property for any future updates to the DB cluster. When you specify this property for an update, the DB cluster is not restored from the snapshot again, and the data in the database is not changed. However, if you don't specify the ``SnapshotIdentifier`` property, an empty DB cluster is created, and the original DB cluster is deleted. If you specify a property that is different from the previous snapshot restore property, a new DB cluster is restored from the specified ``SnapshotIdentifier`` property, and the original DB cluster is deleted. If you specify the ``SnapshotIdentifier`` property to restore a DB cluster (as opposed to specifying it for DB cluster updates), then don't specify the following properties: - ``GlobalClusterIdentifier`` - ``MasterUsername`` - ``MasterUserPassword`` - ``ReplicationSourceIdentifier`` - ``RestoreType`` - ``SourceDBClusterIdentifier`` - ``SourceRegion`` - ``StorageEncrypted`` (for an encrypted snapshot) - ``UseLatestRestorableTime`` Constraints: - Must match the identifier of an existing Snapshot. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    source_db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='When restoring a DB cluster to a point in time, the identifier of the source DB cluster from which to restore. Constraints: - Must match the identifier of an existing DBCluster. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    source_region: typing.Optional[str] = pydantic.Field(None, description='The AWS Region which contains the source DB cluster when replicating a DB cluster. For example, ``us-east-1`` . Valid for: Aurora DB clusters only\n')
    storage_encrypted: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Indicates whether the DB cluster is encrypted. If you specify the ``KmsKeyId`` property, then you must enable encryption. If you specify the ``SourceDBClusterIdentifier`` property, don't specify this property. The value is inherited from the source DB cluster, and if the DB cluster is encrypted, the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot is encrypted, don't specify this property. The value is inherited from the snapshot, and the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot isn't encrypted, you can use this property to specify that the restored DB cluster is encrypted. Specify the ``KmsKeyId`` property for the KMS key to use for encryption. If you don't want the restored DB cluster to be encrypted, then don't set this property or set it to ``false`` . Valid for: Aurora DB clusters and Multi-AZ DB clusters\n")
    storage_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the storage type to be associated with the DB cluster. This setting is required to create a Multi-AZ DB cluster. When specified for a Multi-AZ DB cluster, a value for the ``Iops`` parameter is required. Valid values: ``aurora`` , ``aurora-iopt1`` (Aurora DB clusters); ``io1`` (Multi-AZ DB clusters) Default: ``aurora`` (Aurora DB clusters); ``io1`` (Multi-AZ DB clusters) Valid for: Aurora DB clusters and Multi-AZ DB clusters For more information on storage types for Aurora DB clusters, see `Storage configurations for Amazon Aurora DB clusters <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Overview.StorageReliability.html#aurora-storage-type>`_ . For more information on storage types for Multi-AZ DB clusters, see `Settings for creating Multi-AZ DB clusters <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/create-multi-az-db-cluster.html#create-multi-az-db-cluster-settings>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB cluster. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    use_latest_restorable_time: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to restore the DB cluster to the latest restorable backup time. By default, the DB cluster is not restored to the latest restorable backup time. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of EC2 VPC security groups to associate with this DB cluster. If you plan to update the resource, don\'t specify VPC security groups in a shared VPC. Valid for: Aurora DB clusters and Multi-AZ DB clusters\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbcluster.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBCluster_props = rds.CfnDBClusterProps(\n        allocated_storage=123,\n        associated_roles=[rds.CfnDBCluster.DBClusterRoleProperty(\n            role_arn="roleArn",\n\n            # the properties below are optional\n            feature_name="featureName"\n        )],\n        auto_minor_version_upgrade=False,\n        availability_zones=["availabilityZones"],\n        backtrack_window=123,\n        backup_retention_period=123,\n        copy_tags_to_snapshot=False,\n        database_name="databaseName",\n        db_cluster_identifier="dbClusterIdentifier",\n        db_cluster_instance_class="dbClusterInstanceClass",\n        db_cluster_parameter_group_name="dbClusterParameterGroupName",\n        db_instance_parameter_group_name="dbInstanceParameterGroupName",\n        db_subnet_group_name="dbSubnetGroupName",\n        db_system_id="dbSystemId",\n        deletion_protection=False,\n        domain="domain",\n        domain_iam_role_name="domainIamRoleName",\n        enable_cloudwatch_logs_exports=["enableCloudwatchLogsExports"],\n        enable_http_endpoint=False,\n        enable_iam_database_authentication=False,\n        engine="engine",\n        engine_mode="engineMode",\n        engine_version="engineVersion",\n        global_cluster_identifier="globalClusterIdentifier",\n        iops=123,\n        kms_key_id="kmsKeyId",\n        manage_master_user_password=False,\n        master_username="masterUsername",\n        master_user_password="masterUserPassword",\n        master_user_secret=rds.CfnDBCluster.MasterUserSecretProperty(\n            kms_key_id="kmsKeyId",\n            secret_arn="secretArn"\n        ),\n        monitoring_interval=123,\n        monitoring_role_arn="monitoringRoleArn",\n        network_type="networkType",\n        performance_insights_enabled=False,\n        performance_insights_kms_key_id="performanceInsightsKmsKeyId",\n        performance_insights_retention_period=123,\n        port=123,\n        preferred_backup_window="preferredBackupWindow",\n        preferred_maintenance_window="preferredMaintenanceWindow",\n        publicly_accessible=False,\n        replication_source_identifier="replicationSourceIdentifier",\n        restore_to_time="restoreToTime",\n        restore_type="restoreType",\n        scaling_configuration=rds.CfnDBCluster.ScalingConfigurationProperty(\n            auto_pause=False,\n            max_capacity=123,\n            min_capacity=123,\n            seconds_before_timeout=123,\n            seconds_until_auto_pause=123,\n            timeout_action="timeoutAction"\n        ),\n        serverless_v2_scaling_configuration=rds.CfnDBCluster.ServerlessV2ScalingConfigurationProperty(\n            max_capacity=123,\n            min_capacity=123\n        ),\n        snapshot_identifier="snapshotIdentifier",\n        source_db_cluster_identifier="sourceDbClusterIdentifier",\n        source_region="sourceRegion",\n        storage_encrypted=False,\n        storage_type="storageType",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        use_latest_restorable_time=False,\n        vpc_security_group_ids=["vpcSecurityGroupIds"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allocated_storage', 'associated_roles', 'auto_minor_version_upgrade', 'availability_zones', 'backtrack_window', 'backup_retention_period', 'copy_tags_to_snapshot', 'database_name', 'db_cluster_identifier', 'db_cluster_instance_class', 'db_cluster_parameter_group_name', 'db_instance_parameter_group_name', 'db_subnet_group_name', 'db_system_id', 'deletion_protection', 'domain', 'domain_iam_role_name', 'enable_cloudwatch_logs_exports', 'enable_http_endpoint', 'enable_iam_database_authentication', 'engine', 'engine_mode', 'engine_version', 'global_cluster_identifier', 'iops', 'kms_key_id', 'manage_master_user_password', 'master_username', 'master_user_password', 'master_user_secret', 'monitoring_interval', 'monitoring_role_arn', 'network_type', 'performance_insights_enabled', 'performance_insights_kms_key_id', 'performance_insights_retention_period', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'publicly_accessible', 'replication_source_identifier', 'restore_to_time', 'restore_type', 'scaling_configuration', 'serverless_v2_scaling_configuration', 'snapshot_identifier', 'source_db_cluster_identifier', 'source_region', 'storage_encrypted', 'storage_type', 'tags', 'use_latest_restorable_time', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBInstanceProps
class CfnDBInstancePropsDef(BaseCfnProperty):
    allocated_storage: typing.Optional[str] = pydantic.Field(None, description='The amount of storage in gibibytes (GiB) to be initially allocated for the database instance. .. epigraph:: If any value is set in the ``Iops`` parameter, ``AllocatedStorage`` must be at least 100 GiB, which corresponds to the minimum Iops value of 1,000. If you increase the ``Iops`` value (in 1,000 IOPS increments), then you must also increase the ``AllocatedStorage`` value (in 100-GiB increments). *Amazon Aurora* Not applicable. Aurora cluster volumes automatically grow as the amount of data in your database increases, though you are only charged for the space that you use in an Aurora cluster volume. *MySQL* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 5 to 3072. *MariaDB* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 5 to 3072. *PostgreSQL* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 5 to 3072. *Oracle* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): Must be an integer from 20 to 65536. - Provisioned IOPS storage (io1): Must be an integer from 100 to 65536. - Magnetic storage (standard): Must be an integer from 10 to 3072. *SQL Server* Constraints to the amount of storage for each storage type are the following: - General Purpose (SSD) storage (gp2): - Enterprise and Standard editions: Must be an integer from 20 to 16384. - Web and Express editions: Must be an integer from 20 to 16384. - Provisioned IOPS storage (io1): - Enterprise and Standard editions: Must be an integer from 20 to 16384. - Web and Express editions: Must be an integer from 20 to 16384. - Magnetic storage (standard): - Enterprise and Standard editions: Must be an integer from 20 to 1024. - Web and Express editions: Must be an integer from 20 to 1024.\n')
    allow_major_version_upgrade: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether major version upgrades are allowed. Changing this parameter doesn't result in an outage and the change is asynchronously applied as soon as possible. Constraints: Major version upgrades must be allowed when specifying a value for the ``EngineVersion`` parameter that is a different major version than the DB instance's current version.\n")
    associated_roles: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_DBInstanceRolePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The AWS Identity and Access Management (IAM) roles associated with the DB instance. *Amazon Aurora* Not applicable. The associated roles are managed by the DB cluster.\n')
    auto_minor_version_upgrade: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether minor engine upgrades are applied automatically to the DB instance during the maintenance window. By default, minor engine upgrades are applied automatically.\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description="The Availability Zone (AZ) where the database will be created. For information on AWS Regions and Availability Zones, see `Regions and Availability Zones <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RegionsAndAvailabilityZones.html>`_ . *Amazon Aurora* Each Aurora DB cluster hosts copies of its storage in three separate Availability Zones. Specify one of these Availability Zones. Aurora automatically chooses an appropriate Availability Zone if you don't specify one. Default: A random, system-chosen Availability Zone in the endpoint's AWS Region . Example: ``us-east-1d`` Constraint: The ``AvailabilityZone`` parameter can't be specified if the DB instance is a Multi-AZ deployment. The specified Availability Zone must be in the same AWS Region as the current endpoint.\n")
    backup_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description="The number of days for which automated backups are retained. Setting this parameter to a positive number enables backups. Setting this parameter to 0 disables automated backups. *Amazon Aurora* Not applicable. The retention period for automated backups is managed by the DB cluster. Default: 1 Constraints: - Must be a value from 0 to 35 - Can't be set to 0 if the DB instance is a source to read replicas\n")
    ca_certificate_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the CA certificate for this DB instance. .. epigraph:: Specifying or updating this property triggers a reboot. For more information about CA certificate identifiers for RDS DB engines, see `Rotating Your SSL/TLS Certificate <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon RDS User Guide* . For more information about CA certificate identifiers for Aurora DB engines, see `Rotating Your SSL/TLS Certificate <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon Aurora User Guide* .\n')
    certificate_details: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_CertificateDetailsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description="The details of the DB instance's server certificate.\n")
    certificate_rotation_restart: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance is restarted when you rotate your SSL/TLS certificate. By default, the DB instance is restarted when you rotate your SSL/TLS certificate. The certificate is not updated until the DB instance is restarted. .. epigraph:: Set this parameter only if you are *not* using SSL/TLS to connect to the DB instance. If you are using SSL/TLS to connect to the DB instance, follow the appropriate instructions for your DB engine to rotate your SSL/TLS certificate: - For more information about rotating your SSL/TLS certificate for RDS DB engines, see `Rotating Your SSL/TLS Certificate. <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon RDS User Guide.* - For more information about rotating your SSL/TLS certificate for Aurora DB engines, see `Rotating Your SSL/TLS Certificate <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.SSL-certificate-rotation.html>`_ in the *Amazon Aurora User Guide* . This setting doesn't apply to RDS Custom.\n")
    character_set_name: typing.Optional[str] = pydantic.Field(None, description='For supported engines, indicates that the DB instance should be associated with the specified character set. *Amazon Aurora* Not applicable. The character set is managed by the DB cluster. For more information, see `AWS::RDS::DBCluster <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbcluster.html>`_ .\n')
    copy_tags_to_snapshot: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to copy tags from the DB instance to snapshots of the DB instance. By default, tags are not copied. *Amazon Aurora* Not applicable. Copying tags to snapshots is managed by the DB cluster. Setting this value for an Aurora DB instance has no effect on the DB cluster setting.\n')
    custom_iam_instance_profile: typing.Optional[str] = pydantic.Field(None, description='The instance profile associated with the underlying Amazon EC2 instance of an RDS Custom DB instance. The instance profile must meet the following requirements: - The profile must exist in your account. - The profile must have an IAM role that Amazon EC2 has permissions to assume. - The instance profile name and the associated IAM role name must start with the prefix ``AWSRDSCustom`` . For the list of permissions required for the IAM role, see `Configure IAM and your VPC <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/custom-setup-orcl.html#custom-setup-orcl.iam-vpc>`_ in the *Amazon RDS User Guide* . This setting is required for RDS Custom.\n')
    db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the DB cluster that the instance will belong to.\n')
    db_cluster_snapshot_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier for the RDS for MySQL Multi-AZ DB cluster snapshot to restore from. For more information on Multi-AZ DB clusters, see `Multi-AZ DB cluster deployments <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/multi-az-db-clusters-concepts.html>`_ in the *Amazon RDS User Guide* . Constraints: - Must match the identifier of an existing Multi-AZ DB cluster snapshot. - Can't be specified when ``DBSnapshotIdentifier`` is specified. - Must be specified when ``DBSnapshotIdentifier`` isn't specified. - If you are restoring from a shared manual Multi-AZ DB cluster snapshot, the ``DBClusterSnapshotIdentifier`` must be the ARN of the shared snapshot. - Can't be the identifier of an Aurora DB cluster snapshot. - Can't be the identifier of an RDS for PostgreSQL Multi-AZ DB cluster snapshot.\n")
    db_instance_class: typing.Optional[str] = pydantic.Field(None, description='The compute and memory capacity of the DB instance, for example, ``db.m4.large`` . Not all DB instance classes are available in all AWS Regions, or for all database engines. For the full list of DB instance classes, and availability for your engine, see `DB Instance Class <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html>`_ in the *Amazon RDS User Guide.* For more information about DB instance class pricing and AWS Region support for DB instance classes, see `Amazon RDS Pricing <https://docs.aws.amazon.com/rds/pricing/>`_ .\n')
    db_instance_identifier: typing.Optional[str] = pydantic.Field(None, description="A name for the DB instance. If you specify a name, AWS CloudFormation converts it to lowercase. If you don't specify a name, AWS CloudFormation generates a unique physical ID and uses that ID for the DB instance. For more information, see `Name Type <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-name.html>`_ . For information about constraints that apply to DB instance identifiers, see `Naming constraints in Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.Constraints>`_ in the *Amazon RDS User Guide* . .. epigraph:: If you specify a name, you can't perform updates that require replacement of this resource. You can perform updates that require no or some interruption. If you must replace the resource, specify a new name.\n")
    db_name: typing.Optional[str] = pydantic.Field(None, description="The meaning of this parameter differs according to the database engine you use. .. epigraph:: If you specify the ``[DBSnapshotIdentifier](https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-dbsnapshotidentifier)`` property, this property only applies to RDS for Oracle. *Amazon Aurora* Not applicable. The database name is managed by the DB cluster. *MySQL* The name of the database to create when the DB instance is created. If this parameter is not specified, no database is created in the DB instance. Constraints: - Must contain 1 to 64 letters or numbers. - Can't be a word reserved by the specified database engine *MariaDB* The name of the database to create when the DB instance is created. If this parameter is not specified, no database is created in the DB instance. Constraints: - Must contain 1 to 64 letters or numbers. - Can't be a word reserved by the specified database engine *PostgreSQL* The name of the database to create when the DB instance is created. If this parameter is not specified, the default ``postgres`` database is created in the DB instance. Constraints: - Must begin with a letter. Subsequent characters can be letters, underscores, or digits (0-9). - Must contain 1 to 63 characters. - Can't be a word reserved by the specified database engine *Oracle* The Oracle System ID (SID) of the created DB instance. If you specify ``null`` , the default value ``ORCL`` is used. You can't specify the string NULL, or any other reserved word, for ``DBName`` . Default: ``ORCL`` Constraints: - Can't be longer than 8 characters *SQL Server* Not applicable. Must be null.\n")
    db_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description='The name of an existing DB parameter group or a reference to an `AWS::RDS::DBParameterGroup <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-dbparametergroup.html>`_ resource created in the template. To list all of the available DB parameter group names, use the following command: ``aws rds describe-db-parameter-groups --query "DBParameterGroups[].DBParameterGroupName" --output text`` .. epigraph:: If any of the data members of the referenced parameter group are changed during an update, the DB instance might need to be restarted, which causes some interruption. If the parameter group contains static parameters, whether they were changed or not, an update triggers a reboot. If you don\'t specify a value for ``DBParameterGroupName`` property, the default DB parameter group for the specified engine and engine version is used.\n')
    db_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of the DB security groups to assign to the DB instance. The list can include both the name of existing DB security groups or references to AWS::RDS::DBSecurityGroup resources created in the template. If you set DBSecurityGroups, you must not set VPCSecurityGroups, and vice versa. Also, note that the DBSecurityGroups property exists only for backwards compatibility with older regions and is no longer recommended for providing security information to an RDS DB instance. Instead, use VPCSecurityGroups. .. epigraph:: If you specify this property, AWS CloudFormation sends only the following properties (if specified) to Amazon RDS during create operations: - ``AllocatedStorage`` - ``AutoMinorVersionUpgrade`` - ``AvailabilityZone`` - ``BackupRetentionPeriod`` - ``CharacterSetName`` - ``DBInstanceClass`` - ``DBName`` - ``DBParameterGroupName`` - ``DBSecurityGroups`` - ``DBSubnetGroupName`` - ``Engine`` - ``EngineVersion`` - ``Iops`` - ``LicenseModel`` - ``MasterUsername`` - ``MasterUserPassword`` - ``MultiAZ`` - ``OptionGroupName`` - ``PreferredBackupWindow`` - ``PreferredMaintenanceWindow`` All other properties are ignored. Specify a virtual private cloud (VPC) security group if you want to submit other properties, such as ``StorageType`` , ``StorageEncrypted`` , or ``KmsKeyId`` . If you're already using the ``DBSecurityGroups`` property, you can't use these other properties by updating your DB instance to use a VPC security group. You must recreate the DB instance.\n")
    db_snapshot_identifier: typing.Optional[str] = pydantic.Field(None, description="The name or Amazon Resource Name (ARN) of the DB snapshot that's used to restore the DB instance. If you're restoring from a shared manual DB snapshot, you must specify the ARN of the snapshot. By specifying this property, you can create a DB instance from the specified DB snapshot. If the ``DBSnapshotIdentifier`` property is an empty string or the ``AWS::RDS::DBInstance`` declaration has no ``DBSnapshotIdentifier`` property, AWS CloudFormation creates a new database. If the property contains a value (other than an empty string), AWS CloudFormation creates a database from the specified snapshot. If a snapshot with the specified name doesn't exist, AWS CloudFormation can't create the database and it rolls back the stack. Some DB instance properties aren't valid when you restore from a snapshot, such as the ``MasterUsername`` and ``MasterUserPassword`` properties. For information about the properties that you can specify, see the ``RestoreDBInstanceFromDBSnapshot`` action in the *Amazon RDS API Reference* . After you restore a DB instance with a ``DBSnapshotIdentifier`` property, you must specify the same ``DBSnapshotIdentifier`` property for any future updates to the DB instance. When you specify this property for an update, the DB instance is not restored from the DB snapshot again, and the data in the database is not changed. However, if you don't specify the ``DBSnapshotIdentifier`` property, an empty DB instance is created, and the original DB instance is deleted. If you specify a property that is different from the previous snapshot restore property, a new DB instance is restored from the specified ``DBSnapshotIdentifier`` property, and the original DB instance is deleted. If you specify the ``DBSnapshotIdentifier`` property to restore a DB instance (as opposed to specifying it for DB instance updates), then don't specify the following properties: - ``CharacterSetName`` - ``DBClusterIdentifier`` - ``DBName`` - ``DeleteAutomatedBackups`` - ``EnablePerformanceInsights`` - ``KmsKeyId`` - ``MasterUsername`` - ``MasterUserPassword`` - ``PerformanceInsightsKMSKeyId`` - ``PerformanceInsightsRetentionPeriod`` - ``PromotionTier`` - ``SourceDBInstanceIdentifier`` - ``SourceRegion`` - ``StorageEncrypted`` (for an encrypted snapshot) - ``Timezone`` *Amazon Aurora* Not applicable. Snapshot restore is managed by the DB cluster.\n")
    db_subnet_group_name: typing.Optional[str] = pydantic.Field(None, description="A DB subnet group to associate with the DB instance. If you update this value, the new subnet group must be a subnet group in a new VPC. If there's no DB subnet group, then the DB instance isn't a VPC DB instance. For more information about using Amazon RDS in a VPC, see `Using Amazon RDS with Amazon Virtual Private Cloud (VPC) <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. The DB subnet group is managed by the DB cluster. If specified, the setting must match the DB cluster setting.\n")
    delete_automated_backups: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to remove automated backups immediately after the DB instance is deleted. This parameter isn't case-sensitive. The default is to remove automated backups immediately after the DB instance is deleted. *Amazon Aurora* Not applicable. When you delete a DB cluster, all automated backups for that DB cluster are deleted and can't be recovered. Manual DB cluster snapshots of the DB cluster are not deleted.\n")
    deletion_protection: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance has deletion protection enabled. The database can't be deleted when deletion protection is enabled. By default, deletion protection is disabled. For more information, see `Deleting a DB Instance <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_DeleteInstance.html>`_ . *Amazon Aurora* Not applicable. You can enable or disable deletion protection for the DB cluster. For more information, see ``CreateDBCluster`` . DB instances in a DB cluster can be deleted even when deletion protection is enabled for the DB cluster.\n")
    domain: typing.Optional[str] = pydantic.Field(None, description='The Active Directory directory ID to create the DB instance in. Currently, only Microsoft SQL Server, Oracle, and PostgreSQL DB instances can be created in an Active Directory Domain. For more information, see `Kerberos Authentication <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/kerberos-authentication.html>`_ in the *Amazon RDS User Guide* .\n')
    domain_iam_role_name: typing.Optional[str] = pydantic.Field(None, description="Specify the name of the IAM role to be used when making API calls to the Directory Service. This setting doesn't apply to RDS Custom. *Amazon Aurora* Not applicable. The domain is managed by the DB cluster.\n")
    enable_cloudwatch_logs_exports: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The list of log types that need to be enabled for exporting to CloudWatch Logs. The values in the list depend on the DB engine being used. For more information, see `Publishing Database Logs to Amazon CloudWatch Logs <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_LogAccess.html#USER_LogAccess.Procedural.UploadtoCloudWatch>`_ in the *Amazon Relational Database Service User Guide* . *Amazon Aurora* Not applicable. CloudWatch Logs exports are managed by the DB cluster. *MariaDB* Valid values: ``audit`` , ``error`` , ``general`` , ``slowquery`` *Microsoft SQL Server* Valid values: ``agent`` , ``error`` *MySQL* Valid values: ``audit`` , ``error`` , ``general`` , ``slowquery`` *Oracle* Valid values: ``alert`` , ``audit`` , ``listener`` , ``trace`` *PostgreSQL* Valid values: ``postgresql`` , ``upgrade``\n')
    enable_iam_database_authentication: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A value that indicates whether to enable mapping of AWS Identity and Access Management (IAM) accounts to database accounts. By default, mapping is disabled. This property is supported for RDS for MariaDB, RDS for MySQL, and RDS for PostgreSQL. For more information, see `IAM Database Authentication for MariaDB, MySQL, and PostgreSQL <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html>`_ in the *Amazon RDS User Guide.* *Amazon Aurora* Not applicable. Mapping AWS IAM accounts to database accounts is managed by the DB cluster.\n')
    enable_performance_insights: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to enable Performance Insights for the DB instance. For more information, see `Using Amazon Performance Insights <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PerfInsights.html>`_ in the *Amazon RDS User Guide* . This setting doesn't apply to RDS Custom.\n")
    endpoint: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_EndpointPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specifies the connection endpoint. .. epigraph:: The endpoint might not be shown for instances whose status is ``creating`` .\n')
    engine: typing.Optional[str] = pydantic.Field(None, description='The name of the database engine that you want to use for this DB instance. .. epigraph:: When you are creating a DB instance, the ``Engine`` property is required. Valid Values: - ``aurora-mysql`` (for Aurora MySQL DB instances) - ``aurora-postgresql`` (for Aurora PostgreSQL DB instances) - ``custom-oracle-ee`` (for RDS Custom for Oracle DB instances) - ``custom-oracle-ee-cdb`` (for RDS Custom for Oracle DB instances) - ``custom-sqlserver-ee`` (for RDS Custom for SQL Server DB instances) - ``custom-sqlserver-se`` (for RDS Custom for SQL Server DB instances) - ``custom-sqlserver-web`` (for RDS Custom for SQL Server DB instances) - ``mariadb`` - ``mysql`` - ``oracle-ee`` - ``oracle-ee-cdb`` - ``oracle-se2`` - ``oracle-se2-cdb`` - ``postgres`` - ``sqlserver-ee`` - ``sqlserver-se`` - ``sqlserver-ex`` - ``sqlserver-web``\n')
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The version number of the database engine to use. For a list of valid engine versions, use the ``DescribeDBEngineVersions`` action. The following are the database engines and links to information about the major and minor versions that are available with Amazon RDS. Not every database engine is available for every AWS Region. *Amazon Aurora* Not applicable. The version number of the database engine to be used by the DB instance is managed by the DB cluster. *MariaDB* See `MariaDB on Amazon RDS Versions <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MariaDB.html#MariaDB.Concepts.VersionMgmt>`_ in the *Amazon RDS User Guide.* *Microsoft SQL Server* See `Microsoft SQL Server Versions on Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.General.VersionSupport>`_ in the *Amazon RDS User Guide.* *MySQL* See `MySQL on Amazon RDS Versions <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_MySQL.html#MySQL.Concepts.VersionMgmt>`_ in the *Amazon RDS User Guide.* *Oracle* See `Oracle Database Engine Release Notes <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Appendix.Oracle.PatchComposition.html>`_ in the *Amazon RDS User Guide.* *PostgreSQL* See `Supported PostgreSQL Database Versions <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_PostgreSQL.html#PostgreSQL.Concepts.General.DBVersions>`_ in the *Amazon RDS User Guide.*\n')
    iops: typing.Union[int, float, None] = pydantic.Field(None, description='The number of I/O operations per second (IOPS) that the database provisions. The value must be equal to or greater than 1000. If you specify this property, you must follow the range of allowed ratios of your requested IOPS rate to the amount of storage that you allocate (IOPS to allocated storage). For example, you can provision an Oracle database instance with 1000 IOPS and 200 GiB of storage (a ratio of 5:1), or specify 2000 IOPS with 200 GiB of storage (a ratio of 10:1). For more information, see `Amazon RDS Provisioned IOPS Storage to Improve Performance <https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/CHAP_Storage.html#USER_PIOPS>`_ in the *Amazon RDS User Guide* . .. epigraph:: If you specify ``io1`` for the ``StorageType`` property, then you must also specify the ``Iops`` property.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The ARN of the AWS KMS key that's used to encrypt the DB instance, such as ``arn:aws:kms:us-east-1:012345678910:key/abcd1234-a123-456a-a12b-a123b4cd56ef`` . If you enable the StorageEncrypted property but don't specify this property, AWS CloudFormation uses the default KMS key. If you specify this property, you must set the StorageEncrypted property to true. If you specify the ``SourceDBInstanceIdentifier`` property, the value is inherited from the source DB instance if the read replica is created in the same region. If you create an encrypted read replica in a different AWS Region, then you must specify a KMS key for the destination AWS Region. KMS encryption keys are specific to the region that they're created in, and you can't use encryption keys from one region in another region. If you specify the ``SnapshotIdentifier`` property, the ``StorageEncrypted`` property value is inherited from the snapshot, and if the DB instance is encrypted, the specified ``KmsKeyId`` property is used. If you specify ``DBSecurityGroups`` , AWS CloudFormation ignores this property. To specify both a security group and this property, you must use a VPC security group. For more information about Amazon RDS and VPC, see `Using Amazon RDS with Amazon VPC <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. The KMS key identifier is managed by the DB cluster.\n")
    license_model: typing.Optional[str] = pydantic.Field(None, description="License model information for this DB instance. Valid values: - Aurora MySQL - ``general-public-license`` - Aurora PostgreSQL - ``postgresql-license`` - MariaDB - ``general-public-license`` - Microsoft SQL Server - ``license-included`` - MySQL - ``general-public-license`` - Oracle - ``bring-your-own-license`` or ``license-included`` - PostgreSQL - ``postgresql-license`` .. epigraph:: If you've specified ``DBSecurityGroups`` and then you update the license model, AWS CloudFormation replaces the underlying DB instance. This will incur some interruptions to database availability.\n")
    manage_master_user_password: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to manage the master user password with AWS Secrets Manager. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide.* Constraints: - Can't manage the master user password with AWS Secrets Manager if ``MasterUserPassword`` is specified.\n")
    master_username: typing.Optional[str] = pydantic.Field(None, description="The master user name for the DB instance. .. epigraph:: If you specify the ``SourceDBInstanceIdentifier`` or ``DBSnapshotIdentifier`` property, don't specify this property. The value is inherited from the source DB instance or snapshot. *Amazon Aurora* Not applicable. The name for the master user is managed by the DB cluster. *MariaDB* Constraints: - Required for MariaDB. - Must be 1 to 16 letters or numbers. - Can't be a reserved word for the chosen database engine. *Microsoft SQL Server* Constraints: - Required for SQL Server. - Must be 1 to 128 letters or numbers. - The first character must be a letter. - Can't be a reserved word for the chosen database engine. *MySQL* Constraints: - Required for MySQL. - Must be 1 to 16 letters or numbers. - First character must be a letter. - Can't be a reserved word for the chosen database engine. *Oracle* Constraints: - Required for Oracle. - Must be 1 to 30 letters or numbers. - First character must be a letter. - Can't be a reserved word for the chosen database engine. *PostgreSQL* Constraints: - Required for PostgreSQL. - Must be 1 to 63 letters or numbers. - First character must be a letter. - Can't be a reserved word for the chosen database engine.\n")
    master_user_password: typing.Optional[str] = pydantic.Field(None, description='The password for the master user. The password can include any printable ASCII character except "/", """, or "@". *Amazon Aurora* Not applicable. The password for the master user is managed by the DB cluster. *MariaDB* Constraints: Must contain from 8 to 41 characters. *Microsoft SQL Server* Constraints: Must contain from 8 to 128 characters. *MySQL* Constraints: Must contain from 8 to 41 characters. *Oracle* Constraints: Must contain from 8 to 30 characters. *PostgreSQL* Constraints: Must contain from 8 to 128 characters.\n')
    master_user_secret: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_MasterUserSecretPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Contains the secret managed by RDS in AWS Secrets Manager for the master user password. For more information, see `Password management with AWS Secrets Manager <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/rds-secrets-manager.html>`_ in the *Amazon RDS User Guide.*\n')
    max_allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description="The upper limit in gibibytes (GiB) to which Amazon RDS can automatically scale the storage of the DB instance. For more information about this setting, including limitations that apply to it, see `Managing capacity automatically with Amazon RDS storage autoscaling <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html#USER_PIOPS.Autoscaling>`_ in the *Amazon RDS User Guide* . This setting doesn't apply to RDS Custom. *Amazon Aurora* Not applicable. Storage is managed by the DB cluster.\n")
    monitoring_interval: typing.Union[int, float, None] = pydantic.Field(None, description="The interval, in seconds, between points when Enhanced Monitoring metrics are collected for the DB instance. To disable collection of Enhanced Monitoring metrics, specify 0. The default is 0. If ``MonitoringRoleArn`` is specified, then you must set ``MonitoringInterval`` to a value other than 0. This setting doesn't apply to RDS Custom. Valid Values: ``0, 1, 5, 10, 15, 30, 60``\n")
    monitoring_role_arn: typing.Optional[str] = pydantic.Field(None, description="The ARN for the IAM role that permits RDS to send enhanced monitoring metrics to Amazon CloudWatch Logs. For example, ``arn:aws:iam:123456789012:role/emaccess`` . For information on creating a monitoring role, see `Setting Up and Enabling Enhanced Monitoring <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#USER_Monitoring.OS.Enabling>`_ in the *Amazon RDS User Guide* . If ``MonitoringInterval`` is set to a value other than 0, then you must supply a ``MonitoringRoleArn`` value. This setting doesn't apply to RDS Custom.\n")
    multi_az: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="Specifies whether the database instance is a Multi-AZ DB instance deployment. You can't set the ``AvailabilityZone`` parameter if the ``MultiAZ`` parameter is set to true. For more information, see `Multi-AZ deployments for high availability <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.MultiAZ.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. Amazon Aurora storage is replicated across all of the Availability Zones and doesn't require the ``MultiAZ`` option to be set.\n")
    nchar_character_set_name: typing.Optional[str] = pydantic.Field(None, description="The name of the NCHAR character set for the Oracle DB instance. This parameter doesn't apply to RDS Custom.\n")
    network_type: typing.Optional[str] = pydantic.Field(None, description='The network type of the DB instance. Valid values: - ``IPV4`` - ``DUAL`` The network type is determined by the ``DBSubnetGroup`` specified for the DB instance. A ``DBSubnetGroup`` can support only the IPv4 protocol or the IPv4 and IPv6 protocols ( ``DUAL`` ). For more information, see `Working with a DB instance in a VPC <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.WorkingWithRDSInstanceinaVPC.html>`_ in the *Amazon RDS User Guide.*\n')
    option_group_name: typing.Optional[str] = pydantic.Field(None, description="Indicates that the DB instance should be associated with the specified option group. Permanent options, such as the TDE option for Oracle Advanced Security TDE, can't be removed from an option group. Also, that option group can't be removed from a DB instance once it is associated with a DB instance.\n")
    performance_insights_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The AWS KMS key identifier for encryption of Performance Insights data. The KMS key identifier is the key ARN, key ID, alias ARN, or alias name for the KMS key. If you do not specify a value for ``PerformanceInsightsKMSKeyId`` , then Amazon RDS uses your default KMS key. There is a default KMS key for your AWS account. Your AWS account has a different default KMS key for each AWS Region. For information about enabling Performance Insights, see `EnablePerformanceInsights <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-enableperformanceinsights>`_ .\n')
    performance_insights_retention_period: typing.Union[int, float, None] = pydantic.Field(None, description="The number of days to retain Performance Insights data. The default is 7 days. The following values are valid:. - 7 - *month* * 31, where *month* is a number of months from 1-23 - 731 For example, the following values are valid: - 93 (3 months * 31) - 341 (11 months * 31) - 589 (19 months * 31) - 731 If you specify a retention period such as 94, which isn't a valid value, RDS issues an error. This setting doesn't apply to RDS Custom.\n")
    port: typing.Optional[str] = pydantic.Field(None, description='The port number on which the database accepts connections. *Amazon Aurora* Not applicable. The port number is managed by the DB cluster.\n')
    preferred_backup_window: typing.Optional[str] = pydantic.Field(None, description='The daily time range during which automated backups are created if automated backups are enabled, using the ``BackupRetentionPeriod`` parameter. For more information, see `Backup Window <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithAutomatedBackups.html#USER_WorkingWithAutomatedBackups.BackupWindow>`_ in the *Amazon RDS User Guide.* Constraints: - Must be in the format ``hh24:mi-hh24:mi`` . - Must be in Universal Coordinated Time (UTC). - Must not conflict with the preferred maintenance window. - Must be at least 30 minutes. *Amazon Aurora* Not applicable. The daily time range for creating automated backups is managed by the DB cluster.\n')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range during which system maintenance can occur, in Universal Coordinated Time (UTC). Format: ``ddd:hh24:mi-ddd:hh24:mi`` The default is a 30-minute window selected at random from an 8-hour block of time for each AWS Region, occurring on a random day of the week. To see the time blocks available, see `Adjusting the Preferred DB Instance Maintenance Window <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_UpgradeDBInstance.Maintenance.html#AdjustingTheMaintenanceWindow>`_ in the *Amazon RDS User Guide.* .. epigraph:: This property applies when AWS CloudFormation initially creates the DB instance. If you use AWS CloudFormation to update the DB instance, those updates are applied immediately. Constraints: Minimum 30-minute window.\n')
    processor_features: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBInstance_ProcessorFeaturePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="The number of CPU cores and the number of threads per core for the DB instance class of the DB instance. This setting doesn't apply to RDS Custom. *Amazon Aurora* Not applicable.\n")
    promotion_tier: typing.Union[int, float, None] = pydantic.Field(None, description="A value that specifies the order in which an Aurora Replica is promoted to the primary instance after a failure of the existing primary instance. For more information, see `Fault Tolerance for an Aurora DB Cluster <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html#Aurora.Managing.FaultTolerance>`_ in the *Amazon Aurora User Guide* . This setting doesn't apply to RDS Custom. Default: 1 Valid Values: 0 - 15\n")
    publicly_accessible: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Indicates whether the DB instance is an internet-facing instance. If you specify true, AWS CloudFormation creates an instance with a publicly resolvable DNS name, which resolves to a public IP address. If you specify false, AWS CloudFormation creates an internal instance with a DNS name that resolves to a private IP address. The default behavior value depends on your VPC setup and the database subnet group. For more information, see the ``PubliclyAccessible`` parameter in the `CreateDBInstance <https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_CreateDBInstance.html>`_ in the *Amazon RDS API Reference* .\n')
    replica_mode: typing.Optional[str] = pydantic.Field(None, description='The open mode of an Oracle read replica. For more information, see `Working with Oracle Read Replicas for Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/oracle-read-replicas.html>`_ in the *Amazon RDS User Guide* . This setting is only supported in RDS for Oracle. Default: ``open-read-only`` Valid Values: ``open-read-only`` or ``mounted``\n')
    restore_time: typing.Optional[str] = pydantic.Field(None, description="The date and time to restore from. Valid Values: Value must be a time in Universal Coordinated Time (UTC) format Constraints: - Must be before the latest restorable time for the DB instance - Can't be specified if the ``UseLatestRestorableTime`` parameter is enabled Example: ``2009-09-07T23:45:00Z``\n")
    source_db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the Multi-AZ DB cluster that will act as the source for the read replica. Each DB cluster can have up to 15 read replicas. Constraints: - Must be the identifier of an existing Multi-AZ DB cluster. - Can't be specified if the ``SourceDBInstanceIdentifier`` parameter is also specified. - The specified DB cluster must have automatic backups enabled, that is, its backup retention period must be greater than 0. - The source DB cluster must be in the same AWS Region as the read replica. Cross-Region replication isn't supported.\n")
    source_db_instance_automated_backups_arn: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the replicated automated backups from which to restore, for example, ``arn:aws:rds:useast-1:123456789012:auto-backup:ab-L2IJCEXJP7XQ7HOJ4SIEXAMPLE`` . This setting doesn't apply to RDS Custom.\n")
    source_db_instance_identifier: typing.Optional[str] = pydantic.Field(None, description="If you want to create a read replica DB instance, specify the ID of the source DB instance. Each DB instance can have a limited number of read replicas. For more information, see `Working with Read Replicas <https://docs.aws.amazon.com/AmazonRDS/latest/DeveloperGuide/USER_ReadRepl.html>`_ in the *Amazon RDS User Guide* . For information about constraints that apply to DB instance identifiers, see `Naming constraints in Amazon RDS <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Limits.html#RDS_Limits.Constraints>`_ in the *Amazon RDS User Guide* . The ``SourceDBInstanceIdentifier`` property determines whether a DB instance is a read replica. If you remove the ``SourceDBInstanceIdentifier`` property from your template and then update your stack, AWS CloudFormation promotes the Read Replica to a standalone DB instance. .. epigraph:: - If you specify a source DB instance that uses VPC security groups, we recommend that you specify the ``VPCSecurityGroups`` property. If you don't specify the property, the read replica inherits the value of the ``VPCSecurityGroups`` property from the source DB when you create the replica. However, if you update the stack, AWS CloudFormation reverts the replica's ``VPCSecurityGroups`` property to the default value because it's not defined in the stack's template. This change might cause unexpected issues. - Read replicas don't support deletion policies. AWS CloudFormation ignores any deletion policy that's associated with a read replica. - If you specify ``SourceDBInstanceIdentifier`` , don't specify the ``DBSnapshotIdentifier`` property. You can't create a read replica from a snapshot. - Don't set the ``BackupRetentionPeriod`` , ``DBName`` , ``MasterUsername`` , ``MasterUserPassword`` , and ``PreferredBackupWindow`` properties. The database attributes are inherited from the source DB instance, and backups are disabled for read replicas. - If the source DB instance is in a different region than the read replica, specify the source region in ``SourceRegion`` , and specify an ARN for a valid DB instance in ``SourceDBInstanceIdentifier`` . For more information, see `Constructing a Amazon RDS Amazon Resource Name (ARN) <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Tagging.html#USER_Tagging.ARN>`_ in the *Amazon RDS User Guide* . - For DB instances in Amazon Aurora clusters, don't specify this property. Amazon RDS automatically assigns writer and reader DB instances.\n")
    source_dbi_resource_id: typing.Optional[str] = pydantic.Field(None, description='The resource ID of the source DB instance from which to restore.\n')
    source_region: typing.Optional[str] = pydantic.Field(None, description='The ID of the region that contains the source DB instance for the read replica.\n')
    storage_encrypted: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance is encrypted. By default, it isn't encrypted. If you specify the ``KmsKeyId`` property, then you must enable encryption. If you specify the ``SourceDBInstanceIdentifier`` property, don't specify this property. The value is inherited from the source DB instance, and if the DB instance is encrypted, the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot is encrypted, don't specify this property. The value is inherited from the snapshot, and the specified ``KmsKeyId`` property is used. If you specify the ``SnapshotIdentifier`` and the specified snapshot isn't encrypted, you can use this property to specify that the restored DB instance is encrypted. Specify the ``KmsKeyId`` property for the KMS key to use for encryption. If you don't want the restored DB instance to be encrypted, then don't set this property or set it to ``false`` . *Amazon Aurora* Not applicable. The encryption for DB instances is managed by the DB cluster.\n")
    storage_throughput: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies the storage throughput value for the DB instance. This setting applies only to the ``gp3`` storage type. This setting doesn't apply to RDS Custom or Amazon Aurora.\n")
    storage_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the storage type to be associated with the DB instance. Valid values: ``gp2 | gp3 | io1 | standard`` The ``standard`` value is also known as magnetic. If you specify ``io1`` or ``gp3`` , you must also include a value for the ``Iops`` parameter. Default: ``io1`` if the ``Iops`` parameter is specified, otherwise ``gp2`` For more information, see `Amazon RDS DB Instance Storage <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_Storage.html>`_ in the *Amazon RDS User Guide* . *Amazon Aurora* Not applicable. Aurora data is stored in the cluster volume, which is a single, virtual volume that uses solid state drives (SSDs).\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB instance.\n')
    timezone: typing.Optional[str] = pydantic.Field(None, description='The time zone of the DB instance. The time zone parameter is currently supported only by `Microsoft SQL Server <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/CHAP_SQLServer.html#SQLServer.Concepts.General.TimeZone>`_ .\n')
    use_default_processor_features: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance class of the DB instance uses its default processor features. This setting doesn't apply to RDS Custom.\n")
    use_latest_restorable_time: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether the DB instance is restored from the latest backup time. By default, the DB instance isn't restored from the latest backup time. Constraints: Can't be specified if the ``RestoreTime`` parameter is provided.\n")
    vpc_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of the VPC security group IDs to assign to the DB instance. The list can include both the physical IDs of existing VPC security groups and references to `AWS::EC2::SecurityGroup <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-ec2-security-group.html>`_ resources created in the template. If you plan to update the resource, don\'t specify VPC security groups in a shared VPC. If you set ``VPCSecurityGroups`` , you must not set ```DBSecurityGroups`` <https://docs.aws.amazon.com//AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-dbsecuritygroups>`_ , and vice versa. .. epigraph:: You can migrate a DB instance in your stack from an RDS DB security group to a VPC security group, but keep the following in mind: - You can\'t revert to using an RDS security group after you establish a VPC security group membership. - When you migrate your DB instance to VPC security groups, if your stack update rolls back because the DB instance update fails or because an update fails in another AWS CloudFormation resource, the rollback fails because it can\'t revert to an RDS security group. - To use the properties that are available when you use a VPC security group, you must recreate the DB instance. If you don\'t, AWS CloudFormation submits only the property values that are listed in the ```DBSecurityGroups`` <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html#cfn-rds-dbinstance-dbsecuritygroups>`_ property. To avoid this situation, migrate your DB instance to using VPC security groups only when that is the only change in your stack template. *Amazon Aurora* Not applicable. The associated list of EC2 VPC security groups is managed by the DB cluster. If specified, the setting must match the DB cluster setting.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbinstance.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBInstance_props = rds.CfnDBInstanceProps(\n        allocated_storage="allocatedStorage",\n        allow_major_version_upgrade=False,\n        associated_roles=[rds.CfnDBInstance.DBInstanceRoleProperty(\n            feature_name="featureName",\n            role_arn="roleArn"\n        )],\n        auto_minor_version_upgrade=False,\n        availability_zone="availabilityZone",\n        backup_retention_period=123,\n        ca_certificate_identifier="caCertificateIdentifier",\n        certificate_details=rds.CfnDBInstance.CertificateDetailsProperty(\n            ca_identifier="caIdentifier",\n            valid_till="validTill"\n        ),\n        certificate_rotation_restart=False,\n        character_set_name="characterSetName",\n        copy_tags_to_snapshot=False,\n        custom_iam_instance_profile="customIamInstanceProfile",\n        db_cluster_identifier="dbClusterIdentifier",\n        db_cluster_snapshot_identifier="dbClusterSnapshotIdentifier",\n        db_instance_class="dbInstanceClass",\n        db_instance_identifier="dbInstanceIdentifier",\n        db_name="dbName",\n        db_parameter_group_name="dbParameterGroupName",\n        db_security_groups=["dbSecurityGroups"],\n        db_snapshot_identifier="dbSnapshotIdentifier",\n        db_subnet_group_name="dbSubnetGroupName",\n        delete_automated_backups=False,\n        deletion_protection=False,\n        domain="domain",\n        domain_iam_role_name="domainIamRoleName",\n        enable_cloudwatch_logs_exports=["enableCloudwatchLogsExports"],\n        enable_iam_database_authentication=False,\n        enable_performance_insights=False,\n        endpoint=rds.CfnDBInstance.EndpointProperty(\n            address="address",\n            hosted_zone_id="hostedZoneId",\n            port="port"\n        ),\n        engine="engine",\n        engine_version="engineVersion",\n        iops=123,\n        kms_key_id="kmsKeyId",\n        license_model="licenseModel",\n        manage_master_user_password=False,\n        master_username="masterUsername",\n        master_user_password="masterUserPassword",\n        master_user_secret=rds.CfnDBInstance.MasterUserSecretProperty(\n            kms_key_id="kmsKeyId",\n            secret_arn="secretArn"\n        ),\n        max_allocated_storage=123,\n        monitoring_interval=123,\n        monitoring_role_arn="monitoringRoleArn",\n        multi_az=False,\n        nchar_character_set_name="ncharCharacterSetName",\n        network_type="networkType",\n        option_group_name="optionGroupName",\n        performance_insights_kms_key_id="performanceInsightsKmsKeyId",\n        performance_insights_retention_period=123,\n        port="port",\n        preferred_backup_window="preferredBackupWindow",\n        preferred_maintenance_window="preferredMaintenanceWindow",\n        processor_features=[rds.CfnDBInstance.ProcessorFeatureProperty(\n            name="name",\n            value="value"\n        )],\n        promotion_tier=123,\n        publicly_accessible=False,\n        replica_mode="replicaMode",\n        restore_time="restoreTime",\n        source_db_cluster_identifier="sourceDbClusterIdentifier",\n        source_db_instance_automated_backups_arn="sourceDbInstanceAutomatedBackupsArn",\n        source_db_instance_identifier="sourceDbInstanceIdentifier",\n        source_dbi_resource_id="sourceDbiResourceId",\n        source_region="sourceRegion",\n        storage_encrypted=False,\n        storage_throughput=123,\n        storage_type="storageType",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        timezone="timezone",\n        use_default_processor_features=False,\n        use_latest_restorable_time=False,\n        vpc_security_groups=["vpcSecurityGroups"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['allocated_storage', 'allow_major_version_upgrade', 'associated_roles', 'auto_minor_version_upgrade', 'availability_zone', 'backup_retention_period', 'ca_certificate_identifier', 'certificate_details', 'certificate_rotation_restart', 'character_set_name', 'copy_tags_to_snapshot', 'custom_iam_instance_profile', 'db_cluster_identifier', 'db_cluster_snapshot_identifier', 'db_instance_class', 'db_instance_identifier', 'db_name', 'db_parameter_group_name', 'db_security_groups', 'db_snapshot_identifier', 'db_subnet_group_name', 'delete_automated_backups', 'deletion_protection', 'domain', 'domain_iam_role_name', 'enable_cloudwatch_logs_exports', 'enable_iam_database_authentication', 'enable_performance_insights', 'endpoint', 'engine', 'engine_version', 'iops', 'kms_key_id', 'license_model', 'manage_master_user_password', 'master_username', 'master_user_password', 'master_user_secret', 'max_allocated_storage', 'monitoring_interval', 'monitoring_role_arn', 'multi_az', 'nchar_character_set_name', 'network_type', 'option_group_name', 'performance_insights_kms_key_id', 'performance_insights_retention_period', 'port', 'preferred_backup_window', 'preferred_maintenance_window', 'processor_features', 'promotion_tier', 'publicly_accessible', 'replica_mode', 'restore_time', 'source_db_cluster_identifier', 'source_db_instance_automated_backups_arn', 'source_db_instance_identifier', 'source_dbi_resource_id', 'source_region', 'storage_encrypted', 'storage_throughput', 'storage_type', 'tags', 'timezone', 'use_default_processor_features', 'use_latest_restorable_time', 'vpc_security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBInstanceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBParameterGroupProps
class CfnDBParameterGroupPropsDef(BaseCfnProperty):
    description: str = pydantic.Field(..., description='Provides the customer-specified description for this DB parameter group.\n')
    family: str = pydantic.Field(..., description='The DB parameter group family name. A DB parameter group can be associated with one and only one DB parameter group family, and can be applied only to a DB instance running a DB engine and engine version compatible with that DB parameter group family. .. epigraph:: The DB parameter group family can\'t be changed when updating a DB parameter group. To list all of the available parameter group families, use the following command: ``aws rds describe-db-engine-versions --query "DBEngineVersions[].DBParameterGroupFamily"`` The output contains duplicates. For more information, see ``[CreateDBParameterGroup](https://docs.aws.amazon.com//AmazonRDS/latest/APIReference/API_CreateDBParameterGroup.html)`` .\n')
    db_parameter_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the DB parameter group. Constraints: - Must be 1 to 255 letters, numbers, or hyphens. - First character must be a letter - Can't end with a hyphen or contain two consecutive hyphens If you don't specify a value for ``DBParameterGroupName`` property, a name is automatically created for the DB parameter group. .. epigraph:: This value is stored as a lowercase string.\n")
    parameters: typing.Any = pydantic.Field(None, description="An array of parameter names and values for the parameter update. At least one parameter name and value must be supplied. Subsequent arguments are optional. For more information about DB parameters and DB parameter groups for Amazon RDS DB engines, see `Working with DB Parameter Groups <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_WorkingWithParamGroups.html>`_ in the *Amazon RDS User Guide* . For more information about DB cluster and DB instance parameters and parameter groups for Amazon Aurora DB engines, see `Working with DB Parameter Groups and DB Cluster Parameter Groups <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_WorkingWithParamGroups.html>`_ in the *Amazon Aurora User Guide* . .. epigraph:: AWS CloudFormation doesn't support specifying an apply method for each individual parameter. The default apply method for each parameter is used.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB parameter group. .. epigraph:: Currently, this is the only property that supports drift detection.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbparametergroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    # parameters: Any\n\n    cfn_dBParameter_group_props = rds.CfnDBParameterGroupProps(\n        description="description",\n        family="family",\n\n        # the properties below are optional\n        db_parameter_group_name="dbParameterGroupName",\n        parameters=parameters,\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['description', 'family', 'db_parameter_group_name', 'parameters', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBParameterGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBProxyEndpointProps
class CfnDBProxyEndpointPropsDef(BaseCfnProperty):
    db_proxy_endpoint_name: str = pydantic.Field(..., description='The name of the DB proxy endpoint to create.\n')
    db_proxy_name: str = pydantic.Field(..., description='The name of the DB proxy associated with the DB proxy endpoint that you create.\n')
    vpc_subnet_ids: typing.Sequence[str] = pydantic.Field(..., description='The VPC subnet IDs for the DB proxy endpoint that you create. You can specify a different set of subnet IDs than for the original DB proxy.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.CfnDBProxyEndpoint_TagFormatPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional set of key-value pairs to associate arbitrary data of your choosing with the proxy.\n')
    target_role: typing.Optional[str] = pydantic.Field(None, description='A value that indicates whether the DB proxy endpoint can be used for read/write or read-only operations. Valid Values: ``READ_WRITE | READ_ONLY``\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The VPC security group IDs for the DB proxy endpoint that you create. You can specify a different set of security group IDs than for the original DB proxy. The default is the default security group for the VPC.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbproxyendpoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBProxy_endpoint_props = rds.CfnDBProxyEndpointProps(\n        db_proxy_endpoint_name="dbProxyEndpointName",\n        db_proxy_name="dbProxyName",\n        vpc_subnet_ids=["vpcSubnetIds"],\n\n        # the properties below are optional\n        tags=[rds.CfnDBProxyEndpoint.TagFormatProperty(\n            key="key",\n            value="value"\n        )],\n        target_role="targetRole",\n        vpc_security_group_ids=["vpcSecurityGroupIds"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['db_proxy_endpoint_name', 'db_proxy_name', 'vpc_subnet_ids', 'tags', 'target_role', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxyEndpointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBProxyProps
class CfnDBProxyPropsDef(BaseCfnProperty):
    auth: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBProxy_AuthFormatPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='The authorization mechanism that the proxy uses.\n')
    db_proxy_name: str = pydantic.Field(..., description="The identifier for the proxy. This name must be unique for all proxies owned by your AWS account in the specified AWS Region . An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens; it can't end with a hyphen or contain two consecutive hyphens.\n")
    engine_family: str = pydantic.Field(..., description='The kinds of databases that the proxy can connect to. This value determines which database network protocol the proxy recognizes when it interprets network traffic to and from the database. For Aurora MySQL, RDS for MariaDB, and RDS for MySQL databases, specify ``MYSQL`` . For Aurora PostgreSQL and RDS for PostgreSQL databases, specify ``POSTGRESQL`` . For RDS for Microsoft SQL Server, specify ``SQLSERVER`` . *Valid values* : ``MYSQL`` | ``POSTGRESQL`` | ``SQLSERVER``\n')
    role_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the IAM role that the proxy uses to access secrets in AWS Secrets Manager.\n')
    vpc_subnet_ids: typing.Sequence[str] = pydantic.Field(..., description='One or more VPC subnet IDs to associate with the new proxy.\n')
    debug_logging: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='Whether the proxy includes detailed information about SQL statements in its logs. This information helps you to debug issues involving SQL behavior or the performance and scalability of the proxy connections. The debug information includes the text of SQL statements that you submit through the proxy. Thus, only enable this setting when needed for debugging, and only when you have security measures in place to safeguard any sensitive information that appears in the logs.\n')
    idle_client_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The number of seconds that a connection to the proxy can be inactive before the proxy disconnects it. You can set this value higher or lower than the connection timeout limit for the associated database.\n')
    require_tls: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='A Boolean parameter that specifies whether Transport Layer Security (TLS) encryption is required for connections to the proxy. By enabling this setting, you can enforce encrypted TLS connections to the proxy.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.aws_rds.CfnDBProxy_TagFormatPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional set of key-value pairs to associate arbitrary data of your choosing with the proxy.\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more VPC security group IDs to associate with the new proxy. If you plan to update the resource, don\'t specify VPC security groups in a shared VPC.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbproxy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBProxy_props = rds.CfnDBProxyProps(\n        auth=[rds.CfnDBProxy.AuthFormatProperty(\n            auth_scheme="authScheme",\n            client_password_auth_type="clientPasswordAuthType",\n            description="description",\n            iam_auth="iamAuth",\n            secret_arn="secretArn"\n        )],\n        db_proxy_name="dbProxyName",\n        engine_family="engineFamily",\n        role_arn="roleArn",\n        vpc_subnet_ids=["vpcSubnetIds"],\n\n        # the properties below are optional\n        debug_logging=False,\n        idle_client_timeout=123,\n        require_tls=False,\n        tags=[rds.CfnDBProxy.TagFormatProperty(\n            key="key",\n            value="value"\n        )],\n        vpc_security_group_ids=["vpcSecurityGroupIds"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth', 'db_proxy_name', 'engine_family', 'role_arn', 'vpc_subnet_ids', 'debug_logging', 'idle_client_timeout', 'require_tls', 'tags', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBProxyTargetGroupProps
class CfnDBProxyTargetGroupPropsDef(BaseCfnProperty):
    db_proxy_name: str = pydantic.Field(..., description='The identifier of the ``DBProxy`` that is associated with the ``DBProxyTargetGroup`` .\n')
    target_group_name: str = pydantic.Field(..., description='The identifier for the target group. .. epigraph:: Currently, this property must be set to ``default`` .\n')
    connection_pool_configuration_info: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBProxyTargetGroup_ConnectionPoolConfigurationInfoFormatPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings that control the size and behavior of the connection pool associated with a ``DBProxyTargetGroup`` .\n')
    db_cluster_identifiers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more DB cluster identifiers.\n')
    db_instance_identifiers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='One or more DB instance identifiers.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbproxytargetgroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBProxy_target_group_props = rds.CfnDBProxyTargetGroupProps(\n        db_proxy_name="dbProxyName",\n        target_group_name="targetGroupName",\n\n        # the properties below are optional\n        connection_pool_configuration_info=rds.CfnDBProxyTargetGroup.ConnectionPoolConfigurationInfoFormatProperty(\n            connection_borrow_timeout=123,\n            init_query="initQuery",\n            max_connections_percent=123,\n            max_idle_connections_percent=123,\n            session_pinning_filters=["sessionPinningFilters"]\n        ),\n        db_cluster_identifiers=["dbClusterIdentifiers"],\n        db_instance_identifiers=["dbInstanceIdentifiers"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['db_proxy_name', 'target_group_name', 'connection_pool_configuration_info', 'db_cluster_identifiers', 'db_instance_identifiers']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBProxyTargetGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBSecurityGroupIngressProps
class CfnDBSecurityGroupIngressPropsDef(BaseCfnProperty):
    db_security_group_name: str = pydantic.Field(..., description='The name of the DB security group to add authorization to.\n')
    cidrip: typing.Optional[str] = pydantic.Field(None, description='The IP range to authorize.\n')
    ec2_security_group_id: typing.Optional[str] = pydantic.Field(None, description='Id of the EC2 security group to authorize. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n')
    ec2_security_group_name: typing.Optional[str] = pydantic.Field(None, description='Name of the EC2 security group to authorize. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n')
    ec2_security_group_owner_id: typing.Optional[str] = pydantic.Field(None, description='AWS account number of the owner of the EC2 security group specified in the ``EC2SecurityGroupName`` parameter. The AWS access key ID isn\'t an acceptable value. For VPC DB security groups, ``EC2SecurityGroupId`` must be provided. Otherwise, ``EC2SecurityGroupOwnerId`` and either ``EC2SecurityGroupName`` or ``EC2SecurityGroupId`` must be provided.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-security-group-ingress.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBSecurity_group_ingress_props = rds.CfnDBSecurityGroupIngressProps(\n        db_security_group_name="dbSecurityGroupName",\n\n        # the properties below are optional\n        cidrip="cidrip",\n        ec2_security_group_id="ec2SecurityGroupId",\n        ec2_security_group_name="ec2SecurityGroupName",\n        ec2_security_group_owner_id="ec2SecurityGroupOwnerId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['db_security_group_name', 'cidrip', 'ec2_security_group_id', 'ec2_security_group_name', 'ec2_security_group_owner_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBSecurityGroupIngressProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBSecurityGroupProps
class CfnDBSecurityGroupPropsDef(BaseCfnProperty):
    db_security_group_ingress: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnDBSecurityGroup_IngressPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='Ingress rules to be applied to the DB security group.\n')
    group_description: str = pydantic.Field(..., description='Provides the description of the DB security group.\n')
    ec2_vpc_id: typing.Optional[str] = pydantic.Field(None, description='The identifier of an Amazon VPC. This property indicates the VPC that this DB security group belongs to. .. epigraph:: The ``EC2VpcId`` property is for backward compatibility with older regions, and is no longer recommended for providing security information to an RDS DB instance.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB security group.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-security-group.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBSecurity_group_props = rds.CfnDBSecurityGroupProps(\n        db_security_group_ingress=[rds.CfnDBSecurityGroup.IngressProperty(\n            cidrip="cidrip",\n            ec2_security_group_id="ec2SecurityGroupId",\n            ec2_security_group_name="ec2SecurityGroupName",\n            ec2_security_group_owner_id="ec2SecurityGroupOwnerId"\n        )],\n        group_description="groupDescription",\n\n        # the properties below are optional\n        ec2_vpc_id="ec2VpcId",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['db_security_group_ingress', 'group_description', 'ec2_vpc_id', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBSecurityGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnDBSubnetGroupProps
class CfnDBSubnetGroupPropsDef(BaseCfnProperty):
    db_subnet_group_description: str = pydantic.Field(..., description='The description for the DB subnet group.\n')
    subnet_ids: typing.Sequence[str] = pydantic.Field(..., description='The EC2 Subnet IDs for the DB subnet group.\n')
    db_subnet_group_name: typing.Optional[str] = pydantic.Field(None, description='The name for the DB subnet group. This value is stored as a lowercase string. Constraints: Must contain no more than 255 lowercase alphanumeric characters or hyphens. Must not be "Default". Example: ``mysubnetgroup``\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this DB subnet group.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-dbsubnetgroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_dBSubnet_group_props = rds.CfnDBSubnetGroupProps(\n        db_subnet_group_description="dbSubnetGroupDescription",\n        subnet_ids=["subnetIds"],\n\n        # the properties below are optional\n        db_subnet_group_name="dbSubnetGroupName",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['db_subnet_group_description', 'subnet_ids', 'db_subnet_group_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnDBSubnetGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnEventSubscriptionProps
class CfnEventSubscriptionPropsDef(BaseCfnProperty):
    sns_topic_arn: str = pydantic.Field(..., description='The Amazon Resource Name (ARN) of the SNS topic created for event notification. The ARN is created by Amazon SNS when you create a topic and subscribe to it.\n')
    enabled: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="A value that indicates whether to activate the subscription. If the event notification subscription isn't activated, the subscription is created but not active.\n")
    event_categories: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of event categories for a particular source type ( ``SourceType`` ) that you want to subscribe to. You can see a list of the categories for a given source type in the "Amazon RDS event categories and event messages" section of the `*Amazon RDS User Guide* <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Events.Messages.html>`_ or the `*Amazon Aurora User Guide* <https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_Events.Messages.html>`_ . You can also see this list by using the ``DescribeEventCategories`` operation.\n')
    source_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="The list of identifiers of the event sources for which events are returned. If not specified, then all sources are included in the response. An identifier must begin with a letter and must contain only ASCII letters, digits, and hyphens. It can't end with a hyphen or contain two consecutive hyphens. Constraints: - If a ``SourceIds`` value is supplied, ``SourceType`` must also be provided. - If the source type is a DB instance, a ``DBInstanceIdentifier`` value must be supplied. - If the source type is a DB cluster, a ``DBClusterIdentifier`` value must be supplied. - If the source type is a DB parameter group, a ``DBParameterGroupName`` value must be supplied. - If the source type is a DB security group, a ``DBSecurityGroupName`` value must be supplied. - If the source type is a DB snapshot, a ``DBSnapshotIdentifier`` value must be supplied. - If the source type is a DB cluster snapshot, a ``DBClusterSnapshotIdentifier`` value must be supplied.\n")
    source_type: typing.Optional[str] = pydantic.Field(None, description="The type of source that is generating the events. For example, if you want to be notified of events generated by a DB instance, set this parameter to ``db-instance`` . If this value isn't specified, all events are returned. Valid values: ``db-instance`` | ``db-cluster`` | ``db-parameter-group`` | ``db-security-group`` | ``db-snapshot`` | ``db-cluster-snapshot``\n")
    subscription_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription. Constraints: The name must be less than 255 characters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this subscription.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-eventsubscription.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_event_subscription_props = rds.CfnEventSubscriptionProps(\n        sns_topic_arn="snsTopicArn",\n\n        # the properties below are optional\n        enabled=False,\n        event_categories=["eventCategories"],\n        source_ids=["sourceIds"],\n        source_type="sourceType",\n        subscription_name="subscriptionName",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['sns_topic_arn', 'enabled', 'event_categories', 'source_ids', 'source_type', 'subscription_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnEventSubscriptionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnGlobalClusterProps
class CfnGlobalClusterPropsDef(BaseCfnProperty):
    deletion_protection: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description="The deletion protection setting for the new global database. The global database can't be deleted when deletion protection is enabled.\n")
    engine: typing.Optional[str] = pydantic.Field(None, description="The name of the database engine to be used for this DB cluster. If this property isn't specified, the database engine is derived from the source DB cluster specified by the ``SourceDBClusterIdentifier`` property. .. epigraph:: If the ``SourceDBClusterIdentifier`` property isn't specified, this property is required. If the ``SourceDBClusterIdentifier`` property is specified, make sure this property isn't specified.\n")
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The engine version of the Aurora global database.\n')
    global_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description='The cluster identifier of the global database cluster.\n')
    source_db_cluster_identifier: typing.Optional[str] = pydantic.Field(None, description="The DB cluster identifier or Amazon Resource Name (ARN) to use as the primary cluster of the global database. .. epigraph:: If the ``Engine`` property isn't specified, this property is required. If the ``Engine`` property is specified, make sure this property isn't specified.\n")
    storage_encrypted: typing.Union[bool, typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], None] = pydantic.Field(None, description='The storage encryption setting for the global database cluster.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-globalcluster.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_global_cluster_props = rds.CfnGlobalClusterProps(\n        deletion_protection=False,\n        engine="engine",\n        engine_version="engineVersion",\n        global_cluster_identifier="globalClusterIdentifier",\n        source_db_cluster_identifier="sourceDbClusterIdentifier",\n        storage_encrypted=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['deletion_protection', 'engine', 'engine_version', 'global_cluster_identifier', 'source_db_cluster_identifier', 'storage_encrypted']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnGlobalClusterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_rds.CfnOptionGroupProps
class CfnOptionGroupPropsDef(BaseCfnProperty):
    engine_name: str = pydantic.Field(..., description='Specifies the name of the engine that this option group should be associated with. Valid Values: - ``mariadb`` - ``mysql`` - ``oracle-ee`` - ``oracle-ee-cdb`` - ``oracle-se2`` - ``oracle-se2-cdb`` - ``postgres`` - ``sqlserver-ee`` - ``sqlserver-se`` - ``sqlserver-ex`` - ``sqlserver-web``\n')
    major_engine_version: str = pydantic.Field(..., description='Specifies the major version of the engine that this option group should be associated with.\n')
    option_group_description: str = pydantic.Field(..., description='The description of the option group.\n')
    option_configurations: typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], typing.Sequence[typing.Union[typing.Union[models.CfnDynamicReferenceDef, models.IntrinsicDef, models.JsonNullDef, models.ReferenceDef, models.SecretValueDef, models.CfnConditionDef, models.CfnJsonDef, models.aws_events.EventFieldDef, models.aws_events.MatchDef, models.aws_iam.PolicyDocumentDef, models.custom_resources.PhysicalResourceIdReferenceDef], models.aws_rds.CfnOptionGroup_OptionConfigurationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A list of options and the settings for each option.\n')
    option_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the option group to be created. Constraints: - Must be 1 to 255 letters, numbers, or hyphens - First character must be a letter - Can't end with a hyphen or contain two consecutive hyphens Example: ``myoptiongroup`` If you don't specify a value for ``OptionGroupName`` property, a name is automatically created for the option group. .. epigraph:: This value is stored as a lowercase string.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An optional array of key-value pairs to apply to this option group.\n\n:link: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-rds-optiongroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_rds as rds\n\n    cfn_option_group_props = rds.CfnOptionGroupProps(\n        engine_name="engineName",\n        major_engine_version="majorEngineVersion",\n        option_group_description="optionGroupDescription",\n\n        # the properties below are optional\n        option_configurations=[rds.CfnOptionGroup.OptionConfigurationProperty(\n            option_name="optionName",\n\n            # the properties below are optional\n            db_security_group_memberships=["dbSecurityGroupMemberships"],\n            option_settings=[rds.CfnOptionGroup.OptionSettingProperty(\n                name="name",\n                value="value"\n            )],\n            option_version="optionVersion",\n            port=123,\n            vpc_security_group_memberships=["vpcSecurityGroupMemberships"]\n        )],\n        option_group_name="optionGroupName",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine_name', 'major_engine_version', 'option_group_description', 'option_configurations', 'option_group_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_rds.CfnOptionGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




import models

class ModuleModel(pydantic.BaseModel):
    AuroraEngineVersion: typing.Optional[dict[str, AuroraEngineVersionDef]] = pydantic.Field(None)
    AuroraMysqlEngineVersion: typing.Optional[dict[str, AuroraMysqlEngineVersionDef]] = pydantic.Field(None)
    AuroraPostgresEngineVersion: typing.Optional[dict[str, AuroraPostgresEngineVersionDef]] = pydantic.Field(None)
    ClusterInstance: typing.Optional[dict[str, ClusterInstanceDef]] = pydantic.Field(None)
    ClusterInstanceType: typing.Optional[dict[str, ClusterInstanceTypeDef]] = pydantic.Field(None)
    Credentials: typing.Optional[dict[str, CredentialsDef]] = pydantic.Field(None)
    DatabaseClusterBase: typing.Optional[dict[str, DatabaseClusterBaseDef]] = pydantic.Field(None)
    DatabaseClusterEngine: typing.Optional[dict[str, DatabaseClusterEngineDef]] = pydantic.Field(None)
    DatabaseInstanceBase: typing.Optional[dict[str, DatabaseInstanceBaseDef]] = pydantic.Field(None)
    DatabaseInstanceEngine: typing.Optional[dict[str, DatabaseInstanceEngineDef]] = pydantic.Field(None)
    Endpoint: typing.Optional[dict[str, EndpointDef]] = pydantic.Field(None)
    MariaDbEngineVersion: typing.Optional[dict[str, MariaDbEngineVersionDef]] = pydantic.Field(None)
    MysqlEngineVersion: typing.Optional[dict[str, MysqlEngineVersionDef]] = pydantic.Field(None)
    OracleEngineVersion: typing.Optional[dict[str, OracleEngineVersionDef]] = pydantic.Field(None)
    PostgresEngineVersion: typing.Optional[dict[str, PostgresEngineVersionDef]] = pydantic.Field(None)
    ProxyTarget: typing.Optional[dict[str, ProxyTargetDef]] = pydantic.Field(None)
    SessionPinningFilter: typing.Optional[dict[str, SessionPinningFilterDef]] = pydantic.Field(None)
    SnapshotCredentials: typing.Optional[dict[str, SnapshotCredentialsDef]] = pydantic.Field(None)
    SqlServerEngineVersion: typing.Optional[dict[str, SqlServerEngineVersionDef]] = pydantic.Field(None)
    DatabaseCluster: typing.Optional[dict[str, DatabaseClusterDef]] = pydantic.Field(None)
    DatabaseClusterFromSnapshot: typing.Optional[dict[str, DatabaseClusterFromSnapshotDef]] = pydantic.Field(None)
    DatabaseInstance: typing.Optional[dict[str, DatabaseInstanceDef]] = pydantic.Field(None)
    DatabaseInstanceFromSnapshot: typing.Optional[dict[str, DatabaseInstanceFromSnapshotDef]] = pydantic.Field(None)
    DatabaseInstanceReadReplica: typing.Optional[dict[str, DatabaseInstanceReadReplicaDef]] = pydantic.Field(None)
    DatabaseProxy: typing.Optional[dict[str, DatabaseProxyDef]] = pydantic.Field(None)
    DatabaseSecret: typing.Optional[dict[str, DatabaseSecretDef]] = pydantic.Field(None)
    OptionGroup: typing.Optional[dict[str, OptionGroupDef]] = pydantic.Field(None)
    ParameterGroup: typing.Optional[dict[str, ParameterGroupDef]] = pydantic.Field(None)
    ServerlessCluster: typing.Optional[dict[str, ServerlessClusterDef]] = pydantic.Field(None)
    ServerlessClusterFromSnapshot: typing.Optional[dict[str, ServerlessClusterFromSnapshotDef]] = pydantic.Field(None)
    SubnetGroup: typing.Optional[dict[str, SubnetGroupDef]] = pydantic.Field(None)
    AuroraClusterEngineProps: typing.Optional[dict[str, AuroraClusterEnginePropsDef]] = pydantic.Field(None)
    AuroraMysqlClusterEngineProps: typing.Optional[dict[str, AuroraMysqlClusterEnginePropsDef]] = pydantic.Field(None)
    AuroraPostgresClusterEngineProps: typing.Optional[dict[str, AuroraPostgresClusterEnginePropsDef]] = pydantic.Field(None)
    AuroraPostgresEngineFeatures: typing.Optional[dict[str, AuroraPostgresEngineFeaturesDef]] = pydantic.Field(None)
    BackupProps: typing.Optional[dict[str, BackupPropsDef]] = pydantic.Field(None)
    CfnDBCluster_DBClusterRoleProperty: typing.Optional[dict[str, CfnDBCluster_DBClusterRolePropertyDef]] = pydantic.Field(None)
    CfnDBCluster_EndpointProperty: typing.Optional[dict[str, CfnDBCluster_EndpointPropertyDef]] = pydantic.Field(None)
    CfnDBCluster_MasterUserSecretProperty: typing.Optional[dict[str, CfnDBCluster_MasterUserSecretPropertyDef]] = pydantic.Field(None)
    CfnDBCluster_ReadEndpointProperty: typing.Optional[dict[str, CfnDBCluster_ReadEndpointPropertyDef]] = pydantic.Field(None)
    CfnDBCluster_ScalingConfigurationProperty: typing.Optional[dict[str, CfnDBCluster_ScalingConfigurationPropertyDef]] = pydantic.Field(None)
    CfnDBCluster_ServerlessV2ScalingConfigurationProperty: typing.Optional[dict[str, CfnDBCluster_ServerlessV2ScalingConfigurationPropertyDef]] = pydantic.Field(None)
    CfnDBInstance_CertificateDetailsProperty: typing.Optional[dict[str, CfnDBInstance_CertificateDetailsPropertyDef]] = pydantic.Field(None)
    CfnDBInstance_DBInstanceRoleProperty: typing.Optional[dict[str, CfnDBInstance_DBInstanceRolePropertyDef]] = pydantic.Field(None)
    CfnDBInstance_EndpointProperty: typing.Optional[dict[str, CfnDBInstance_EndpointPropertyDef]] = pydantic.Field(None)
    CfnDBInstance_MasterUserSecretProperty: typing.Optional[dict[str, CfnDBInstance_MasterUserSecretPropertyDef]] = pydantic.Field(None)
    CfnDBInstance_ProcessorFeatureProperty: typing.Optional[dict[str, CfnDBInstance_ProcessorFeaturePropertyDef]] = pydantic.Field(None)
    CfnDBProxy_AuthFormatProperty: typing.Optional[dict[str, CfnDBProxy_AuthFormatPropertyDef]] = pydantic.Field(None)
    CfnDBProxy_TagFormatProperty: typing.Optional[dict[str, CfnDBProxy_TagFormatPropertyDef]] = pydantic.Field(None)
    CfnDBProxyEndpoint_TagFormatProperty: typing.Optional[dict[str, CfnDBProxyEndpoint_TagFormatPropertyDef]] = pydantic.Field(None)
    CfnDBProxyTargetGroup_ConnectionPoolConfigurationInfoFormatProperty: typing.Optional[dict[str, CfnDBProxyTargetGroup_ConnectionPoolConfigurationInfoFormatPropertyDef]] = pydantic.Field(None)
    CfnDBSecurityGroup_IngressProperty: typing.Optional[dict[str, CfnDBSecurityGroup_IngressPropertyDef]] = pydantic.Field(None)
    CfnOptionGroup_OptionConfigurationProperty: typing.Optional[dict[str, CfnOptionGroup_OptionConfigurationPropertyDef]] = pydantic.Field(None)
    CfnOptionGroup_OptionSettingProperty: typing.Optional[dict[str, CfnOptionGroup_OptionSettingPropertyDef]] = pydantic.Field(None)
    ClusterEngineBindOptions: typing.Optional[dict[str, ClusterEngineBindOptionsDef]] = pydantic.Field(None)
    ClusterEngineConfig: typing.Optional[dict[str, ClusterEngineConfigDef]] = pydantic.Field(None)
    ClusterEngineFeatures: typing.Optional[dict[str, ClusterEngineFeaturesDef]] = pydantic.Field(None)
    ClusterInstanceBindOptions: typing.Optional[dict[str, ClusterInstanceBindOptionsDef]] = pydantic.Field(None)
    ClusterInstanceOptions: typing.Optional[dict[str, ClusterInstanceOptionsDef]] = pydantic.Field(None)
    ClusterInstanceProps: typing.Optional[dict[str, ClusterInstancePropsDef]] = pydantic.Field(None)
    CommonRotationUserOptions: typing.Optional[dict[str, CommonRotationUserOptionsDef]] = pydantic.Field(None)
    CredentialsBaseOptions: typing.Optional[dict[str, CredentialsBaseOptionsDef]] = pydantic.Field(None)
    CredentialsFromUsernameOptions: typing.Optional[dict[str, CredentialsFromUsernameOptionsDef]] = pydantic.Field(None)
    DatabaseClusterAttributes: typing.Optional[dict[str, DatabaseClusterAttributesDef]] = pydantic.Field(None)
    DatabaseClusterFromSnapshotProps: typing.Optional[dict[str, DatabaseClusterFromSnapshotPropsDef]] = pydantic.Field(None)
    DatabaseClusterProps: typing.Optional[dict[str, DatabaseClusterPropsDef]] = pydantic.Field(None)
    DatabaseInstanceAttributes: typing.Optional[dict[str, DatabaseInstanceAttributesDef]] = pydantic.Field(None)
    DatabaseInstanceFromSnapshotProps: typing.Optional[dict[str, DatabaseInstanceFromSnapshotPropsDef]] = pydantic.Field(None)
    DatabaseInstanceNewProps: typing.Optional[dict[str, DatabaseInstanceNewPropsDef]] = pydantic.Field(None)
    DatabaseInstanceProps: typing.Optional[dict[str, DatabaseInstancePropsDef]] = pydantic.Field(None)
    DatabaseInstanceReadReplicaProps: typing.Optional[dict[str, DatabaseInstanceReadReplicaPropsDef]] = pydantic.Field(None)
    DatabaseInstanceSourceProps: typing.Optional[dict[str, DatabaseInstanceSourcePropsDef]] = pydantic.Field(None)
    DatabaseProxyAttributes: typing.Optional[dict[str, DatabaseProxyAttributesDef]] = pydantic.Field(None)
    DatabaseProxyOptions: typing.Optional[dict[str, DatabaseProxyOptionsDef]] = pydantic.Field(None)
    DatabaseProxyProps: typing.Optional[dict[str, DatabaseProxyPropsDef]] = pydantic.Field(None)
    DatabaseSecretProps: typing.Optional[dict[str, DatabaseSecretPropsDef]] = pydantic.Field(None)
    EngineVersion: typing.Optional[dict[str, EngineVersionDef]] = pydantic.Field(None)
    InstanceEngineBindOptions: typing.Optional[dict[str, InstanceEngineBindOptionsDef]] = pydantic.Field(None)
    InstanceEngineConfig: typing.Optional[dict[str, InstanceEngineConfigDef]] = pydantic.Field(None)
    InstanceEngineFeatures: typing.Optional[dict[str, InstanceEngineFeaturesDef]] = pydantic.Field(None)
    InstanceProps: typing.Optional[dict[str, InstancePropsDef]] = pydantic.Field(None)
    MariaDbInstanceEngineProps: typing.Optional[dict[str, MariaDbInstanceEnginePropsDef]] = pydantic.Field(None)
    MySqlInstanceEngineProps: typing.Optional[dict[str, MySqlInstanceEnginePropsDef]] = pydantic.Field(None)
    OptionConfiguration: typing.Optional[dict[str, OptionConfigurationDef]] = pydantic.Field(None)
    OptionGroupProps: typing.Optional[dict[str, OptionGroupPropsDef]] = pydantic.Field(None)
    OracleEeCdbInstanceEngineProps: typing.Optional[dict[str, OracleEeCdbInstanceEnginePropsDef]] = pydantic.Field(None)
    OracleEeInstanceEngineProps: typing.Optional[dict[str, OracleEeInstanceEnginePropsDef]] = pydantic.Field(None)
    OracleSe2CdbInstanceEngineProps: typing.Optional[dict[str, OracleSe2CdbInstanceEnginePropsDef]] = pydantic.Field(None)
    OracleSe2InstanceEngineProps: typing.Optional[dict[str, OracleSe2InstanceEnginePropsDef]] = pydantic.Field(None)
    ParameterGroupClusterBindOptions: typing.Optional[dict[str, ParameterGroupClusterBindOptionsDef]] = pydantic.Field(None)
    ParameterGroupClusterConfig: typing.Optional[dict[str, ParameterGroupClusterConfigDef]] = pydantic.Field(None)
    ParameterGroupInstanceBindOptions: typing.Optional[dict[str, ParameterGroupInstanceBindOptionsDef]] = pydantic.Field(None)
    ParameterGroupInstanceConfig: typing.Optional[dict[str, ParameterGroupInstanceConfigDef]] = pydantic.Field(None)
    ParameterGroupProps: typing.Optional[dict[str, ParameterGroupPropsDef]] = pydantic.Field(None)
    PostgresEngineFeatures: typing.Optional[dict[str, PostgresEngineFeaturesDef]] = pydantic.Field(None)
    PostgresInstanceEngineProps: typing.Optional[dict[str, PostgresInstanceEnginePropsDef]] = pydantic.Field(None)
    ProcessorFeatures: typing.Optional[dict[str, ProcessorFeaturesDef]] = pydantic.Field(None)
    ProvisionedClusterInstanceProps: typing.Optional[dict[str, ProvisionedClusterInstancePropsDef]] = pydantic.Field(None)
    ProxyTargetConfig: typing.Optional[dict[str, ProxyTargetConfigDef]] = pydantic.Field(None)
    RotationMultiUserOptions: typing.Optional[dict[str, RotationMultiUserOptionsDef]] = pydantic.Field(None)
    RotationSingleUserOptions: typing.Optional[dict[str, RotationSingleUserOptionsDef]] = pydantic.Field(None)
    ServerlessClusterAttributes: typing.Optional[dict[str, ServerlessClusterAttributesDef]] = pydantic.Field(None)
    ServerlessClusterFromSnapshotProps: typing.Optional[dict[str, ServerlessClusterFromSnapshotPropsDef]] = pydantic.Field(None)
    ServerlessClusterProps: typing.Optional[dict[str, ServerlessClusterPropsDef]] = pydantic.Field(None)
    ServerlessScalingOptions: typing.Optional[dict[str, ServerlessScalingOptionsDef]] = pydantic.Field(None)
    ServerlessV2ClusterInstanceProps: typing.Optional[dict[str, ServerlessV2ClusterInstancePropsDef]] = pydantic.Field(None)
    SnapshotCredentialsFromGeneratedPasswordOptions: typing.Optional[dict[str, SnapshotCredentialsFromGeneratedPasswordOptionsDef]] = pydantic.Field(None)
    SqlServerEeInstanceEngineProps: typing.Optional[dict[str, SqlServerEeInstanceEnginePropsDef]] = pydantic.Field(None)
    SqlServerExInstanceEngineProps: typing.Optional[dict[str, SqlServerExInstanceEnginePropsDef]] = pydantic.Field(None)
    SqlServerSeInstanceEngineProps: typing.Optional[dict[str, SqlServerSeInstanceEnginePropsDef]] = pydantic.Field(None)
    SqlServerWebInstanceEngineProps: typing.Optional[dict[str, SqlServerWebInstanceEnginePropsDef]] = pydantic.Field(None)
    SubnetGroupProps: typing.Optional[dict[str, SubnetGroupPropsDef]] = pydantic.Field(None)
    CfnDBCluster: typing.Optional[dict[str, CfnDBClusterDef]] = pydantic.Field(None)
    CfnDBClusterParameterGroup: typing.Optional[dict[str, CfnDBClusterParameterGroupDef]] = pydantic.Field(None)
    CfnDBInstance: typing.Optional[dict[str, CfnDBInstanceDef]] = pydantic.Field(None)
    CfnDBParameterGroup: typing.Optional[dict[str, CfnDBParameterGroupDef]] = pydantic.Field(None)
    CfnDBProxy: typing.Optional[dict[str, CfnDBProxyDef]] = pydantic.Field(None)
    CfnDBProxyEndpoint: typing.Optional[dict[str, CfnDBProxyEndpointDef]] = pydantic.Field(None)
    CfnDBProxyTargetGroup: typing.Optional[dict[str, CfnDBProxyTargetGroupDef]] = pydantic.Field(None)
    CfnDBSecurityGroup: typing.Optional[dict[str, CfnDBSecurityGroupDef]] = pydantic.Field(None)
    CfnDBSecurityGroupIngress: typing.Optional[dict[str, CfnDBSecurityGroupIngressDef]] = pydantic.Field(None)
    CfnDBSubnetGroup: typing.Optional[dict[str, CfnDBSubnetGroupDef]] = pydantic.Field(None)
    CfnEventSubscription: typing.Optional[dict[str, CfnEventSubscriptionDef]] = pydantic.Field(None)
    CfnGlobalCluster: typing.Optional[dict[str, CfnGlobalClusterDef]] = pydantic.Field(None)
    CfnOptionGroup: typing.Optional[dict[str, CfnOptionGroupDef]] = pydantic.Field(None)
    CfnDBClusterParameterGroupProps: typing.Optional[dict[str, CfnDBClusterParameterGroupPropsDef]] = pydantic.Field(None)
    CfnDBClusterProps: typing.Optional[dict[str, CfnDBClusterPropsDef]] = pydantic.Field(None)
    CfnDBInstanceProps: typing.Optional[dict[str, CfnDBInstancePropsDef]] = pydantic.Field(None)
    CfnDBParameterGroupProps: typing.Optional[dict[str, CfnDBParameterGroupPropsDef]] = pydantic.Field(None)
    CfnDBProxyEndpointProps: typing.Optional[dict[str, CfnDBProxyEndpointPropsDef]] = pydantic.Field(None)
    CfnDBProxyProps: typing.Optional[dict[str, CfnDBProxyPropsDef]] = pydantic.Field(None)
    CfnDBProxyTargetGroupProps: typing.Optional[dict[str, CfnDBProxyTargetGroupPropsDef]] = pydantic.Field(None)
    CfnDBSecurityGroupIngressProps: typing.Optional[dict[str, CfnDBSecurityGroupIngressPropsDef]] = pydantic.Field(None)
    CfnDBSecurityGroupProps: typing.Optional[dict[str, CfnDBSecurityGroupPropsDef]] = pydantic.Field(None)
    CfnDBSubnetGroupProps: typing.Optional[dict[str, CfnDBSubnetGroupPropsDef]] = pydantic.Field(None)
    CfnEventSubscriptionProps: typing.Optional[dict[str, CfnEventSubscriptionPropsDef]] = pydantic.Field(None)
    CfnGlobalClusterProps: typing.Optional[dict[str, CfnGlobalClusterPropsDef]] = pydantic.Field(None)
    CfnOptionGroupProps: typing.Optional[dict[str, CfnOptionGroupPropsDef]] = pydantic.Field(None)
    ...
