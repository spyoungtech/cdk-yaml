from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_logs.DataIdentifier
class DataIdentifierDef(BaseClass):
    identifier: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['identifier']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.DataIdentifier'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.DataProtectionPolicy
class DataProtectionPolicyDef(BaseClass):
    identifiers: typing.Union[typing.Sequence[models.aws_logs.DataIdentifierDef], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='List of data protection identifiers. Must be in the following list: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/protect-sensitive-log-data-types.html')
    delivery_stream_name_audit_destination: typing.Optional[str] = pydantic.Field(None, description='Amazon Kinesis Data Firehose delivery stream to send audit findings to. The delivery stream must already exist. Default: - no firehose delivery stream audit destination\n')
    description: typing.Optional[str] = pydantic.Field(None, description="Description of the data protection policy. Default: - 'cdk generated data protection policy'\n")
    log_group_audit_destination: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='CloudWatch Logs log group to send audit findings to. The log group must already exist prior to creating the data protection policy. Default: - no CloudWatch Logs audit destination\n')
    name: typing.Optional[str] = pydantic.Field(None, description="Name of the data protection policy. Default: - 'data-protection-policy-cdk'\n")
    s3_bucket_audit_destination: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='S3 bucket to send audit findings to. The bucket must already exist. Default: - no S3 bucket audit destination')
    _init_params: typing.ClassVar[list[str]] = ['identifiers', 'delivery_stream_name_audit_destination', 'description', 'log_group_audit_destination', 'name', 's3_bucket_audit_destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.DataProtectionPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.FilterPattern
class FilterPatternDef(BaseClass):
    _init_params: typing.ClassVar[list[str]] = []
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = ['all', 'all_events', 'all_terms', 'any', 'any_term', 'any_term_group', 'boolean_value', 'exists', 'literal', 'not_exists', 'number_value', 'space_delimited', 'string_value']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.FilterPattern'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['all_events', 'all_terms', 'any_term', 'any_term_group', 'literal']
    ...


    all_events: typing.Optional[models.aws_logs.FilterPatternDefAllEventsParams] = pydantic.Field(None, description='A log pattern that matches all events.')
    all_terms: typing.Optional[models.aws_logs.FilterPatternDefAllTermsParams] = pydantic.Field(None, description='A log pattern that matches if all the strings given appear in the event.')
    any_term: typing.Optional[models.aws_logs.FilterPatternDefAnyTermParams] = pydantic.Field(None, description='A log pattern that matches if any of the strings given appear in the event.')
    any_term_group: typing.Optional[models.aws_logs.FilterPatternDefAnyTermGroupParams] = pydantic.Field(None, description='A log pattern that matches if any of the given term groups matches the event.\nA term group matches an event if all the terms in it appear in the event string.')
    literal: typing.Optional[models.aws_logs.FilterPatternDefLiteralParams] = pydantic.Field(None, description='Use the given string as log pattern.\nSee https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html\nfor information on writing log patterns.')
    resource_config: typing.Optional[models.aws_logs.FilterPatternDefConfig] = pydantic.Field(None)


class FilterPatternDefConfig(pydantic.BaseModel):
    all: typing.Optional[list[models.aws_logs.FilterPatternDefAllParams]] = pydantic.Field(None, description='A JSON log pattern that matches if all given JSON log patterns match.')
    any: typing.Optional[list[models.aws_logs.FilterPatternDefAnyParams]] = pydantic.Field(None, description='A JSON log pattern that matches if any of the given JSON log patterns match.')
    boolean_value: typing.Optional[list[models.aws_logs.FilterPatternDefBooleanValueParams]] = pydantic.Field(None, description='A JSON log pattern that matches if the field exists and equals the boolean value.')
    exists: typing.Optional[list[models.aws_logs.FilterPatternDefExistsParams]] = pydantic.Field(None, description="A JSON log patter that matches if the field exists.\nThis is a readable convenience wrapper over 'field = *'")
    not_exists: typing.Optional[list[models.aws_logs.FilterPatternDefNotExistsParams]] = pydantic.Field(None, description='A JSON log pattern that matches if the field does not exist.')
    number_value: typing.Optional[list[models.aws_logs.FilterPatternDefNumberValueParams]] = pydantic.Field(None, description="A JSON log pattern that compares numerical values.\nThis pattern only matches if the event is a JSON event, and the indicated field inside\ncompares with the value in the indicated way.\n\nUse '$' to indicate the root of the JSON structure. The comparison operator can only\ncompare equality or inequality. The '*' wildcard may appear in the value may at the\nstart or at the end.\n\nFor more information, see:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html")
    space_delimited: typing.Optional[list[models.aws_logs.FilterPatternDefSpaceDelimitedParams]] = pydantic.Field(None, description='A space delimited log pattern matcher.\nThe log event is divided into space-delimited columns (optionally\nenclosed by "" or [] to capture spaces into column values), and names\nare given to each column.\n\n\'...\' may be specified once to match any number of columns.\n\nAfterwards, conditions may be added to individual columns.')
    string_value: typing.Optional[list[models.aws_logs.FilterPatternDefStringValueParams]] = pydantic.Field(None, description="A JSON log pattern that compares string values.\nThis pattern only matches if the event is a JSON event, and the indicated field inside\ncompares with the string value.\n\nUse '$' to indicate the root of the JSON structure. The comparison operator can only\ncompare equality or inequality. The '*' wildcard may appear in the value may at the\nstart or at the end.\n\nFor more information, see:\n\nhttps://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html")

class FilterPatternDefAllParams(pydantic.BaseModel):
    patterns: list[models.aws_logs.JsonPatternDef] = pydantic.Field(...)
    ...

class FilterPatternDefAllEventsParams(pydantic.BaseModel):
    ...

class FilterPatternDefAllTermsParams(pydantic.BaseModel):
    terms: list[str] = pydantic.Field(...)
    ...

class FilterPatternDefAnyParams(pydantic.BaseModel):
    patterns: list[models.aws_logs.JsonPatternDef] = pydantic.Field(...)
    ...

class FilterPatternDefAnyTermParams(pydantic.BaseModel):
    terms: list[str] = pydantic.Field(...)
    ...

class FilterPatternDefAnyTermGroupParams(pydantic.BaseModel):
    term_groups: list[list[str]] = pydantic.Field(...)
    ...

class FilterPatternDefBooleanValueParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description='Field inside JSON. Example: "$.myField"\n')
    value: bool = pydantic.Field(..., description='The value to match.')
    ...

class FilterPatternDefExistsParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description='Field inside JSON. Example: "$.myField"')
    ...

class FilterPatternDefLiteralParams(pydantic.BaseModel):
    log_pattern_string: str = pydantic.Field(..., description='The pattern string to use.')
    ...

class FilterPatternDefNotExistsParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description='Field inside JSON. Example: "$.myField"')
    ...

class FilterPatternDefNumberValueParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description='Field inside JSON. Example: "$.myField"\n')
    comparison: str = pydantic.Field(..., description='Comparison to carry out. One of =, !=, <, <=, >, >=.\n')
    value: typing.Union[int, float] = pydantic.Field(..., description='The numerical value to compare to.')
    ...

class FilterPatternDefSpaceDelimitedParams(pydantic.BaseModel):
    columns: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_logs.SpaceDelimitedTextPatternDefConfig]] = pydantic.Field(None)
    ...

class FilterPatternDefStringValueParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description='Field inside JSON. Example: "$.myField"\n')
    comparison: str = pydantic.Field(..., description='Comparison to carry out. Either = or !=.\n')
    value: str = pydantic.Field(..., description="The string value to compare to. May use '*' as wildcard at start or end of string.")
    ...


#  autogenerated from aws_cdk.aws_logs.JsonPattern
class JsonPatternDef(BaseClass):
    json_pattern_string: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['json_pattern_string']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.JsonPattern'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.QueryString
class QueryStringDef(BaseClass):
    display: typing.Optional[str] = pydantic.Field(None, description='Specifies which fields to display in the query results. Default: - no display in QueryString')
    fields: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Retrieves the specified fields from log events for display. Default: - no fields in QueryString\n')
    filter: typing.Optional[str] = pydantic.Field(None, description='(deprecated) A single statement for filtering the results of a query based on a boolean expression. Default: - no filter in QueryString\n')
    filter_statements: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of one or more statements for filtering the results of a query based on a boolean expression. Each provided statement generates a separate filter line in the query string. Note: If provided, this property overrides any value provided for the ``filter`` property. Default: - no filter in QueryString\n')
    limit: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the number of log events returned by the query. Default: - no limit in QueryString\n')
    parse: typing.Optional[str] = pydantic.Field(None, description='(deprecated) A single statement for parsing data from a log field and creating ephemeral fields that can be processed further in the query. Default: - no parse in QueryString\n')
    parse_statements: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of one or more statements for parsing data from a log field and creating ephemeral fields that can be processed further in the query. Each provided statement generates a separate parse line in the query string. Note: If provided, this property overrides any value provided for the ``parse`` property. Default: - no parse in QueryString\n')
    sort: typing.Optional[str] = pydantic.Field(None, description='Sorts the retrieved log events. Default: - no sort in QueryString\n')
    stats: typing.Optional[str] = pydantic.Field(None, description='Uses log field values to calculate aggregate statistics. Default: - no stats in QueryString')
    _init_params: typing.ClassVar[list[str]] = ['display', 'fields', 'filter', 'filter_statements', 'limit', 'parse', 'parse_statements', 'sort', 'stats']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.QueryString'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.SpaceDelimitedTextPattern
class SpaceDelimitedTextPatternDef(BaseClass):
    columns: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    restrictions: typing.Union[typing.Mapping[str, typing.Sequence[typing.Union[models.aws_logs.ColumnRestrictionDef, dict[str, typing.Any]]]], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['columns', 'restrictions']
    _method_names: typing.ClassVar[list[str]] = ['where_number', 'where_string']
    _classmethod_names: typing.ClassVar[list[str]] = ['construct']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.SpaceDelimitedTextPattern'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.SpaceDelimitedTextPatternDefConfig] = pydantic.Field(None)


class SpaceDelimitedTextPatternDefConfig(pydantic.BaseModel):
    construct_: typing.Optional[list[models.aws_logs.SpaceDelimitedTextPatternDefConstructParams]] = pydantic.Field(None, description="Construct a new instance of a space delimited text pattern.\nSince this class must be public, we can't rely on the user only creating it through\nthe ``LogPattern.spaceDelimited()`` factory function. We must therefore validate the\nargument in the constructor. Since we're returning a copy on every mutation, and we\ndon't want to re-validate the same things on every construction, we provide a limited\nset of mutator functions and only validate the new data every time.", alias='construct')
    where_number: typing.Optional[list[models.aws_logs.SpaceDelimitedTextPatternDefWhereNumberParams]] = pydantic.Field(None, description='Restrict where the pattern applies.')
    where_string: typing.Optional[list[models.aws_logs.SpaceDelimitedTextPatternDefWhereStringParams]] = pydantic.Field(None, description='Restrict where the pattern applies.')

class SpaceDelimitedTextPatternDefConstructParams(pydantic.BaseModel):
    columns: typing.Sequence[str] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_logs.SpaceDelimitedTextPatternDefConfig]] = pydantic.Field(None)
    ...

class SpaceDelimitedTextPatternDefWhereNumberParams(pydantic.BaseModel):
    column_name: str = pydantic.Field(..., description='-\n')
    comparison: str = pydantic.Field(..., description='-\n')
    value: typing.Union[int, float] = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_logs.SpaceDelimitedTextPatternDefConfig]] = pydantic.Field(None)
    ...

class SpaceDelimitedTextPatternDefWhereStringParams(pydantic.BaseModel):
    column_name: str = pydantic.Field(..., description='-\n')
    comparison: str = pydantic.Field(..., description='-\n')
    value: str = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_logs.SpaceDelimitedTextPatternDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_logs.CrossAccountDestination
class CrossAccountDestinationDef(BaseConstruct):
    role: typing.Union[_REQUIRED_INIT_PARAM, models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(REQUIRED_INIT_PARAM, description="The role to assume that grants permissions to write to 'target'. The role must be assumable by 'logs.{REGION}.amazonaws.com'.\n")
    target_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The log destination target's ARN.\n")
    destination_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log destination. Default: Automatically generated')
    _init_params: typing.ClassVar[list[str]] = ['role', 'target_arn', 'destination_name']
    _method_names: typing.ClassVar[list[str]] = ['add_to_policy', 'apply_removal_policy', 'bind']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CrossAccountDestination'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CrossAccountDestinationDefConfig] = pydantic.Field(None)


class CrossAccountDestinationDefConfig(pydantic.BaseModel):
    add_to_policy: typing.Optional[list[models.aws_logs.CrossAccountDestinationDefAddToPolicyParams]] = pydantic.Field(None, description='')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    bind: typing.Optional[list[models.aws_logs.CrossAccountDestinationDefBindParams]] = pydantic.Field(None, description='Return the properties required to send subscription events to this destination.\nIf necessary, the destination can use the properties of the SubscriptionFilter\nobject itself to configure its permissions to allow the subscription to write\nto it.\n\nThe destination may reconfigure its own permissions in response to this\nfunction call.')
    policy_document_config: typing.Optional[models.aws_iam.PolicyDocumentDefConfig] = pydantic.Field(None)

class CrossAccountDestinationDefAddToPolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class CrossAccountDestinationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class CrossAccountDestinationDefBindParams(pydantic.BaseModel):
    ...


#  autogenerated from aws_cdk.aws_logs.LogGroup
class LogGroupDef(BaseConstruct):
    data_protection_policy: typing.Optional[models.aws_logs.DataProtectionPolicyDef] = pydantic.Field(None, description='Data Protection Policy for this log group. Default: - no data protection policy\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS customer managed key to encrypt the log group with. Default: Server-side encrpytion managed by the CloudWatch Logs service\n')
    log_group_name: typing.Optional[str] = pydantic.Field(None, description='Name of the log group. Default: Automatically generated\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Determine the removal policy of this log group. Normally you want to retain the log group so you can diagnose issues from logs even after a deployment that no longer includes the log group. In that case, use the normal date-based retention policy to age out your logs. Default: RemovalPolicy.Retain\n')
    retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='How long, in days, the log contents will be retained. To retain all logs, set this value to RetentionDays.INFINITE. Default: RetentionDays.TWO_YEARS')
    _init_params: typing.ClassVar[list[str]] = ['data_protection_policy', 'encryption_key', 'log_group_name', 'removal_policy', 'retention']
    _method_names: typing.ClassVar[list[str]] = ['add_metric_filter', 'add_stream', 'add_subscription_filter', 'add_to_resource_policy', 'apply_removal_policy', 'extract_metric', 'grant', 'grant_read', 'grant_write', 'log_group_physical_name']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_log_group_arn', 'from_log_group_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_log_group_arn', 'from_log_group_name']
    ...


    from_log_group_arn: typing.Optional[models.aws_logs.LogGroupDefFromLogGroupArnParams] = pydantic.Field(None, description='Import an existing LogGroup given its ARN.')
    from_log_group_name: typing.Optional[models.aws_logs.LogGroupDefFromLogGroupNameParams] = pydantic.Field(None, description='Import an existing LogGroup given its name.')
    resource_config: typing.Optional[models.aws_logs.LogGroupDefConfig] = pydantic.Field(None)


class LogGroupDefConfig(pydantic.BaseModel):
    add_metric_filter: typing.Optional[list[models.aws_logs.LogGroupDefAddMetricFilterParams]] = pydantic.Field(None, description='Create a new Metric Filter on this Log Group.')
    add_stream: typing.Optional[list[models.aws_logs.LogGroupDefAddStreamParams]] = pydantic.Field(None, description='Create a new Log Stream for this Log Group.')
    add_subscription_filter: typing.Optional[list[models.aws_logs.LogGroupDefAddSubscriptionFilterParams]] = pydantic.Field(None, description='Create a new Subscription Filter on this Log Group.')
    add_to_resource_policy: typing.Optional[list[models.aws_logs.LogGroupDefAddToResourcePolicyParams]] = pydantic.Field(None, description='Adds a statement to the resource policy associated with this log group.\nA resource policy will be automatically created upon the first call to ``addToResourcePolicy``.\n\nAny ARN Principals inside of the statement will be converted into AWS Account ID strings\nbecause CloudWatch Logs Resource Policies do not accept ARN principals.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    extract_metric: typing.Optional[list[models.aws_logs.LogGroupDefExtractMetricParams]] = pydantic.Field(None, description='Extract a metric from structured log events in the LogGroup.\nCreates a MetricFilter on this LogGroup that will extract the value\nof the indicated JSON field in all records where it occurs.\n\nThe metric will be available in CloudWatch Metrics under the\nindicated namespace and name.')
    grant: typing.Optional[list[models.aws_logs.LogGroupDefGrantParams]] = pydantic.Field(None, description='Give the indicated permissions on this log group and all streams.')
    grant_read: typing.Optional[list[models.aws_logs.LogGroupDefGrantReadParams]] = pydantic.Field(None, description='Give permissions to read and filter events from this log group.')
    grant_write: typing.Optional[list[models.aws_logs.LogGroupDefGrantWriteParams]] = pydantic.Field(None, description='Give permissions to create and write to streams in this log group.')
    log_group_physical_name: typing.Optional[bool] = pydantic.Field(None, description='Public method to get the physical name of this log group.\n:return: Physical name of log group')

class LogGroupDefAddMetricFilterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Unique identifier for the construct in its parent.\n')
    filter_pattern: typing.Union[models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(..., description='Pattern to search for log events.\n')
    metric_name: str = pydantic.Field(..., description='The name of the metric to emit.\n')
    metric_namespace: str = pydantic.Field(..., description='The namespace of the metric to emit.\n')
    default_value: typing.Union[int, float, None] = pydantic.Field(None, description='The value to emit if the pattern does not match a particular event. Default: No metric emitted.\n')
    dimensions: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The fields to use as dimensions for the metric. One metric filter can include as many as three dimensions. Default: - No dimensions attached to metrics.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the metric filter. Default: - Cloudformation generated name.\n')
    metric_value: typing.Optional[str] = pydantic.Field(None, description='The value to emit for the metric. Can either be a literal number (typically "1"), or the name of a field in the structure to take the value from the matched event. If you are using a field value, the field value must have been matched using the pattern. If you want to specify a field from a matched JSON structure, use \'$.fieldName\', and make sure the field is in the pattern (if only as \'$.fieldName = *\'). If you want to specify a field from a matched space-delimited structure, use \'$fieldName\'. Default: "1"\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='The unit to assign to the metric. Default: - No unit attached to metrics.')
    return_config: typing.Optional[list[models.aws_logs.MetricFilterDefConfig]] = pydantic.Field(None)
    ...

class LogGroupDefAddStreamParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Unique identifier for the construct in its parent.\n')
    log_stream_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log stream to create. The name must be unique within the log group. Default: Automatically generated')
    return_config: typing.Optional[list[models.aws_logs.LogStreamDefConfig]] = pydantic.Field(None)
    ...

class LogGroupDefAddSubscriptionFilterParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='Unique identifier for the construct in its parent.\n')
    destination: typing.Union[models.aws_logs.CrossAccountDestinationDef, models.aws_logs_destinations.KinesisDestinationDef, models.aws_logs_destinations.LambdaDestinationDef] = pydantic.Field(..., description='The destination to send the filtered events to. For example, a Kinesis stream or a Lambda function.\n')
    filter_pattern: typing.Union[models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(..., description='Log events matching this pattern will be sent to the destination.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription filter. Default: Automatically generated')
    return_config: typing.Optional[list[models.aws_logs.SubscriptionFilterDefConfig]] = pydantic.Field(None)
    ...

class LogGroupDefAddToResourcePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='The policy statement to add.')
    ...

class LogGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class LogGroupDefExtractMetricParams(pydantic.BaseModel):
    json_field: str = pydantic.Field(..., description="JSON field to extract (example: '$.myfield').\n")
    metric_namespace: str = pydantic.Field(..., description='Namespace to emit the metric under.\n')
    metric_name: str = pydantic.Field(..., description='Name to emit the metric under.\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class LogGroupDefFromLogGroupArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    log_group_arn: str = pydantic.Field(..., description='-')
    ...

class LogGroupDefFromLogGroupNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    log_group_name: str = pydantic.Field(..., description='-')
    ...

class LogGroupDefGrantParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-\n')
    actions: list[str] = pydantic.Field(...)
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class LogGroupDefGrantReadParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class LogGroupDefGrantWriteParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_logs.LogRetention
class LogRetentionDef(BaseConstruct):
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group name.\n')
    retention: typing.Union[aws_cdk.aws_logs.RetentionDays, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of days log events are kept in CloudWatch Logs.\n')
    log_group_region: typing.Optional[str] = pydantic.Field(None, description='The region where the log group should be created. Default: - same region as the stack\n')
    log_retention_retry_options: typing.Union[models.aws_logs.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Retry options for all AWS API calls. Default: - AWS SDK default retry options\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removalPolicy for the log group when the stack is deleted. Default: RemovalPolicy.RETAIN\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource. Default: - A new role is created')
    _init_params: typing.ClassVar[list[str]] = ['log_group_name', 'retention', 'log_group_region', 'log_retention_retry_options', 'removal_policy', 'role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogRetention'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.LogStream
class LogStreamDef(BaseConstruct):
    log_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.LogGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to create a log stream for.\n')
    log_stream_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log stream to create. The name must be unique within the log group. Default: Automatically generated\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Determine what happens when the log stream resource is removed from the app. Normally you want to retain the log stream so you can diagnose issues from logs even after a deployment that no longer includes the log stream. The date-based retention policy of your log group will age out the logs after a certain time. Default: RemovalPolicy.Retain')
    _init_params: typing.ClassVar[list[str]] = ['log_group', 'log_stream_name', 'removal_policy']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = ['from_log_stream_name']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogStream'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_log_stream_name']
    ...


    from_log_stream_name: typing.Optional[models.aws_logs.LogStreamDefFromLogStreamNameParams] = pydantic.Field(None, description='Import an existing LogGroup.')
    resource_config: typing.Optional[models.aws_logs.LogStreamDefConfig] = pydantic.Field(None)


class LogStreamDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class LogStreamDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class LogStreamDefFromLogStreamNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    log_stream_name: str = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_logs.MetricFilter
class MetricFilterDef(BaseConstruct):
    log_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.LogGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to create the filter on.\n')
    filter_pattern: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Pattern to search for log events.\n')
    metric_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the metric to emit.\n')
    metric_namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The namespace of the metric to emit.\n')
    default_value: typing.Union[int, float, None] = pydantic.Field(None, description='The value to emit if the pattern does not match a particular event. Default: No metric emitted.\n')
    dimensions: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The fields to use as dimensions for the metric. One metric filter can include as many as three dimensions. Default: - No dimensions attached to metrics.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the metric filter. Default: - Cloudformation generated name.\n')
    metric_value: typing.Optional[str] = pydantic.Field(None, description='The value to emit for the metric. Can either be a literal number (typically "1"), or the name of a field in the structure to take the value from the matched event. If you are using a field value, the field value must have been matched using the pattern. If you want to specify a field from a matched JSON structure, use \'$.fieldName\', and make sure the field is in the pattern (if only as \'$.fieldName = *\'). If you want to specify a field from a matched space-delimited structure, use \'$fieldName\'. Default: "1"\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='The unit to assign to the metric. Default: - No unit attached to metrics.')
    _init_params: typing.ClassVar[list[str]] = ['log_group', 'filter_pattern', 'metric_name', 'metric_namespace', 'default_value', 'dimensions', 'filter_name', 'metric_value', 'unit']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy', 'metric']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.MetricFilter'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.MetricFilterDefConfig] = pydantic.Field(None)


class MetricFilterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    metric: typing.Optional[list[models.aws_logs.MetricFilterDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Metric Filter.')

class MetricFilterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class MetricFilterDefMetricParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: avg over 5 minutes\n')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_logs.QueryDefinition
class QueryDefinitionDef(BaseConstruct):
    query_definition_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name of the query definition.\n')
    query_string: typing.Union[models.aws_logs.QueryStringDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The query string to use for this query definition.\n')
    log_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_logs.LogGroupDef]]] = pydantic.Field(None, description='Specify certain log groups for the query definition. Default: - no specified log groups')
    _init_params: typing.ClassVar[list[str]] = ['query_definition_name', 'query_string', 'log_groups']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.QueryDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.QueryDefinitionDefConfig] = pydantic.Field(None)


class QueryDefinitionDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class QueryDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_logs.ResourcePolicy
class ResourcePolicyDef(BaseConstruct):
    policy_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial statements to add to the resource policy. Default: - No statements\n')
    resource_policy_name: typing.Optional[str] = pydantic.Field(None, description='Name of the log group resource policy. Default: - Uses a unique id based on the construct path')
    _init_params: typing.ClassVar[list[str]] = ['policy_statements', 'resource_policy_name']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.ResourcePolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.ResourcePolicyDefConfig] = pydantic.Field(None)


class ResourcePolicyDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    document_config: typing.Optional[models.aws_iam.PolicyDocumentDefConfig] = pydantic.Field(None)

class ResourcePolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_logs.SubscriptionFilter
class SubscriptionFilterDef(BaseConstruct):
    log_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.LogGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to create the subscription on.\n')
    destination: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.CrossAccountDestinationDef, models.aws_logs_destinations.KinesisDestinationDef, models.aws_logs_destinations.LambdaDestinationDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The destination to send the filtered events to. For example, a Kinesis stream or a Lambda function.\n')
    filter_pattern: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Log events matching this pattern will be sent to the destination.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription filter. Default: Automatically generated')
    _init_params: typing.ClassVar[list[str]] = ['log_group', 'destination', 'filter_pattern', 'filter_name']
    _method_names: typing.ClassVar[list[str]] = ['apply_removal_policy']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.SubscriptionFilter'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.SubscriptionFilterDefConfig] = pydantic.Field(None)


class SubscriptionFilterDefConfig(pydantic.BaseModel):
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)

class SubscriptionFilterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnMetricFilter.DimensionProperty
class CfnMetricFilter_DimensionPropertyDef(BaseStruct):
    key: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name for the CloudWatch metric dimension that the metric filter creates. Dimension names must contain only ASCII characters, must include at least one non-whitespace character, and cannot start with a colon (:).\n')
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log event field that will contain the value for this dimension. This dimension will only be published for a metric if the value is found in the log event. For example, ``$.eventType`` for JSON log events, or ``$server`` for space-delimited log events.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-logs-metricfilter-dimension.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    dimension_property = logs.CfnMetricFilter.DimensionProperty(\n        key="key",\n        value="value"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['key', 'value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnMetricFilter.DimensionProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnMetricFilter.MetricTransformationProperty
class CfnMetricFilter_MetricTransformationPropertyDef(BaseStruct):
    metric_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the CloudWatch metric.\n')
    metric_namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A custom namespace to contain your metric in CloudWatch. Use namespaces to group together metrics that are similar. For more information, see `Namespaces <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Namespace>`_ .\n')
    metric_value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The value that is published to the CloudWatch metric. For example, if you're counting the occurrences of a particular term like ``Error`` , specify 1 for the metric value. If you're counting the number of bytes transferred, reference the value that is in the log event by using $. followed by the name of the field that you specified in the filter pattern, such as ``$.size`` .\n")
    default_value: typing.Union[int, float, None] = pydantic.Field(None, description='(Optional) The value to emit when a filter pattern does not match a log event. This value can be null.\n')
    dimensions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_logs.CfnMetricFilter_DimensionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The fields to use as dimensions for the metric. One metric filter can include as many as three dimensions. .. epigraph:: Metrics extracted from log events are charged as custom metrics. To prevent unexpected high charges, do not specify high-cardinality fields such as ``IPAddress`` or ``requestID`` as dimensions. Each different value found for a dimension is treated as a separate metric and accrues charges as a separate custom metric. CloudWatch Logs disables a metric filter if it generates 1000 different name/value pairs for your specified dimensions within a certain amount of time. This helps to prevent accidental high charges. You can also set up a billing alarm to alert you if your charges are higher than expected. For more information, see `Creating a Billing Alarm to Monitor Your Estimated AWS Charges <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/monitor_estimated_charges_with_cloudwatch.html>`_ .\n')
    unit: typing.Optional[str] = pydantic.Field(None, description='The unit to assign to the metric. If you omit this, the unit is set as ``None`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-logs-metricfilter-metrictransformation.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    metric_transformation_property = logs.CfnMetricFilter.MetricTransformationProperty(\n        metric_name="metricName",\n        metric_namespace="metricNamespace",\n        metric_value="metricValue",\n\n        # the properties below are optional\n        default_value=123,\n        dimensions=[logs.CfnMetricFilter.DimensionProperty(\n            key="key",\n            value="value"\n        )],\n        unit="unit"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['metric_name', 'metric_namespace', 'metric_value', 'default_value', 'dimensions', 'unit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnMetricFilter.MetricTransformationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.ColumnRestriction
class ColumnRestrictionDef(BaseStruct):
    comparison: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Comparison operator to use.')
    number_value: typing.Union[int, float, None] = pydantic.Field(None, description="Number value to compare to. Exactly one of 'stringValue' and 'numberValue' must be set.\n")
    string_value: typing.Optional[str] = pydantic.Field(None, description='String value to compare to. Exactly one of \'stringValue\' and \'numberValue\' must be set.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    column_restriction = logs.ColumnRestriction(\n        comparison="comparison",\n\n        # the properties below are optional\n        number_value=123,\n        string_value="stringValue"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['comparison', 'number_value', 'string_value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.ColumnRestriction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CrossAccountDestinationProps
class CrossAccountDestinationPropsDef(BaseStruct):
    role: typing.Union[_REQUIRED_INIT_PARAM, models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef] = pydantic.Field(REQUIRED_INIT_PARAM, description="The role to assume that grants permissions to write to 'target'. The role must be assumable by 'logs.{REGION}.amazonaws.com'.\n")
    target_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The log destination target's ARN.\n")
    destination_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log destination. Default: Automatically generated\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_logs as logs\n\n    # role: iam.Role\n\n    cross_account_destination_props = logs.CrossAccountDestinationProps(\n        role=role,\n        target_arn="targetArn",\n\n        # the properties below are optional\n        destination_name="destinationName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['role', 'target_arn', 'destination_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CrossAccountDestinationProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CrossAccountDestinationPropsDefConfig] = pydantic.Field(None)


class CrossAccountDestinationPropsDefConfig(pydantic.BaseModel):
    role_config: typing.Optional[models._interface_methods.AwsIamIRoleDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_logs.DataProtectionPolicyConfig
class DataProtectionPolicyConfigDef(BaseStruct):
    description: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Description of the data protection policy. Default: - 'cdk generated data protection policy'\n")
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="Name of the data protection policy. Default: - 'data-protection-policy-cdk'\n")
    statement: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Statements within the data protection policy. Must contain one Audit and one Redact statement\n')
    version: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Version of the data protection policy.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    # statement: Any\n\n    data_protection_policy_config = logs.DataProtectionPolicyConfig(\n        description="description",\n        name="name",\n        statement=statement,\n        version="version"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['description', 'name', 'statement', 'version']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.DataProtectionPolicyConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.DataProtectionPolicyProps
class DataProtectionPolicyPropsDef(BaseStruct):
    identifiers: typing.Union[typing.Sequence[models.aws_logs.DataIdentifierDef], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='List of data protection identifiers. Must be in the following list: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/protect-sensitive-log-data-types.html\n')
    delivery_stream_name_audit_destination: typing.Optional[str] = pydantic.Field(None, description='Amazon Kinesis Data Firehose delivery stream to send audit findings to. The delivery stream must already exist. Default: - no firehose delivery stream audit destination\n')
    description: typing.Optional[str] = pydantic.Field(None, description="Description of the data protection policy. Default: - 'cdk generated data protection policy'\n")
    log_group_audit_destination: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='CloudWatch Logs log group to send audit findings to. The log group must already exist prior to creating the data protection policy. Default: - no CloudWatch Logs audit destination\n')
    name: typing.Optional[str] = pydantic.Field(None, description="Name of the data protection policy. Default: - 'data-protection-policy-cdk'\n")
    s3_bucket_audit_destination: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='S3 bucket to send audit findings to. The bucket must already exist. Default: - no S3 bucket audit destination\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_kinesisfirehose_alpha as kinesisfirehose\n    import aws_cdk.aws_kinesisfirehose_destinations_alpha as destinations\n\n\n    log_group_destination = logs.LogGroup(self, "LogGroupLambdaAudit",\n        log_group_name="auditDestinationForCDK"\n    )\n\n    bucket = s3.Bucket(self, "audit-bucket")\n    s3_destination = destinations.S3Bucket(bucket)\n\n    delivery_stream = kinesisfirehose.DeliveryStream(self, "Delivery Stream",\n        destinations=[s3_destination]\n    )\n\n    data_protection_policy = logs.DataProtectionPolicy(\n        name="data protection policy",\n        description="policy description",\n        identifiers=[logs.DataIdentifier.DRIVERSLICENSE_US, logs.DataIdentifier("EmailAddress")],\n        log_group_audit_destination=log_group_destination,\n        s3_bucket_audit_destination=bucket,\n        delivery_stream_name_audit_destination=delivery_stream.delivery_stream_name\n    )\n\n    logs.LogGroup(self, "LogGroupLambda",\n        log_group_name="cdkIntegLogGroup",\n        data_protection_policy=data_protection_policy\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['identifiers', 'delivery_stream_name_audit_destination', 'description', 'log_group_audit_destination', 'name', 's3_bucket_audit_destination']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.DataProtectionPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.LogGroupProps
class LogGroupPropsDef(BaseStruct):
    data_protection_policy: typing.Optional[models.aws_logs.DataProtectionPolicyDef] = pydantic.Field(None, description='Data Protection Policy for this log group. Default: - no data protection policy\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The KMS customer managed key to encrypt the log group with. Default: Server-side encrpytion managed by the CloudWatch Logs service\n')
    log_group_name: typing.Optional[str] = pydantic.Field(None, description='Name of the log group. Default: Automatically generated\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Determine the removal policy of this log group. Normally you want to retain the log group so you can diagnose issues from logs even after a deployment that no longer includes the log group. In that case, use the normal date-based retention policy to age out your logs. Default: RemovalPolicy.Retain\n')
    retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='How long, in days, the log contents will be retained. To retain all logs, set this value to RetentionDays.INFINITE. Default: RetentionDays.TWO_YEARS\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_kinesisfirehose_alpha as kinesisfirehose\n    import aws_cdk.aws_kinesisfirehose_destinations_alpha as destinations\n\n\n    log_group_destination = logs.LogGroup(self, "LogGroupLambdaAudit",\n        log_group_name="auditDestinationForCDK"\n    )\n\n    bucket = s3.Bucket(self, "audit-bucket")\n    s3_destination = destinations.S3Bucket(bucket)\n\n    delivery_stream = kinesisfirehose.DeliveryStream(self, "Delivery Stream",\n        destinations=[s3_destination]\n    )\n\n    data_protection_policy = logs.DataProtectionPolicy(\n        name="data protection policy",\n        description="policy description",\n        identifiers=[logs.DataIdentifier.DRIVERSLICENSE_US, logs.DataIdentifier("EmailAddress")],\n        log_group_audit_destination=log_group_destination,\n        s3_bucket_audit_destination=bucket,\n        delivery_stream_name_audit_destination=delivery_stream.delivery_stream_name\n    )\n\n    logs.LogGroup(self, "LogGroupLambda",\n        log_group_name="cdkIntegLogGroup",\n        data_protection_policy=data_protection_policy\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_protection_policy', 'encryption_key', 'log_group_name', 'removal_policy', 'retention']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.LogRetentionProps
class LogRetentionPropsDef(BaseStruct):
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group name.\n')
    retention: typing.Union[aws_cdk.aws_logs.RetentionDays, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The number of days log events are kept in CloudWatch Logs.\n')
    log_group_region: typing.Optional[str] = pydantic.Field(None, description='The region where the log group should be created. Default: - same region as the stack\n')
    log_retention_retry_options: typing.Union[models.aws_logs.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Retry options for all AWS API calls. Default: - AWS SDK default retry options\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='The removalPolicy for the log group when the stack is deleted. Default: RemovalPolicy.RETAIN\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource. Default: - A new role is created\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_logs as logs\n\n    # role: iam.Role\n\n    log_retention_props = logs.LogRetentionProps(\n        log_group_name="logGroupName",\n        retention=logs.RetentionDays.ONE_DAY,\n\n        # the properties below are optional\n        log_group_region="logGroupRegion",\n        log_retention_retry_options=logs.LogRetentionRetryOptions(\n            base=cdk.Duration.minutes(30),\n            max_retries=123\n        ),\n        removal_policy=cdk.RemovalPolicy.DESTROY,\n        role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_group_name', 'retention', 'log_group_region', 'log_retention_retry_options', 'removal_policy', 'role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogRetentionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.LogRetentionRetryOptions
class LogRetentionRetryOptionsDef(BaseStruct):
    base: typing.Optional[models.DurationDef] = pydantic.Field(None, description='(deprecated) The base duration to use in the exponential backoff for operation retries. Default: - none, not used anymore\n')
    max_retries: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum amount of retries. Default: 5\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_logs as logs\n\n    log_retention_retry_options = logs.LogRetentionRetryOptions(\n        base=cdk.Duration.minutes(30),\n        max_retries=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['base', 'max_retries']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogRetentionRetryOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.LogStreamProps
class LogStreamPropsDef(BaseStruct):
    log_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.LogGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to create a log stream for.\n')
    log_stream_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log stream to create. The name must be unique within the log group. Default: Automatically generated\n')
    removal_policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='Determine what happens when the log stream resource is removed from the app. Normally you want to retain the log stream so you can diagnose issues from logs even after a deployment that no longer includes the log stream. The date-based retention policy of your log group will age out the logs after a certain time. Default: RemovalPolicy.Retain\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    import aws_cdk as cdk\n    from aws_cdk import aws_logs as logs\n\n    # log_group: logs.LogGroup\n\n    log_stream_props = logs.LogStreamProps(\n        log_group=log_group,\n\n        # the properties below are optional\n        log_stream_name="logStreamName",\n        removal_policy=cdk.RemovalPolicy.DESTROY\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_group', 'log_stream_name', 'removal_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogStreamProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.LogStreamPropsDefConfig] = pydantic.Field(None)


class LogStreamPropsDefConfig(pydantic.BaseModel):
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_logs.LogSubscriptionDestinationConfig
class LogSubscriptionDestinationConfigDef(BaseStruct):
    arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="The ARN of the subscription's destination.\n")
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The role to assume to write log events to the destination. Default: No role assumed\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_logs as logs\n\n    # role: iam.Role\n\n    log_subscription_destination_config = logs.LogSubscriptionDestinationConfig(\n        arn="arn",\n\n        # the properties below are optional\n        role=role\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['arn', 'role']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.LogSubscriptionDestinationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.MetricFilterOptions
class MetricFilterOptionsDef(BaseStruct):
    filter_pattern: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Pattern to search for log events.\n')
    metric_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the metric to emit.\n')
    metric_namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The namespace of the metric to emit.\n')
    default_value: typing.Union[int, float, None] = pydantic.Field(None, description='The value to emit if the pattern does not match a particular event. Default: No metric emitted.\n')
    dimensions: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The fields to use as dimensions for the metric. One metric filter can include as many as three dimensions. Default: - No dimensions attached to metrics.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the metric filter. Default: - Cloudformation generated name.\n')
    metric_value: typing.Optional[str] = pydantic.Field(None, description='The value to emit for the metric. Can either be a literal number (typically "1"), or the name of a field in the structure to take the value from the matched event. If you are using a field value, the field value must have been matched using the pattern. If you want to specify a field from a matched JSON structure, use \'$.fieldName\', and make sure the field is in the pattern (if only as \'$.fieldName = *\'). If you want to specify a field from a matched space-delimited structure, use \'$fieldName\'. Default: "1"\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='The unit to assign to the metric. Default: - No unit attached to metrics.\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudwatch as cloudwatch\n    from aws_cdk import aws_logs as logs\n\n    # filter_pattern: logs.IFilterPattern\n\n    metric_filter_options = logs.MetricFilterOptions(\n        filter_pattern=filter_pattern,\n        metric_name="metricName",\n        metric_namespace="metricNamespace",\n\n        # the properties below are optional\n        default_value=123,\n        dimensions={\n            "dimensions_key": "dimensions"\n        },\n        filter_name="filterName",\n        metric_value="metricValue",\n        unit=cloudwatch.Unit.SECONDS\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['filter_pattern', 'metric_name', 'metric_namespace', 'default_value', 'dimensions', 'filter_name', 'metric_value', 'unit']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.MetricFilterOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.MetricFilterProps
class MetricFilterPropsDef(BaseStruct):
    filter_pattern: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Pattern to search for log events.\n')
    metric_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the metric to emit.\n')
    metric_namespace: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The namespace of the metric to emit.\n')
    default_value: typing.Union[int, float, None] = pydantic.Field(None, description='The value to emit if the pattern does not match a particular event. Default: No metric emitted.\n')
    dimensions: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The fields to use as dimensions for the metric. One metric filter can include as many as three dimensions. Default: - No dimensions attached to metrics.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the metric filter. Default: - Cloudformation generated name.\n')
    metric_value: typing.Optional[str] = pydantic.Field(None, description='The value to emit for the metric. Can either be a literal number (typically "1"), or the name of a field in the structure to take the value from the matched event. If you are using a field value, the field value must have been matched using the pattern. If you want to specify a field from a matched JSON structure, use \'$.fieldName\', and make sure the field is in the pattern (if only as \'$.fieldName = *\'). If you want to specify a field from a matched space-delimited structure, use \'$fieldName\'. Default: "1"\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='The unit to assign to the metric. Default: - No unit attached to metrics.\n')
    log_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.LogGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to create the filter on.\n\n:exampleMetadata: lit=aws-logs/test/integ.metricfilter.lit.ts infused\n\nExample::\n\n    MetricFilter(self, "MetricFilter",\n        log_group=log_group,\n        metric_namespace="MyApp",\n        metric_name="Latency",\n        filter_pattern=FilterPattern.exists("$.latency"),\n        metric_value="$.latency"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['filter_pattern', 'metric_name', 'metric_namespace', 'default_value', 'dimensions', 'filter_name', 'metric_value', 'unit', 'log_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.MetricFilterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.MetricFilterPropsDefConfig] = pydantic.Field(None)


class MetricFilterPropsDefConfig(pydantic.BaseModel):
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_logs.QueryDefinitionProps
class QueryDefinitionPropsDef(BaseStruct):
    query_definition_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Name of the query definition.\n')
    query_string: typing.Union[models.aws_logs.QueryStringDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The query string to use for this query definition.\n')
    log_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_logs.LogGroupDef]]] = pydantic.Field(None, description='Specify certain log groups for the query definition. Default: - no specified log groups\n\n:exampleMetadata: infused\n\nExample::\n\n    logs.QueryDefinition(self, "QueryDefinition",\n        query_definition_name="MyQuery",\n        query_string=logs.QueryString(\n            fields=["@timestamp", "@message"],\n            parse_statements=["@message "[*] *" as loggingType, loggingMessage", "@message "<*>: *" as differentLoggingType, differentLoggingMessage"\n            ],\n            filter_statements=["loggingType = "ERROR"", "loggingMessage = "A very strange error occurred!""\n            ],\n            sort="@timestamp desc",\n            limit=20\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['query_definition_name', 'query_string', 'log_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.QueryDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.QueryStringProps
class QueryStringPropsDef(BaseStruct):
    display: typing.Optional[str] = pydantic.Field(None, description='Specifies which fields to display in the query results. Default: - no display in QueryString\n')
    fields: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Retrieves the specified fields from log events for display. Default: - no fields in QueryString\n')
    filter: typing.Optional[str] = pydantic.Field(None, description='(deprecated) A single statement for filtering the results of a query based on a boolean expression. Default: - no filter in QueryString\n')
    filter_statements: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of one or more statements for filtering the results of a query based on a boolean expression. Each provided statement generates a separate filter line in the query string. Note: If provided, this property overrides any value provided for the ``filter`` property. Default: - no filter in QueryString\n')
    limit: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the number of log events returned by the query. Default: - no limit in QueryString\n')
    parse: typing.Optional[str] = pydantic.Field(None, description='(deprecated) A single statement for parsing data from a log field and creating ephemeral fields that can be processed further in the query. Default: - no parse in QueryString\n')
    parse_statements: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of one or more statements for parsing data from a log field and creating ephemeral fields that can be processed further in the query. Each provided statement generates a separate parse line in the query string. Note: If provided, this property overrides any value provided for the ``parse`` property. Default: - no parse in QueryString\n')
    sort: typing.Optional[str] = pydantic.Field(None, description='Sorts the retrieved log events. Default: - no sort in QueryString\n')
    stats: typing.Optional[str] = pydantic.Field(None, description='Uses log field values to calculate aggregate statistics. Default: - no stats in QueryString\n\n:exampleMetadata: infused\n\nExample::\n\n    logs.QueryDefinition(self, "QueryDefinition",\n        query_definition_name="MyQuery",\n        query_string=logs.QueryString(\n            fields=["@timestamp", "@message"],\n            parse_statements=["@message "[*] *" as loggingType, loggingMessage", "@message "<*>: *" as differentLoggingType, differentLoggingMessage"\n            ],\n            filter_statements=["loggingType = "ERROR"", "loggingMessage = "A very strange error occurred!""\n            ],\n            sort="@timestamp desc",\n            limit=20\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['display', 'fields', 'filter', 'filter_statements', 'limit', 'parse', 'parse_statements', 'sort', 'stats']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.QueryStringProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.ResourcePolicyProps
class ResourcePolicyPropsDef(BaseStruct):
    policy_statements: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial statements to add to the resource policy. Default: - No statements\n')
    resource_policy_name: typing.Optional[str] = pydantic.Field(None, description='Name of the log group resource policy. Default: - Uses a unique id based on the construct path\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_iam as iam\n    from aws_cdk import aws_logs as logs\n\n    # policy_statement: iam.PolicyStatement\n\n    resource_policy_props = logs.ResourcePolicyProps(\n        policy_statements=[policy_statement],\n        resource_policy_name="resourcePolicyName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['policy_statements', 'resource_policy_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.ResourcePolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.StreamOptions
class StreamOptionsDef(BaseStruct):
    log_stream_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log stream to create. The name must be unique within the log group. Default: Automatically generated\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    stream_options = logs.StreamOptions(\n        log_stream_name="logStreamName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_stream_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.StreamOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.SubscriptionFilterOptions
class SubscriptionFilterOptionsDef(BaseStruct):
    destination: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.CrossAccountDestinationDef, models.aws_logs_destinations.KinesisDestinationDef, models.aws_logs_destinations.LambdaDestinationDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The destination to send the filtered events to. For example, a Kinesis stream or a Lambda function.\n')
    filter_pattern: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Log events matching this pattern will be sent to the destination.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription filter. Default: Automatically generated\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    # filter_pattern: logs.IFilterPattern\n    # log_subscription_destination: logs.ILogSubscriptionDestination\n\n    subscription_filter_options = logs.SubscriptionFilterOptions(\n        destination=log_subscription_destination,\n        filter_pattern=filter_pattern,\n\n        # the properties below are optional\n        filter_name="filterName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination', 'filter_pattern', 'filter_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.SubscriptionFilterOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.SubscriptionFilterOptionsDefConfig] = pydantic.Field(None)


class SubscriptionFilterOptionsDefConfig(pydantic.BaseModel):
    destination_config: typing.Optional[models._interface_methods.AwsLogsILogSubscriptionDestinationDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_logs.SubscriptionFilterProps
class SubscriptionFilterPropsDef(BaseStruct):
    destination: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.CrossAccountDestinationDef, models.aws_logs_destinations.KinesisDestinationDef, models.aws_logs_destinations.LambdaDestinationDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The destination to send the filtered events to. For example, a Kinesis stream or a Lambda function.\n')
    filter_pattern: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.JsonPatternDef, models.aws_logs.SpaceDelimitedTextPatternDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='Log events matching this pattern will be sent to the destination.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription filter. Default: Automatically generated\n')
    log_group: typing.Union[_REQUIRED_INIT_PARAM, models.aws_logs.LogGroupDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to create the subscription on.\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_logs_destinations as destinations\n\n    # fn: lambda.Function\n    # log_group: logs.LogGroup\n\n\n    logs.SubscriptionFilter(self, "Subscription",\n        log_group=log_group,\n        destination=destinations.LambdaDestination(fn),\n        filter_pattern=logs.FilterPattern.all_terms("ERROR", "MainThread"),\n        filter_name="ErrorInMainThread"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination', 'filter_pattern', 'filter_name', 'log_group']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.SubscriptionFilterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.SubscriptionFilterPropsDefConfig] = pydantic.Field(None)


class SubscriptionFilterPropsDefConfig(pydantic.BaseModel):
    destination_config: typing.Optional[models._interface_methods.AwsLogsILogSubscriptionDestinationDefConfig] = pydantic.Field(None)
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_logs.RetentionDays
# skipping emum

#  autogenerated from aws_cdk.aws_logs.IFilterPattern
#  skipping Interface

#  autogenerated from aws_cdk.aws_logs.ILogGroup
#  skipping Interface

#  autogenerated from aws_cdk.aws_logs.ILogStream
#  skipping Interface

#  autogenerated from aws_cdk.aws_logs.ILogSubscriptionDestination
#  skipping Interface

#  autogenerated from aws_cdk.aws_logs.CfnAccountPolicy
class CfnAccountPolicyDef(BaseCfnResource):
    scope_: typing.Union[models.constructs.ConstructDef, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Scope in which this resource is defined.')
    policy_document: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specify the data protection policy, in JSON. This policy must include two JSON blocks: - The first block must include both a ``DataIdentifer`` array and an ``Operation`` property with an ``Audit`` action. The ``DataIdentifer`` array lists the types of sensitive data that you want to mask. For more information about the available options, see `Types of data that you can mask <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/mask-sensitive-log-data-types.html>`_ . The ``Operation`` property with an ``Audit`` action is required to find the sensitive data terms. This ``Audit`` action must contain a ``FindingsDestination`` object. You can optionally use that ``FindingsDestination`` object to list one or more destinations to send audit findings to. If you specify destinations such as log groups, Kinesis Data Firehose streams, and S3 buckets, they must already exist. - The second block must include both a ``DataIdentifer`` array and an ``Operation`` property with an ``Deidentify`` action. The ``DataIdentifer`` array must exactly match the ``DataIdentifer`` array in the first block of the policy. The ``Operation`` property with the ``Deidentify`` action is what actually masks the data, and it must contain the ``"MaskConfig": {}`` object. The ``"MaskConfig": {}`` object must be empty. .. epigraph:: The contents of the two ``DataIdentifer`` arrays must match exactly.\n')
    policy_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A name for the policy. This must be unique within the account.\n')
    policy_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Currently the only valid value for this parameter is ``DATA_PROTECTION_POLICY`` .\n')
    _init_params: typing.ClassVar[list[str]] = ['scope_', 'policy_document', 'policy_name', 'policy_type']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnAccountPolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnAccountPolicyDefConfig] = pydantic.Field(None)


class CfnAccountPolicyDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnAccountPolicyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnAccountPolicyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnAccountPolicyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnAccountPolicyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnAccountPolicyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnAccountPolicyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnAccountPolicyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnAccountPolicyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnAccountPolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnAccountPolicyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnAccountPolicyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnAccountPolicyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnAccountPolicyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnAccountPolicyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnAccountPolicyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnDestination
class CfnDestinationDef(BaseCfnResource):
    destination_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the destination.\n')
    role_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of an IAM role that permits CloudWatch Logs to send data to the specified AWS resource.\n')
    target_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the physical target where the log events are delivered (for example, a Kinesis stream).\n')
    destination_policy: typing.Optional[str] = pydantic.Field(None, description='An IAM policy document that governs which AWS accounts can create subscription filters against this destination.')
    _init_params: typing.ClassVar[list[str]] = ['destination_name', 'role_arn', 'target_arn', 'destination_policy']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnDestination'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnDestinationDefConfig] = pydantic.Field(None)


class CfnDestinationDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnDestinationDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnDestinationDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnDestinationDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnDestinationDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnDestinationDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnDestinationDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnDestinationDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnDestinationDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnDestinationDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnDestinationDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnDestinationDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnDestinationDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnDestinationDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnDestinationDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDestinationDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDestinationDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDestinationDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDestinationDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDestinationDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDestinationDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDestinationDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDestinationDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDestinationDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDestinationDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnDestinationDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDestinationDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDestinationDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnLogGroup
class CfnLogGroupDef(BaseCfnResource):
    data_protection_policy: typing.Any = pydantic.Field(None, description="Creates a data protection policy and assigns it to the log group. A data protection policy can help safeguard sensitive data that's ingested by the log group by auditing and masking the sensitive log data. When a user who does not have permission to view masked data views a log event that includes masked data, the sensitive data is replaced by asterisks. For more information, including a list of types of data that can be audited and masked, see `Protect sensitive log data with masking <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/mask-sensitive-log-data.html>`_ .\n")
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the AWS KMS key to use when encrypting log data. To associate an AWS KMS key with the log group, specify the ARN of that KMS key here. If you do so, ingested data is encrypted using this key. This association is stored as long as the data encrypted with the KMS key is still within CloudWatch Logs . This enables CloudWatch Logs to decrypt this data whenever it is requested. If you attempt to associate a KMS key with the log group but the KMS key doesn't exist or is deactivated, you will receive an ``InvalidParameterException`` error. Log group data is always encrypted in CloudWatch Logs . If you omit this key, the encryption does not use AWS KMS . For more information, see `Encrypt log data in CloudWatch Logs using AWS Key Management Service <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html>`_\n")
    log_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the log group. If you don't specify a name, AWS CloudFormation generates a unique ID for the log group.\n")
    retention_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days to retain the log events in the specified log group. Possible values are: 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1096, 1827, 2192, 2557, 2922, 3288, and 3653. To set a log group so that its log events do not expire, use `DeleteRetentionPolicy <https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_DeleteRetentionPolicy.html>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to the log group. For more information, see `Tag <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-resource-tags.html>`_ .')
    _init_params: typing.ClassVar[list[str]] = ['data_protection_policy', 'kms_key_id', 'log_group_name', 'retention_in_days', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnLogGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnLogGroupDefConfig] = pydantic.Field(None)


class CfnLogGroupDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnLogGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnLogGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnLogGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnLogGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnLogGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnLogGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnLogGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnLogGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnLogGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnLogGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnLogGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnLogGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnLogGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnLogGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnLogGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLogGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnLogGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLogGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnLogGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnLogGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnLogGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnLogGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnLogGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLogGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnLogGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnLogGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLogGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnLogStream
class CfnLogStreamDef(BaseCfnResource):
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the log group where the log stream is created.\n')
    log_stream_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log stream. The name must be unique within the log group.')
    _init_params: typing.ClassVar[list[str]] = ['log_group_name', 'log_stream_name']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnLogStream'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnLogStreamDefConfig] = pydantic.Field(None)


class CfnLogStreamDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnLogStreamDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnLogStreamDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnLogStreamDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnLogStreamDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnLogStreamDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnLogStreamDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnLogStreamDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnLogStreamDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnLogStreamDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnLogStreamDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnLogStreamDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnLogStreamDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnLogStreamDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnLogStreamDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnLogStreamDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLogStreamDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnLogStreamDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLogStreamDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnLogStreamDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnLogStreamDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnLogStreamDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnLogStreamDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnLogStreamDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnLogStreamDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnLogStreamDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnLogStreamDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnLogStreamDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnMetricFilter
class CfnMetricFilterDef(BaseCfnResource):
    filter_pattern: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A filter pattern for extracting metric data out of ingested log events. For more information, see `Filter and Pattern Syntax <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html>`_ .\n')
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of an existing log group that you want to associate with this metric filter.\n')
    metric_transformations: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_logs.CfnMetricFilter_MetricTransformationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The metric transformations.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the metric filter.')
    _init_params: typing.ClassVar[list[str]] = ['filter_pattern', 'log_group_name', 'metric_transformations', 'filter_name']
    _method_names: typing.ClassVar[list[str]] = ['DimensionProperty', 'MetricTransformationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnMetricFilter'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnMetricFilterDefConfig] = pydantic.Field(None)


class CfnMetricFilterDefConfig(pydantic.BaseModel):
    DimensionProperty: typing.Optional[list[models.aws_logs.CfnMetricFilterDefDimensionpropertyParams]] = pydantic.Field(None, description='')
    MetricTransformationProperty: typing.Optional[list[models.aws_logs.CfnMetricFilterDefMetrictransformationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnMetricFilterDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnMetricFilterDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnMetricFilterDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnMetricFilterDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnMetricFilterDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnMetricFilterDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnMetricFilterDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnMetricFilterDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnMetricFilterDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnMetricFilterDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnMetricFilterDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnMetricFilterDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnMetricFilterDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnMetricFilterDefDimensionpropertyParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='')
    value: str = pydantic.Field(..., description='')
    ...

class CfnMetricFilterDefMetrictransformationpropertyParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='')
    metric_namespace: str = pydantic.Field(..., description='')
    metric_value: str = pydantic.Field(..., description='')
    default_value: typing.Union[int, float, None] = pydantic.Field(None, description='')
    dimensions: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_logs.CfnMetricFilter_DimensionPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    unit: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnMetricFilterDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnMetricFilterDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMetricFilterDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnMetricFilterDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMetricFilterDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnMetricFilterDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnMetricFilterDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnMetricFilterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnMetricFilterDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnMetricFilterDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMetricFilterDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnMetricFilterDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnMetricFilterDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMetricFilterDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnQueryDefinition
class CfnQueryDefinitionDef(BaseCfnResource):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A name for the query definition.\n')
    query_string: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The query string to use for this query definition. For more information, see `CloudWatch Logs Insights Query Syntax <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html>`_ .\n')
    log_group_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Use this parameter if you want the query to query only certain log groups.')
    _init_params: typing.ClassVar[list[str]] = ['name', 'query_string', 'log_group_names']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnQueryDefinition'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnQueryDefinitionDefConfig] = pydantic.Field(None)


class CfnQueryDefinitionDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnQueryDefinitionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnQueryDefinitionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnQueryDefinitionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnQueryDefinitionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnQueryDefinitionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnQueryDefinitionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnQueryDefinitionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnQueryDefinitionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnQueryDefinitionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnQueryDefinitionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnQueryDefinitionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnQueryDefinitionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnQueryDefinitionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnQueryDefinitionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnQueryDefinitionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnResourcePolicy
class CfnResourcePolicyDef(BaseCfnResource):
    policy_document: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The details of the policy. It must be formatted in JSON, and you must use backslashes to escape characters that need to be escaped in JSON strings, such as double quote marks.\n')
    policy_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the resource policy.')
    _init_params: typing.ClassVar[list[str]] = ['policy_document', 'policy_name']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnResourcePolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnResourcePolicyDefConfig] = pydantic.Field(None)


class CfnResourcePolicyDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnResourcePolicyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnResourcePolicyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnResourcePolicyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnResourcePolicyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnResourcePolicyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnResourcePolicyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnResourcePolicyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnResourcePolicyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnResourcePolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnResourcePolicyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnResourcePolicyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnResourcePolicyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnResourcePolicyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnResourcePolicyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnResourcePolicyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnSubscriptionFilter
class CfnSubscriptionFilterDef(BaseCfnResource):
    destination_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the destination.\n')
    filter_pattern: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The filtering expressions that restrict what gets delivered to the destination AWS resource. For more information about the filter pattern syntax, see `Filter and Pattern Syntax <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html>`_ .\n')
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to associate with the subscription filter. All log events that are uploaded to this log group are filtered and delivered to the specified AWS resource if the filter pattern matches the log events.\n')
    distribution: typing.Optional[str] = pydantic.Field(None, description='The method used to distribute log data to the destination, which can be either random or grouped by log stream.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription filter.\n')
    role_arn: typing.Optional[str] = pydantic.Field(None, description="The ARN of an IAM role that grants CloudWatch Logs permissions to deliver ingested log events to the destination stream. You don't need to provide the ARN when you are working with a logical destination for cross-account delivery.")
    _init_params: typing.ClassVar[list[str]] = ['destination_arn', 'filter_pattern', 'log_group_name', 'distribution', 'filter_name', 'role_arn']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnSubscriptionFilter'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_logs.CfnSubscriptionFilterDefConfig] = pydantic.Field(None)


class CfnSubscriptionFilterDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_logs.CfnSubscriptionFilterDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnSubscriptionFilterDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnSubscriptionFilterDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSubscriptionFilterDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnSubscriptionFilterDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSubscriptionFilterDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnSubscriptionFilterDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnSubscriptionFilterDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnSubscriptionFilterDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnSubscriptionFilterDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnSubscriptionFilterDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnSubscriptionFilterDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnSubscriptionFilterDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnSubscriptionFilterDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnSubscriptionFilterDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_logs.CfnAccountPolicyProps
class CfnAccountPolicyPropsDef(BaseCfnProperty):
    policy_document: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specify the data protection policy, in JSON. This policy must include two JSON blocks: - The first block must include both a ``DataIdentifer`` array and an ``Operation`` property with an ``Audit`` action. The ``DataIdentifer`` array lists the types of sensitive data that you want to mask. For more information about the available options, see `Types of data that you can mask <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/mask-sensitive-log-data-types.html>`_ . The ``Operation`` property with an ``Audit`` action is required to find the sensitive data terms. This ``Audit`` action must contain a ``FindingsDestination`` object. You can optionally use that ``FindingsDestination`` object to list one or more destinations to send audit findings to. If you specify destinations such as log groups, Kinesis Data Firehose streams, and S3 buckets, they must already exist. - The second block must include both a ``DataIdentifer`` array and an ``Operation`` property with an ``Deidentify`` action. The ``DataIdentifer`` array must exactly match the ``DataIdentifer`` array in the first block of the policy. The ``Operation`` property with the ``Deidentify`` action is what actually masks the data, and it must contain the ``"MaskConfig": {}`` object. The ``"MaskConfig": {}`` object must be empty. .. epigraph:: The contents of the two ``DataIdentifer`` arrays must match exactly.\n')
    policy_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A name for the policy. This must be unique within the account.\n')
    policy_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Currently the only valid value for this parameter is ``DATA_PROTECTION_POLICY`` .\n')
    _init_params: typing.ClassVar[list[str]] = ['policy_document', 'policy_name', 'policy_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnAccountPolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnDestinationProps
class CfnDestinationPropsDef(BaseCfnProperty):
    destination_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the destination.\n')
    role_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The ARN of an IAM role that permits CloudWatch Logs to send data to the specified AWS resource.\n')
    target_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the physical target where the log events are delivered (for example, a Kinesis stream).\n')
    destination_policy: typing.Optional[str] = pydantic.Field(None, description='An IAM policy document that governs which AWS accounts can create subscription filters against this destination.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-destination.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    cfn_destination_props = logs.CfnDestinationProps(\n        destination_name="destinationName",\n        role_arn="roleArn",\n        target_arn="targetArn",\n\n        # the properties below are optional\n        destination_policy="destinationPolicy"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination_name', 'role_arn', 'target_arn', 'destination_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnDestinationProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnLogGroupProps
class CfnLogGroupPropsDef(BaseCfnProperty):
    data_protection_policy: typing.Any = pydantic.Field(None, description="Creates a data protection policy and assigns it to the log group. A data protection policy can help safeguard sensitive data that's ingested by the log group by auditing and masking the sensitive log data. When a user who does not have permission to view masked data views a log event that includes masked data, the sensitive data is replaced by asterisks. For more information, including a list of types of data that can be audited and masked, see `Protect sensitive log data with masking <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/mask-sensitive-log-data.html>`_ .\n")
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the AWS KMS key to use when encrypting log data. To associate an AWS KMS key with the log group, specify the ARN of that KMS key here. If you do so, ingested data is encrypted using this key. This association is stored as long as the data encrypted with the KMS key is still within CloudWatch Logs . This enables CloudWatch Logs to decrypt this data whenever it is requested. If you attempt to associate a KMS key with the log group but the KMS key doesn't exist or is deactivated, you will receive an ``InvalidParameterException`` error. Log group data is always encrypted in CloudWatch Logs . If you omit this key, the encryption does not use AWS KMS . For more information, see `Encrypt log data in CloudWatch Logs using AWS Key Management Service <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/encrypt-log-data-kms.html>`_\n")
    log_group_name: typing.Optional[str] = pydantic.Field(None, description="The name of the log group. If you don't specify a name, AWS CloudFormation generates a unique ID for the log group.\n")
    retention_in_days: typing.Union[int, float, None] = pydantic.Field(None, description='The number of days to retain the log events in the specified log group. Possible values are: 1, 3, 5, 7, 14, 30, 60, 90, 120, 150, 180, 365, 400, 545, 731, 1096, 1827, 2192, 2557, 2922, 3288, and 3653. To set a log group so that its log events do not expire, use `DeleteRetentionPolicy <https://docs.aws.amazon.com/AmazonCloudWatchLogs/latest/APIReference/API_DeleteRetentionPolicy.html>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to the log group. For more information, see `Tag <https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-resource-tags.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-loggroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    # data_protection_policy: Any\n\n    cfn_log_group_props = logs.CfnLogGroupProps(\n        data_protection_policy=data_protection_policy,\n        kms_key_id="kmsKeyId",\n        log_group_name="logGroupName",\n        retention_in_days=123,\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_protection_policy', 'kms_key_id', 'log_group_name', 'retention_in_days', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnLogGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnLogStreamProps
class CfnLogStreamPropsDef(BaseCfnProperty):
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the log group where the log stream is created.\n')
    log_stream_name: typing.Optional[str] = pydantic.Field(None, description='The name of the log stream. The name must be unique within the log group.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-logstream.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    cfn_log_stream_props = logs.CfnLogStreamProps(\n        log_group_name="logGroupName",\n\n        # the properties below are optional\n        log_stream_name="logStreamName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['log_group_name', 'log_stream_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnLogStreamProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnMetricFilterProps
class CfnMetricFilterPropsDef(BaseCfnProperty):
    filter_pattern: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A filter pattern for extracting metric data out of ingested log events. For more information, see `Filter and Pattern Syntax <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html>`_ .\n')
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of an existing log group that you want to associate with this metric filter.\n')
    metric_transformations: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_logs.CfnMetricFilter_MetricTransformationPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='The metric transformations.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the metric filter.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-metricfilter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    cfn_metric_filter_props = logs.CfnMetricFilterProps(\n        filter_pattern="filterPattern",\n        log_group_name="logGroupName",\n        metric_transformations=[logs.CfnMetricFilter.MetricTransformationProperty(\n            metric_name="metricName",\n            metric_namespace="metricNamespace",\n            metric_value="metricValue",\n\n            # the properties below are optional\n            default_value=123,\n            dimensions=[logs.CfnMetricFilter.DimensionProperty(\n                key="key",\n                value="value"\n            )],\n            unit="unit"\n        )],\n\n        # the properties below are optional\n        filter_name="filterName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['filter_pattern', 'log_group_name', 'metric_transformations', 'filter_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnMetricFilterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnQueryDefinitionProps
class CfnQueryDefinitionPropsDef(BaseCfnProperty):
    name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A name for the query definition.\n')
    query_string: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The query string to use for this query definition. For more information, see `CloudWatch Logs Insights Query Syntax <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_QuerySyntax.html>`_ .\n')
    log_group_names: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Use this parameter if you want the query to query only certain log groups.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-querydefinition.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    cfn_query_definition_props = logs.CfnQueryDefinitionProps(\n        name="name",\n        query_string="queryString",\n\n        # the properties below are optional\n        log_group_names=["logGroupNames"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['name', 'query_string', 'log_group_names']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnQueryDefinitionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnResourcePolicyProps
class CfnResourcePolicyPropsDef(BaseCfnProperty):
    policy_document: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The details of the policy. It must be formatted in JSON, and you must use backslashes to escape characters that need to be escaped in JSON strings, such as double quote marks.\n')
    policy_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The name of the resource policy.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-resourcepolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    cfn_resource_policy_props = logs.CfnResourcePolicyProps(\n        policy_document="policyDocument",\n        policy_name="policyName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['policy_document', 'policy_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnResourcePolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_logs.CfnSubscriptionFilterProps
class CfnSubscriptionFilterPropsDef(BaseCfnProperty):
    destination_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the destination.\n')
    filter_pattern: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The filtering expressions that restrict what gets delivered to the destination AWS resource. For more information about the filter pattern syntax, see `Filter and Pattern Syntax <https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/FilterAndPatternSyntax.html>`_ .\n')
    log_group_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The log group to associate with the subscription filter. All log events that are uploaded to this log group are filtered and delivered to the specified AWS resource if the filter pattern matches the log events.\n')
    distribution: typing.Optional[str] = pydantic.Field(None, description='The method used to distribute log data to the destination, which can be either random or grouped by log stream.\n')
    filter_name: typing.Optional[str] = pydantic.Field(None, description='The name of the subscription filter.\n')
    role_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of an IAM role that grants CloudWatch Logs permissions to deliver ingested log events to the destination stream. You don\'t need to provide the ARN when you are working with a logical destination for cross-account delivery.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-logs-subscriptionfilter.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_logs as logs\n\n    cfn_subscription_filter_props = logs.CfnSubscriptionFilterProps(\n        destination_arn="destinationArn",\n        filter_pattern="filterPattern",\n        log_group_name="logGroupName",\n\n        # the properties below are optional\n        distribution="distribution",\n        filter_name="filterName",\n        role_arn="roleArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destination_arn', 'filter_pattern', 'log_group_name', 'distribution', 'filter_name', 'role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_logs.CfnSubscriptionFilterProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    DataIdentifier: typing.Optional[dict[str, models.aws_logs.DataIdentifierDef]] = pydantic.Field(None)
    DataProtectionPolicy: typing.Optional[dict[str, models.aws_logs.DataProtectionPolicyDef]] = pydantic.Field(None)
    FilterPattern: typing.Optional[dict[str, models.aws_logs.FilterPatternDef]] = pydantic.Field(None)
    JsonPattern: typing.Optional[dict[str, models.aws_logs.JsonPatternDef]] = pydantic.Field(None)
    QueryString: typing.Optional[dict[str, models.aws_logs.QueryStringDef]] = pydantic.Field(None)
    SpaceDelimitedTextPattern: typing.Optional[dict[str, models.aws_logs.SpaceDelimitedTextPatternDef]] = pydantic.Field(None)
    CrossAccountDestination: typing.Optional[dict[str, models.aws_logs.CrossAccountDestinationDef]] = pydantic.Field(None)
    LogGroup: typing.Optional[dict[str, models.aws_logs.LogGroupDef]] = pydantic.Field(None)
    LogRetention: typing.Optional[dict[str, models.aws_logs.LogRetentionDef]] = pydantic.Field(None)
    LogStream: typing.Optional[dict[str, models.aws_logs.LogStreamDef]] = pydantic.Field(None)
    MetricFilter: typing.Optional[dict[str, models.aws_logs.MetricFilterDef]] = pydantic.Field(None)
    QueryDefinition: typing.Optional[dict[str, models.aws_logs.QueryDefinitionDef]] = pydantic.Field(None)
    ResourcePolicy: typing.Optional[dict[str, models.aws_logs.ResourcePolicyDef]] = pydantic.Field(None)
    SubscriptionFilter: typing.Optional[dict[str, models.aws_logs.SubscriptionFilterDef]] = pydantic.Field(None)
    CfnMetricFilter_DimensionProperty: typing.Optional[dict[str, models.aws_logs.CfnMetricFilter_DimensionPropertyDef]] = pydantic.Field(None)
    CfnMetricFilter_MetricTransformationProperty: typing.Optional[dict[str, models.aws_logs.CfnMetricFilter_MetricTransformationPropertyDef]] = pydantic.Field(None)
    ColumnRestriction: typing.Optional[dict[str, models.aws_logs.ColumnRestrictionDef]] = pydantic.Field(None)
    CrossAccountDestinationProps: typing.Optional[dict[str, models.aws_logs.CrossAccountDestinationPropsDef]] = pydantic.Field(None)
    DataProtectionPolicyConfig: typing.Optional[dict[str, models.aws_logs.DataProtectionPolicyConfigDef]] = pydantic.Field(None)
    DataProtectionPolicyProps: typing.Optional[dict[str, models.aws_logs.DataProtectionPolicyPropsDef]] = pydantic.Field(None)
    LogGroupProps: typing.Optional[dict[str, models.aws_logs.LogGroupPropsDef]] = pydantic.Field(None)
    LogRetentionProps: typing.Optional[dict[str, models.aws_logs.LogRetentionPropsDef]] = pydantic.Field(None)
    LogRetentionRetryOptions: typing.Optional[dict[str, models.aws_logs.LogRetentionRetryOptionsDef]] = pydantic.Field(None)
    LogStreamProps: typing.Optional[dict[str, models.aws_logs.LogStreamPropsDef]] = pydantic.Field(None)
    LogSubscriptionDestinationConfig: typing.Optional[dict[str, models.aws_logs.LogSubscriptionDestinationConfigDef]] = pydantic.Field(None)
    MetricFilterOptions: typing.Optional[dict[str, models.aws_logs.MetricFilterOptionsDef]] = pydantic.Field(None)
    MetricFilterProps: typing.Optional[dict[str, models.aws_logs.MetricFilterPropsDef]] = pydantic.Field(None)
    QueryDefinitionProps: typing.Optional[dict[str, models.aws_logs.QueryDefinitionPropsDef]] = pydantic.Field(None)
    QueryStringProps: typing.Optional[dict[str, models.aws_logs.QueryStringPropsDef]] = pydantic.Field(None)
    ResourcePolicyProps: typing.Optional[dict[str, models.aws_logs.ResourcePolicyPropsDef]] = pydantic.Field(None)
    StreamOptions: typing.Optional[dict[str, models.aws_logs.StreamOptionsDef]] = pydantic.Field(None)
    SubscriptionFilterOptions: typing.Optional[dict[str, models.aws_logs.SubscriptionFilterOptionsDef]] = pydantic.Field(None)
    SubscriptionFilterProps: typing.Optional[dict[str, models.aws_logs.SubscriptionFilterPropsDef]] = pydantic.Field(None)
    CfnAccountPolicy: typing.Optional[dict[str, models.aws_logs.CfnAccountPolicyDef]] = pydantic.Field(None)
    CfnDestination: typing.Optional[dict[str, models.aws_logs.CfnDestinationDef]] = pydantic.Field(None)
    CfnLogGroup: typing.Optional[dict[str, models.aws_logs.CfnLogGroupDef]] = pydantic.Field(None)
    CfnLogStream: typing.Optional[dict[str, models.aws_logs.CfnLogStreamDef]] = pydantic.Field(None)
    CfnMetricFilter: typing.Optional[dict[str, models.aws_logs.CfnMetricFilterDef]] = pydantic.Field(None)
    CfnQueryDefinition: typing.Optional[dict[str, models.aws_logs.CfnQueryDefinitionDef]] = pydantic.Field(None)
    CfnResourcePolicy: typing.Optional[dict[str, models.aws_logs.CfnResourcePolicyDef]] = pydantic.Field(None)
    CfnSubscriptionFilter: typing.Optional[dict[str, models.aws_logs.CfnSubscriptionFilterDef]] = pydantic.Field(None)
    CfnAccountPolicyProps: typing.Optional[dict[str, models.aws_logs.CfnAccountPolicyPropsDef]] = pydantic.Field(None)
    CfnDestinationProps: typing.Optional[dict[str, models.aws_logs.CfnDestinationPropsDef]] = pydantic.Field(None)
    CfnLogGroupProps: typing.Optional[dict[str, models.aws_logs.CfnLogGroupPropsDef]] = pydantic.Field(None)
    CfnLogStreamProps: typing.Optional[dict[str, models.aws_logs.CfnLogStreamPropsDef]] = pydantic.Field(None)
    CfnMetricFilterProps: typing.Optional[dict[str, models.aws_logs.CfnMetricFilterPropsDef]] = pydantic.Field(None)
    CfnQueryDefinitionProps: typing.Optional[dict[str, models.aws_logs.CfnQueryDefinitionPropsDef]] = pydantic.Field(None)
    CfnResourcePolicyProps: typing.Optional[dict[str, models.aws_logs.CfnResourcePolicyPropsDef]] = pydantic.Field(None)
    CfnSubscriptionFilterProps: typing.Optional[dict[str, models.aws_logs.CfnSubscriptionFilterPropsDef]] = pydantic.Field(None)
    ...

import models
