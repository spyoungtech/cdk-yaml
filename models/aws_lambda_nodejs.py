from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_lambda_nodejs.NodejsFunction
class NodejsFunctionDef(BaseConstruct):
    aws_sdk_connection_reuse: typing.Optional[bool] = pydantic.Field(None, description='Whether to automatically reuse TCP connections when working with the AWS SDK for JavaScript. This sets the ``AWS_NODEJS_CONNECTION_REUSE_ENABLED`` environment variable to ``1``. Default: true\n')
    bundling: typing.Union[models.aws_lambda_nodejs.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundling options. Default: - use default bundling options: no minify, no sourcemap, all modules are bundled.\n')
    deps_lock_file_path: typing.Optional[str] = pydantic.Field(None, description='The path to the dependencies lock file (``yarn.lock``, ``pnpm-lock.yaml`` or ``package-lock.json``). This will be used as the source for the volume mounted in the Docker container. Modules specified in ``nodeModules`` will be installed using the right installer (``yarn``, ``pnpm`` or ``npm``) along with this lock file. Default: - the path is found by walking up parent directories searching for a ``yarn.lock``, ``pnpm-lock.yaml`` or ``package-lock.json`` file\n')
    entry: typing.Optional[str] = pydantic.Field(None, description="Path to the entry file (JavaScript or TypeScript). Default: - Derived from the name of the defining file and the construct's id. If the ``NodejsFunction`` is defined in ``stack.ts`` with ``my-handler`` as id (``new NodejsFunction(this, 'my-handler')``), the construct will look at ``stack.my-handler.ts`` and ``stack.my-handler.js``.\n")
    handler: typing.Optional[str] = pydantic.Field(None, description='The name of the exported handler in the entry file. The handler is prefixed with ``index.`` unless the specified handler value contains a ``.``, in which case it is used as-is. Default: handler\n')
    project_root: typing.Optional[str] = pydantic.Field(None, description='The path to the directory containing project config files (``package.json`` or ``tsconfig.json``). Default: - the directory containing the ``depsLockFilePath``\n')
    runtime: typing.Optional[models.aws_lambda.RuntimeDef] = pydantic.Field(None, description='The runtime environment. Only runtimes of the Node.js family are supported. Default: Runtime.NODEJS_18_X\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functionâ€™s /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    _init_params: typing.ClassVar[list[str]] = ['aws_sdk_connection_reuse', 'bundling', 'deps_lock_file_path', 'entry', 'handler', 'project_root', 'runtime', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'layers', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'max_event_age', 'on_failure', 'on_success', 'retry_attempts']
    _method_names: typing.ClassVar[list[str]] = ['add_alias', 'add_environment', 'add_event_source', 'add_event_source_mapping', 'add_function_url', 'add_layers', 'add_permission', 'add_to_role_policy', 'apply_removal_policy', 'configure_async_invoke', 'consider_warning_on_invoke_function_permissions', 'grant_invoke', 'grant_invoke_composite_principal', 'grant_invoke_url', 'invalidate_version_based_on', 'metric', 'metric_duration', 'metric_errors', 'metric_invocations', 'metric_throttles']
    _classmethod_names: typing.ClassVar[list[str]] = ['classify_version_property', 'from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_nodejs.NodejsFunction'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['from_function_arn', 'from_function_attributes', 'from_function_name', 'metric_all', 'metric_all_concurrent_executions', 'metric_all_duration', 'metric_all_errors', 'metric_all_invocations', 'metric_all_throttles', 'metric_all_unreserved_concurrent_executions']
    ...


    from_function_arn: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefFromFunctionArnParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its ARN.')
    from_function_attributes: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefFromFunctionAttributesParams] = pydantic.Field(None, description='Creates a Lambda function object which represents a function not defined within this stack.')
    from_function_name: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefFromFunctionNameParams] = pydantic.Field(None, description='Import a lambda function into the CDK using its name.')
    metric_all: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefMetricAllParams] = pydantic.Field(None, description='Return the given named metric for this Lambda.')
    metric_all_concurrent_executions: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefMetricAllConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of concurrent executions across all Lambdas.')
    metric_all_duration: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefMetricAllDurationParams] = pydantic.Field(None, description='Metric for the Duration executing all Lambdas.')
    metric_all_errors: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefMetricAllErrorsParams] = pydantic.Field(None, description='Metric for the number of Errors executing all Lambdas.')
    metric_all_invocations: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefMetricAllInvocationsParams] = pydantic.Field(None, description='Metric for the number of invocations of all Lambdas.')
    metric_all_throttles: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefMetricAllThrottlesParams] = pydantic.Field(None, description='Metric for the number of throttled invocations of all Lambdas.')
    metric_all_unreserved_concurrent_executions: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefMetricAllUnreservedConcurrentExecutionsParams] = pydantic.Field(None, description='Metric for the number of unreserved concurrent executions across all Lambdas.')
    resource_config: typing.Optional[models.aws_lambda_nodejs.NodejsFunctionDefConfig] = pydantic.Field(None)


class NodejsFunctionDefConfig(pydantic.BaseModel):
    add_alias: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddAliasParams]] = pydantic.Field(None, description='Defines an alias for this function.\nThe alias will automatically be updated to point to the latest version of\nthe function as it is being updated during a deployment::\n\n   # fn: lambda.Function\n\n\n   fn.add_alias("Live")\n\n   # Is equivalent to\n\n   lambda_.Alias(self, "AliasLive",\n       alias_name="Live",\n       version=fn.current_version\n   )')
    add_environment: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddEnvironmentParams]] = pydantic.Field(None, description='Adds an environment variable to this Lambda function.\nIf this is a ref to a Lambda function, this operation results in a no-op.')
    add_event_source: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddEventSourceParams]] = pydantic.Field(None, description="Adds an event source to this function.\nEvent sources are implemented in the aws-cdk-lib/aws-lambda-event-sources module.\n\nThe following example adds an SQS Queue as an event source::\n\n   import { SqsEventSource } from 'aws-cdk-lib/aws-lambda-event-sources';\n   myFunction.addEventSource(new SqsEventSource(myQueue));")
    add_event_source_mapping: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddEventSourceMappingParams]] = pydantic.Field(None, description='Adds an event source that maps to this AWS Lambda function.')
    add_function_url: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddFunctionUrlParams]] = pydantic.Field(None, description='Adds a url to this lambda function.')
    add_layers: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddLayersParams]] = pydantic.Field(None, description='Adds one or more Lambda Layers to this Lambda function.')
    add_permission: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddPermissionParams]] = pydantic.Field(None, description='Adds a permission to the Lambda resource policy.')
    add_to_role_policy: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefAddToRolePolicyParams]] = pydantic.Field(None, description='Adds a statement to the IAM role assumed by the instance.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    classify_version_property: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefClassifyVersionPropertyParams]] = pydantic.Field(None, description="Record whether specific properties in the ``AWS::Lambda::Function`` resource should also be associated to the Version resource.\nSee 'currentVersion' section in the module README for more details.")
    configure_async_invoke: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefConfigureAsyncInvokeParams]] = pydantic.Field(None, description='Configures options for asynchronous invocation.')
    consider_warning_on_invoke_function_permissions: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams]] = pydantic.Field(None, description='A warning will be added to functions under the following conditions: - permissions that include ``lambda:InvokeFunction`` are added to the unqualified function.\n- function.currentVersion is invoked before or after the permission is created.\n\nThis applies only to permissions on Lambda functions, not versions or aliases.\nThis function is overridden as a noOp for QualifiedFunctionBase.')
    grant_invoke: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefGrantInvokeParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda.')
    grant_invoke_composite_principal: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefGrantInvokeCompositePrincipalParams]] = pydantic.Field(None, description='Grant multiple principals the ability to invoke this Lambda via CompositePrincipal.')
    grant_invoke_url: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefGrantInvokeUrlParams]] = pydantic.Field(None, description='Grant the given identity permissions to invoke this Lambda Function URL.')
    invalidate_version_based_on: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefInvalidateVersionBasedOnParams]] = pydantic.Field(None, description='Mix additional information into the hash of the Version object.\nThe Lambda Function construct does its best to automatically create a new\nVersion when anything about the Function changes (its code, its layers,\nany of the other properties).\n\nHowever, you can sometimes source information from places that the CDK cannot\nlook into, like the deploy-time values of SSM parameters. In those cases,\nthe CDK would not force the creation of a new Version object when it actually\nshould.\n\nThis method can be used to invalidate the current Version object. Pass in\nany string into this method, and make sure the string changes when you know\na new Version needs to be created.\n\nThis method may be called more than once.')
    metric: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefMetricParams]] = pydantic.Field(None, description='Return the given named metric for this Function.')
    metric_duration: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefMetricDurationParams]] = pydantic.Field(None, description='How long execution of this Lambda takes.\nAverage over 5 minutes')
    metric_errors: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefMetricErrorsParams]] = pydantic.Field(None, description='How many invocations of this Lambda fail.\nSum over 5 minutes')
    metric_invocations: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefMetricInvocationsParams]] = pydantic.Field(None, description='How often this Lambda is invoked.\nSum over 5 minutes')
    metric_throttles: typing.Optional[list[models.aws_lambda_nodejs.NodejsFunctionDefMetricThrottlesParams]] = pydantic.Field(None, description='How often this Lambda is throttled.\nSum over 5 minutes')
    architecture_config: typing.Optional[models.aws_lambda.ArchitectureDefConfig] = pydantic.Field(None)
    connections_config: typing.Optional[models.aws_ec2.ConnectionsDefConfig] = pydantic.Field(None)
    current_version_config: typing.Optional[models.aws_lambda.VersionDefConfig] = pydantic.Field(None)
    grant_principal_config: typing.Optional[models._interface_methods.AwsIamIPrincipalDefConfig] = pydantic.Field(None)
    latest_version_config: typing.Optional[models._interface_methods.AwsLambdaIVersionDefConfig] = pydantic.Field(None)
    log_group_config: typing.Optional[models._interface_methods.AwsLogsILogGroupDefConfig] = pydantic.Field(None)
    permissions_node_config: typing.Optional[models.constructs.NodeDefConfig] = pydantic.Field(None)
    runtime_config: typing.Optional[models.aws_lambda.RuntimeDefConfig] = pydantic.Field(None)

class NodejsFunctionDefAddAliasParams(pydantic.BaseModel):
    alias_name: str = pydantic.Field(..., description='The name of the alias.\n')
    additional_versions: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.VersionWeightDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Additional versions with individual weights this alias points to. Individual additional version weights specified here should add up to (less than) one. All remaining weight is routed to the default version. For example, the config is version: "1" additionalVersions: [{ version: "2", weight: 0.05 }] Then 5% of traffic will be routed to function version 2, while the remaining 95% of traffic will be routed to function version 1. Default: No additional versions\n')
    description: typing.Optional[str] = pydantic.Field(None, description='Description for the alias. Default: No description\n')
    provisioned_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies a provisioned concurrency configuration for a function's alias. Default: No provisioned concurrency\n")
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    return_config: typing.Optional[list[models.aws_lambda.AliasDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefAddEnvironmentParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='The environment variable key.\n')
    value: str = pydantic.Field(..., description="The environment variable's value.\n")
    remove_in_edge: typing.Optional[bool] = pydantic.Field(None, description='When used in Lambda@Edge via edgeArn() API, these environment variables will be removed. If not set, an error will be thrown. Default: false - using the function in Lambda@Edge will throw')
    return_config: typing.Optional[list[models.aws_lambda.FunctionDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefAddEventSourceParams(pydantic.BaseModel):
    source: typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef] = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefAddEventSourceMappingParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='-\n')
    batch_size: typing.Union[int, float, None] = pydantic.Field(None, description='The largest number of records that AWS Lambda will retrieve from your event source at the time of invoking your function. Your function receives an event with all the retrieved records. Valid Range: Minimum value of 1. Maximum value of 10000. Default: - Amazon Kinesis, Amazon DynamoDB, and Amazon MSK is 100 records. The default for Amazon SQS is 10 messages. For standard SQS queues, the maximum is 10,000. For FIFO SQS queues, the maximum is 10.\n')
    bisect_batch_on_error: typing.Optional[bool] = pydantic.Field(None, description='If the function returns an error, split the batch in two and retry. Default: false\n')
    enabled: typing.Optional[bool] = pydantic.Field(None, description='Set to false to disable the event source upon creation. Default: true\n')
    event_source_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the event source. Any record added to this stream can invoke the Lambda function. Default: - not set if using a self managed Kafka cluster, throws an error otherwise\n')
    filters: typing.Optional[typing.Sequence[typing.Mapping[str, typing.Any]]] = pydantic.Field(None, description='Add filter criteria to Event Source. Default: - none\n')
    kafka_bootstrap_servers: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of host and port pairs that are the addresses of the Kafka brokers in a self managed "bootstrap" Kafka cluster that a Kafka client connects to initially to bootstrap itself. They are in the format ``abc.example.com:9096``. Default: - none\n')
    kafka_consumer_group_id: typing.Optional[str] = pydantic.Field(None, description="The identifier for the Kafka consumer group to join. The consumer group ID must be unique among all your Kafka event sources. After creating a Kafka event source mapping with the consumer group ID specified, you cannot update this value. The value must have a lenght between 1 and 200 and full the pattern '[a-zA-Z0-9-/*:_+=.@-]*'. For more information, see `Customizable consumer group ID <https://docs.aws.amazon.com/lambda/latest/dg/with-msk.html#services-msk-consumer-group-id>`_. Default: - none\n")
    kafka_topic: typing.Optional[str] = pydantic.Field(None, description='The name of the Kafka topic. Default: - no topic\n')
    max_batching_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum amount of time to gather records before invoking the function. Maximum of Duration.minutes(5) Default: Duration.seconds(0)\n')
    max_concurrency: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum concurrency setting limits the number of concurrent instances of the function that an Amazon SQS event source can invoke. Default: - No specific limit.\n')
    max_record_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a record that Lambda sends to a function for processing. Valid Range: - Minimum value of 60 seconds - Maximum value of 7 days Default: - infinite or until the record expires.\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_event_sources.SnsDlqDef, models.aws_lambda_event_sources.SqsDlqDef]] = pydantic.Field(None, description='An Amazon SQS queue or Amazon SNS topic destination for discarded records. Default: discarded records are ignored\n')
    parallelization_factor: typing.Union[int, float, None] = pydantic.Field(None, description='The number of batches to process from each shard concurrently. Valid Range: - Minimum value of 1 - Maximum value of 10 Default: 1\n')
    report_batch_item_failures: typing.Optional[bool] = pydantic.Field(None, description='Allow functions to return partially successful responses for a batch of records. Default: false\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Set to ``undefined`` if you want lambda to keep retrying infinitely or until the record expires. Valid Range: - Minimum value of 0 - Maximum value of 10000 Default: - infinite or until the record expires.\n')
    source_access_configurations: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.SourceAccessConfigurationDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Specific settings like the authentication protocol or the VPC components to secure access to your event source. Default: - none\n')
    starting_position: typing.Optional[aws_cdk.aws_lambda.StartingPosition] = pydantic.Field(None, description='The position in the DynamoDB, Kinesis or MSK stream where AWS Lambda should start reading. Default: - no starting position\n')
    starting_position_timestamp: typing.Union[int, float, None] = pydantic.Field(None, description='The time from which to start reading, in Unix time seconds. Default: - no timestamp\n')
    tumbling_window: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The size of the tumbling windows to group records sent to DynamoDB or Kinesis. Default: - None')
    return_config: typing.Optional[list[models.aws_lambda.EventSourceMappingDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefAddFunctionUrlParams(pydantic.BaseModel):
    auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The type of authentication that your function URL uses. Default: FunctionUrlAuthType.AWS_IAM\n')
    cors: typing.Union[models.aws_lambda.FunctionUrlCorsOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The cross-origin resource sharing (CORS) settings for your function URL. Default: - No CORS configuration.\n')
    invoke_mode: typing.Optional[aws_cdk.aws_lambda.InvokeMode] = pydantic.Field(None, description='The type of invocation mode that your Lambda function uses. Default: InvokeMode.BUFFERED')
    return_config: typing.Optional[list[models.aws_lambda.FunctionUrlDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefAddLayersParams(pydantic.BaseModel):
    layers: list[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]] = pydantic.Field(...)
    ...

class NodejsFunctionDefAddPermissionParams(pydantic.BaseModel):
    id: str = pydantic.Field(..., description='The id for the permission construct.\n')
    principal: typing.Union[models.aws_eks.ServiceAccountDef, models.aws_iam.UnknownPrincipalDef] = pydantic.Field(..., description='The entity for which you are granting permission to invoke the Lambda function. This entity can be any of the following: - a valid AWS service principal, such as ``s3.amazonaws.com`` or ``sns.amazonaws.com`` - an AWS account ID for cross-account permissions. For example, you might want to allow a custom application in another AWS account to push events to Lambda by invoking your function. - an AWS organization principal to grant permissions to an entire organization. The principal can be an AccountPrincipal, an ArnPrincipal, a ServicePrincipal, or an OrganizationPrincipal.\n')
    action: typing.Optional[str] = pydantic.Field(None, description="The Lambda actions that you want to allow in this statement. For example, you can specify lambda:CreateFunction to specify a certain action, or use a wildcard (``lambda:*``) to grant permission to all Lambda actions. For a list of actions, see Actions and Condition Context Keys for AWS Lambda in the IAM User Guide. Default: 'lambda:InvokeFunction'\n")
    event_source_token: typing.Optional[str] = pydantic.Field(None, description='A unique token that must be supplied by the principal invoking the function. Default: - The caller would not need to present a token.\n')
    function_url_auth_type: typing.Optional[aws_cdk.aws_lambda.FunctionUrlAuthType] = pydantic.Field(None, description='The authType for the function URL that you are granting permissions for. Default: - No functionUrlAuthType\n')
    organization_id: typing.Optional[str] = pydantic.Field(None, description='The organization you want to grant permissions to. Use this ONLY if you need to grant permissions to a subset of the organization. If you want to grant permissions to the entire organization, sending the organization principal through the ``principal`` property will suffice. You can use this property to ensure that all source principals are owned by a specific organization. Default: - No organizationId\n')
    scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to which the permission constructs be attached. The default is the Lambda function construct itself, but this would need to be different in cases such as cross-stack references where the Permissions would need to sit closer to the consumer of this permission (i.e., the caller). Default: - The instance of lambda.IFunction\n')
    source_account: typing.Optional[str] = pydantic.Field(None, description="The AWS account ID (without hyphens) of the source owner. For example, if you specify an S3 bucket in the SourceArn property, this value is the bucket owner's account ID. You can use this property to ensure that all source principals are owned by a specific account.\n")
    source_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of a resource that is invoking your function. When granting Amazon Simple Storage Service (Amazon S3) permission to invoke your function, specify this property with the bucket ARN as its value. This ensures that events generated only from the specified bucket, not just any bucket from any AWS account that creates a mapping to your function, can invoke the function.\n\n:see: Permission for details.\n')
    ...

class NodejsFunctionDefAddToRolePolicyParams(pydantic.BaseModel):
    statement: models.aws_iam.PolicyStatementDef = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefClassifyVersionPropertyParams(pydantic.BaseModel):
    property_name: str = pydantic.Field(..., description='The property to classify.\n')
    locked: bool = pydantic.Field(..., description='whether the property should be associated to the version or not.')
    ...

class NodejsFunctionDefConfigureAsyncInvokeParams(pydantic.BaseModel):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2')
    ...

class NodejsFunctionDefConsiderWarningOnInvokeFunctionPermissionsParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    action: str = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefFromFunctionArnParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_arn: str = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefFromFunctionAttributesParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='The parent construct.\n')
    id: str = pydantic.Field(..., description='The name of the lambda construct.\n')
    function_arn: str = pydantic.Field(..., description='The ARN of the Lambda function. Format: arn::lambda:::function:\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The architecture of this Lambda Function (this is an optional attribute and defaults to X86_64). Default: - Architecture.X86_64\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM execution role associated with this function. If the role is not specified, any role-related operations will no-op.\n')
    same_environment: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function is in the same environment as the stack. This affects certain behaviours such as, whether this function's permission can be modified. When not configured, the CDK attempts to auto-determine this. For environment agnostic stacks, i.e., stacks where the account is not specified with the ``env`` property, this is determined to be false. Set this to property *ONLY IF* the imported function is in the same account as the stack it's imported in. Default: - depends: true, if the Stack is configured with an explicit ``env`` (account and region) and the account is the same as this function. For environment-agnostic stacks this will default to ``false``.\n")
    security_group: typing.Optional[typing.Union[models.aws_ec2.SecurityGroupDef]] = pydantic.Field(None, description='The security group of this Lambda, if in a VPC. This needs to be given in order to support allowing connections to this Lambda.\n')
    skip_permissions: typing.Optional[bool] = pydantic.Field(None, description="Setting this property informs the CDK that the imported function ALREADY HAS the necessary permissions for what you are trying to do. When not configured, the CDK attempts to auto-determine whether or not additional permissions are necessary on the function when grant APIs are used. If the CDK tried to add permissions on an imported lambda, it will fail. Set this property *ONLY IF* you are committing to manage the imported function's permissions outside of CDK. You are acknowledging that your CDK code alone will have insufficient permissions to access the imported function. Default: false")
    ...

class NodejsFunctionDefFromFunctionNameParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    function_name: str = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefGrantInvokeParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefGrantInvokeCompositePrincipalParams(pydantic.BaseModel):
    composite_principal: models.aws_iam.CompositePrincipalDef = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefGrantInvokeUrlParams(pydantic.BaseModel):
    grantee: models.AnyResource = pydantic.Field(..., description='-')
    return_config: typing.Optional[list[models.aws_iam.GrantDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefInvalidateVersionBasedOnParams(pydantic.BaseModel):
    x: str = pydantic.Field(..., description='-')
    ...

class NodejsFunctionDefMetricParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefMetricAllParams(pydantic.BaseModel):
    metric_name: str = pydantic.Field(..., description='-\n')
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    ...

class NodejsFunctionDefMetricAllConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class NodejsFunctionDefMetricAllDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: average over 5 minutes\n')
    ...

class NodejsFunctionDefMetricAllErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class NodejsFunctionDefMetricAllInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class NodejsFunctionDefMetricAllThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: sum over 5 minutes\n')
    ...

class NodejsFunctionDefMetricAllUnreservedConcurrentExecutionsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream\n\n:default: max over 5 minutes\n')
    ...

class NodejsFunctionDefMetricDurationParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefMetricErrorsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefMetricInvocationsParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...

class NodejsFunctionDefMetricThrottlesParams(pydantic.BaseModel):
    account: typing.Optional[str] = pydantic.Field(None, description='Account which this metric comes from. Default: - Deployment account.\n')
    color: typing.Optional[str] = pydantic.Field(None, description="The hex color code, prefixed with '#' (e.g. '#00ff00'), to use when this metric is rendered on a graph. The ``Color`` class has a set of standard colors that can be used here. Default: - Automatic color\n")
    dimensions_map: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Dimensions of the metric. Default: - No dimensions.\n')
    label: typing.Optional[str] = pydantic.Field(None, description="Label for this metric when added to a Graph in a Dashboard. You can use `dynamic labels <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/graph-dynamic-labels.html>`_ to show summary information about the entire displayed time series in the legend. For example, if you use:: [max: ${MAX}] MyMetric As the metric label, the maximum value in the visible range will be shown next to the time series name in the graph's legend. Default: - No label\n")
    period: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The period over which the specified statistic is applied. Default: Duration.minutes(5)\n')
    region: typing.Optional[str] = pydantic.Field(None, description='Region which this metric comes from. Default: - Deployment region.\n')
    statistic: typing.Optional[str] = pydantic.Field(None, description='What function to use for aggregating. Use the ``aws_cloudwatch.Stats`` helper class to construct valid input strings. Can be one of the following: - "Minimum" | "min" - "Maximum" | "max" - "Average" | "avg" - "Sum" | "sum" - "SampleCount | "n" - "pNN.NN" - "tmNN.NN" | "tm(NN.NN%:NN.NN%)" - "iqm" - "wmNN.NN" | "wm(NN.NN%:NN.NN%)" - "tcNN.NN" | "tc(NN.NN%:NN.NN%)" - "tsNN.NN" | "ts(NN.NN%:NN.NN%)" Default: Average\n')
    unit: typing.Optional[aws_cdk.aws_cloudwatch.Unit] = pydantic.Field(None, description='Unit used to filter the metric stream. Only refer to datums emitted to the metric stream with the given unit and ignore all others. Only useful when datums are being emitted to the same metric stream under different units. The default is to use all matric datums in the stream, regardless of unit, which is recommended in nearly all cases. CloudWatch does not honor this property for graphs. Default: - All metric datums in the given metric stream')
    return_config: typing.Optional[list[models.aws_cloudwatch.MetricDefConfig]] = pydantic.Field(None)
    ...


#  autogenerated from aws_cdk.aws_lambda_nodejs.BundlingOptions
class BundlingOptionsDef(BaseStruct):
    command: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The command to run in the container. Default: - run the command defined in the image\n')
    entrypoint: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The entrypoint to run in the container. Default: - run the entrypoint defined in the image\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='The environment variables to pass to the container. Default: - no environment variables.\n')
    network: typing.Optional[str] = pydantic.Field(None, description='Docker `Networking options <https://docs.docker.com/engine/reference/commandline/run/#connect-a-container-to-a-network---network>`_. Default: - no networking options\n')
    platform: typing.Optional[str] = pydantic.Field(None, description='Set platform if server is multi-platform capable. *Requires Docker Engine API v1.38+*. Example value: ``linux/amd64`` Default: - no platform specified\n')
    security_opt: typing.Optional[str] = pydantic.Field(None, description='`Security configuration <https://docs.docker.com/engine/reference/run/#security-configuration>`_ when running the docker container. Default: - no security options\n')
    user: typing.Optional[str] = pydantic.Field(None, description='The user to use when running the container. Default: - root or image default\n')
    volumes: typing.Optional[typing.Sequence[typing.Union[models.DockerVolumeDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='Docker volumes to mount. Default: - no volumes are mounted\n')
    volumes_from: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Where to mount the specified volumes from. Default: - no containers are specified to mount volumes from\n')
    working_directory: typing.Optional[str] = pydantic.Field(None, description='Working directory inside the container. Default: - image default\n')
    asset_hash: typing.Optional[str] = pydantic.Field(None, description='Specify a custom hash for this asset. For consistency, this custom hash will be SHA256 hashed and encoded as hex. The resulting hash will be the asset hash. NOTE: the hash is used in order to identify a specific revision of the asset, and used for optimizing and caching deployment activities related to this asset such as packaging, uploading to Amazon S3, etc. If you chose to customize the hash, you will need to make sure it is updated every time the asset changes, or otherwise it is possible that some deployments will not be invalidated. Default: - asset hash is calculated based on the bundled output\n')
    banner: typing.Optional[str] = pydantic.Field(None, description='Use this to insert an arbitrary string at the beginning of generated JavaScript files. This is similar to footer which inserts at the end instead of the beginning. This is commonly used to insert comments: Default: - no comments are passed\n')
    build_args: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Build arguments to pass when building the bundling image. Default: - no build arguments are passed\n')
    bundling_file_access: typing.Optional[aws_cdk.BundlingFileAccess] = pydantic.Field(None, description='Which option to use to copy the source files to the docker container and output files back. Default: - BundlingFileAccess.BIND_MOUNT\n')
    charset: typing.Optional[aws_cdk.aws_lambda_nodejs.Charset] = pydantic.Field(None, description="The charset to use for esbuild's output. By default esbuild's output is ASCII-only. Any non-ASCII characters are escaped using backslash escape sequences. Using escape sequences makes the generated output slightly bigger, and also makes it harder to read. If you would like for esbuild to print the original characters without using escape sequences, use ``Charset.UTF8``. Default: Charset.ASCII\n")
    command_hooks: typing.Optional[models.UnsupportedResource] = pydantic.Field(None, description='Command hooks. Default: - do not run additional commands\n')
    define: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description="Replace global identifiers with constant expressions. For example, ``{ 'process.env.DEBUG': 'true' }``. Another example, ``{ 'process.env.API_KEY': JSON.stringify('xxx-xxxx-xxx') }``. Default: - no replacements are made\n")
    docker_image: typing.Optional[models.DockerImageDef] = pydantic.Field(None, description="A custom bundling Docker image. This image should have esbuild installed globally. If you plan to use ``nodeModules`` it should also have ``npm``, ``yarn`` or ``pnpm`` depending on the lock file you're using. See https://github.com/aws/aws-cdk/blob/main/packages/aws-cdk-lib/aws-lambda-nodejs/lib/Dockerfile for the default image provided by aws-cdk-lib/aws-lambda-nodejs. Default: - use the Docker image provided by aws-cdk-lib/aws-lambda-nodejs\n")
    esbuild_args: typing.Optional[typing.Mapping[str, typing.Union[str, bool]]] = pydantic.Field(None, description='Build arguments to pass into esbuild. For example, to add the `--log-limit <https://esbuild.github.io/api/#log-limit>`_ flag:: new NodejsFunction(scope, id, { ... bundling: { esbuildArgs: { "--log-limit": "0", } } }); Default: - no additional esbuild arguments are passed\n')
    esbuild_version: typing.Optional[str] = pydantic.Field(None, description='The version of esbuild to use when running in a Docker container. Default: - latest v0\n')
    external_modules: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of modules that should be considered as externals (already available in the runtime). Default: - ['aws-sdk'] if the runtime is <= Node.js 16.x, ['@aws-sdk/*'] if Node.js 18.x, [] if using a variable runtime version such as NODEJS_LATEST.\n")
    footer: typing.Optional[str] = pydantic.Field(None, description='Use this to insert an arbitrary string at the end of generated JavaScript files. This is similar to banner which inserts at the beginning instead of the end. This is commonly used to insert comments Default: - no comments are passed\n')
    force_docker_bundling: typing.Optional[bool] = pydantic.Field(None, description='Force bundling in a Docker container even if local bundling is possible. This is useful if your function relies on node modules that should be installed (``nodeModules``) in a Lambda compatible environment. Default: false\n')
    format: typing.Optional[aws_cdk.aws_lambda_nodejs.OutputFormat] = pydantic.Field(None, description='Output format for the generated JavaScript files. Default: OutputFormat.CJS\n')
    inject: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='This option allows you to automatically replace a global variable with an import from another file. Default: - no code is injected\n')
    keep_names: typing.Optional[bool] = pydantic.Field(None, description='Whether to preserve the original ``name`` values even in minified code. In JavaScript the ``name`` property on functions and classes defaults to a nearby identifier in the source code. However, minification renames symbols to reduce code size and bundling sometimes need to rename symbols to avoid collisions. That changes value of the ``name`` property for many of these cases. This is usually fine because the ``name`` property is normally only used for debugging. However, some frameworks rely on the ``name`` property for registration and binding purposes. If this is the case, you can enable this option to preserve the original ``name`` values even in minified code. Default: false\n')
    loader: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description="Use loaders to change how a given input file is interpreted. Configuring a loader for a given file type lets you load that file type with an ``import`` statement or a ``require`` call. For example, ``{ '.png': 'dataurl' }``. Default: - use esbuild default loaders\n")
    log_level: typing.Optional[aws_cdk.aws_lambda_nodejs.LogLevel] = pydantic.Field(None, description='Log level for esbuild. This is also propagated to the package manager and applies to its specific install command. Default: LogLevel.WARNING\n')
    main_fields: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="How to determine the entry point for modules. Try ['module', 'main'] to default to ES module versions. Default: []\n")
    metafile: typing.Optional[bool] = pydantic.Field(None, description="This option tells esbuild to write out a JSON file relative to output directory with metadata about the build. The metadata in this JSON file follows this schema (specified using TypeScript syntax):: { outputs: { [path: string]: { bytes: number inputs: { [path: string]: { bytesInOutput: number } } imports: { path: string }[] exports: string[] } } } This data can then be analyzed by other tools. For example, bundle buddy can consume esbuild's metadata format and generates a treemap visualization of the modules in your bundle and how much space each one takes up. Default: false\n")
    minify: typing.Optional[bool] = pydantic.Field(None, description='Whether to minify files when bundling. Default: false\n')
    node_modules: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='A list of modules that should be installed instead of bundled. Modules are installed in a Lambda compatible environment only when bundling runs in Docker. Default: - all modules are bundled\n')
    pre_compilation: typing.Optional[bool] = pydantic.Field(None, description="Run compilation using tsc before running file through bundling step. This usually is not required unless you are using new experimental features that are only supported by typescript's ``tsc`` compiler. One example of such feature is ``emitDecoratorMetadata``. Default: false\n")
    source_map: typing.Optional[bool] = pydantic.Field(None, description='Whether to include source maps when bundling. Default: false\n')
    source_map_mode: typing.Optional[aws_cdk.aws_lambda_nodejs.SourceMapMode] = pydantic.Field(None, description='Source map mode to be used when bundling. Default: SourceMapMode.DEFAULT\n')
    sources_content: typing.Optional[bool] = pydantic.Field(None, description='Whether to include original source code in source maps when bundling. Default: true\n')
    target: typing.Optional[str] = pydantic.Field(None, description='Target environment for the generated JavaScript code. Default: - the node version of the runtime\n')
    tsconfig: typing.Optional[str] = pydantic.Field(None, description='Normally the esbuild automatically discovers ``tsconfig.json`` files and reads their contents during a build. However, you can also configure a custom ``tsconfig.json`` file to use instead. This is similar to entry path, you need to provide path to your custom ``tsconfig.json``. This can be useful if you need to do multiple builds of the same code with different settings. For example, ``{ \'tsconfig\': \'path/custom.tsconfig.json\' }``. Default: - automatically discovered by ``esbuild``\n\n:exampleMetadata: infused\n\nExample::\n\n    nodejs.NodejsFunction(self, "my-handler",\n        bundling=nodejs.BundlingOptions(\n            docker_image=DockerImage.from_build("/path/to/Dockerfile")\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['command', 'entrypoint', 'environment', 'network', 'platform', 'security_opt', 'user', 'volumes', 'volumes_from', 'working_directory', 'asset_hash', 'banner', 'build_args', 'bundling_file_access', 'charset', 'command_hooks', 'define', 'docker_image', 'esbuild_args', 'esbuild_version', 'external_modules', 'footer', 'force_docker_bundling', 'format', 'inject', 'keep_names', 'loader', 'log_level', 'main_fields', 'metafile', 'minify', 'node_modules', 'pre_compilation', 'source_map', 'source_map_mode', 'sources_content', 'target', 'tsconfig']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_nodejs.BundlingOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_nodejs.NodejsFunctionProps
class NodejsFunctionPropsDef(BaseStruct):
    max_event_age: typing.Optional[models.DurationDef] = pydantic.Field(None, description='The maximum age of a request that Lambda sends to a function for processing. Minimum: 60 seconds Maximum: 6 hours Default: Duration.hours(6)\n')
    on_failure: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for failed invocations. Default: - no destination\n')
    on_success: typing.Optional[typing.Union[models.aws_lambda_destinations.EventBridgeDestinationDef, models.aws_lambda_destinations.LambdaDestinationDef, models.aws_lambda_destinations.SnsDestinationDef, models.aws_lambda_destinations.SqsDestinationDef]] = pydantic.Field(None, description='The destination for successful invocations. Default: - no destination\n')
    retry_attempts: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of times to retry when the function returns an error. Minimum: 0 Maximum: 2 Default: 2\n')
    adot_instrumentation: typing.Union[models.aws_lambda.AdotInstrumentationConfigDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Specify the configuration of AWS Distro for OpenTelemetry (ADOT) instrumentation. Default: - No ADOT instrumentation\n')
    allow_all_outbound: typing.Optional[bool] = pydantic.Field(None, description='Whether to allow the Lambda to send all network traffic. If set to false, you must individually add traffic rules to allow the Lambda to connect to network targets. Default: true\n')
    allow_public_subnet: typing.Optional[bool] = pydantic.Field(None, description='Lambda Functions in a public subnet can NOT access the internet. Use this property to acknowledge this limitation and still place the function in a public subnet. Default: false\n')
    architecture: typing.Optional[models.aws_lambda.ArchitectureDef] = pydantic.Field(None, description='The system architectures compatible with this lambda function. Default: Architecture.X86_64\n')
    code_signing_config: typing.Optional[typing.Union[models.aws_lambda.CodeSigningConfigDef]] = pydantic.Field(None, description='Code signing config associated with this function. Default: - Not Sign the Code\n')
    current_version_options: typing.Union[models.aws_lambda.VersionOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Options for the ``lambda.Version`` resource automatically created by the ``fn.currentVersion`` method. Default: - default options as described in ``VersionOptions``\n')
    dead_letter_queue: typing.Optional[typing.Union[models.aws_sqs.QueueBaseDef, models.aws_sqs.QueueDef]] = pydantic.Field(None, description='The SQS queue to use if DLQ is enabled. If SNS topic is desired, specify ``deadLetterTopic`` property instead. Default: - SQS queue with 14 day retention period if ``deadLetterQueueEnabled`` is ``true``\n')
    dead_letter_queue_enabled: typing.Optional[bool] = pydantic.Field(None, description='Enabled DLQ. If ``deadLetterQueue`` is undefined, an SQS queue with default options will be defined for your Function. Default: - false unless ``deadLetterQueue`` is set, which implies DLQ is enabled.\n')
    dead_letter_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='The SNS topic to use as a DLQ. Note that if ``deadLetterQueueEnabled`` is set to ``true``, an SQS queue will be created rather than an SNS topic. Using an SNS topic as a DLQ requires this property to be set explicitly. Default: - no SNS topic\n')
    description: typing.Optional[str] = pydantic.Field(None, description='A description of the function. Default: - No description.\n')
    environment: typing.Optional[typing.Mapping[str, str]] = pydantic.Field(None, description='Key-value pairs that Lambda caches and makes available for your Lambda functions. Use environment variables to apply configuration changes, such as test and production environment configurations, without changing your Lambda function source code. Default: - No environment variables.\n')
    environment_encryption: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description="The AWS KMS key that's used to encrypt your function's environment variables. Default: - AWS Lambda creates and uses an AWS managed customer master key (CMK).\n")
    ephemeral_storage_size: typing.Optional[models.SizeDef] = pydantic.Field(None, description='The size of the functionâ€™s /tmp directory in MiB. Default: 512 MiB\n')
    events: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda_event_sources.ApiEventSourceDef, models.aws_lambda_event_sources.DynamoEventSourceDef, models.aws_lambda_event_sources.KinesisEventSourceDef, models.aws_lambda_event_sources.ManagedKafkaEventSourceDef, models.aws_lambda_event_sources.S3EventSourceDef, models.aws_lambda_event_sources.SelfManagedKafkaEventSourceDef, models.aws_lambda_event_sources.SnsEventSourceDef, models.aws_lambda_event_sources.SqsEventSourceDef, models.aws_lambda_event_sources.StreamEventSourceDef]]] = pydantic.Field(None, description='Event sources for this function. You can also add event sources using ``addEventSource``. Default: - No event sources.\n')
    filesystem: typing.Optional[models.aws_lambda.FileSystemDef] = pydantic.Field(None, description='The filesystem configuration for the lambda function. Default: - will not mount any filesystem\n')
    function_name: typing.Optional[str] = pydantic.Field(None, description="A name for the function. Default: - AWS CloudFormation generates a unique physical ID and uses that ID for the function's name. For more information, see Name Type.\n")
    initial_policy: typing.Optional[typing.Sequence[models.aws_iam.PolicyStatementDef]] = pydantic.Field(None, description='Initial policy statements to add to the created Lambda Role. You can call ``addToRolePolicy`` to the created lambda to add statements post creation. Default: - No policy statements are added to the created Lambda role.\n')
    insights_version: typing.Optional[models.aws_lambda.LambdaInsightsVersionDef] = pydantic.Field(None, description='Specify the version of CloudWatch Lambda insights to use for monitoring. Default: - No Lambda Insights\n')
    layers: typing.Optional[typing.Sequence[typing.Union[models.aws_lambda.LayerVersionDef, models.lambda_layer_awscli.AwsCliLayerDef, models.lambda_layer_kubectl.KubectlLayerDef, models.lambda_layer_node_proxy_agent.NodeProxyAgentLayerDef]]] = pydantic.Field(None, description="A list of layers to add to the function's execution environment. You can configure your Lambda function to pull in additional code during initialization in the form of layers. Layers are packages of libraries or other dependencies that can be used by multiple functions. Default: - No layers.\n")
    log_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description="The number of days log events are kept in CloudWatch Logs. When updating this property, unsetting it doesn't remove the log retention policy. To remove the retention policy, set the value to ``INFINITE``. Default: logs.RetentionDays.INFINITE\n")
    log_retention_retry_options: typing.Union[models.aws_lambda.LogRetentionRetryOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='When log retention is specified, a custom resource attempts to create the CloudWatch log group. These options control the retry policy when interacting with CloudWatch APIs. Default: - Default AWS SDK retry options.\n')
    log_retention_role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='The IAM role for the Lambda function associated with the custom resource that sets the retention policy. Default: - A new role is created.\n')
    memory_size: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of memory, in MB, that is allocated to your Lambda function. Lambda uses this value to proportionally allocate the amount of CPU power. For more information, see Resource Model in the AWS Lambda Developer Guide. Default: 128\n')
    params_and_secrets: typing.Optional[models.aws_lambda.ParamsAndSecretsLayerVersionDef] = pydantic.Field(None, description='Specify the configuration of Parameters and Secrets Extension. Default: - No Parameters and Secrets Extension\n')
    profiling: typing.Optional[bool] = pydantic.Field(None, description='Enable profiling. Default: - No profiling.\n')
    profiling_group: typing.Optional[typing.Union[models.aws_codeguruprofiler.ProfilingGroupDef]] = pydantic.Field(None, description='Profiling Group. Default: - A new profiling group will be created if ``profiling`` is set.\n')
    reserved_concurrent_executions: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum of concurrent executions you want to reserve for the function. Default: - No specific limit - account limit.\n')
    role: typing.Optional[typing.Union[models.aws_iam.LazyRoleDef, models.aws_iam.RoleDef]] = pydantic.Field(None, description='Lambda execution role. This is the role that will be assumed by the function upon execution. It controls the permissions that the function will have. The Role must be assumable by the \'lambda.amazonaws.com\' service principal. The default Role automatically has permissions granted for Lambda execution. If you provide a Role, you must add the relevant AWS managed policies yourself. The relevant managed policies are "service-role/AWSLambdaBasicExecutionRole" and "service-role/AWSLambdaVPCAccessExecutionRole". Default: - A unique role will be generated for this lambda function. Both supplied and generated roles can always be changed by calling ``addToRolePolicy``.\n')
    runtime_management_mode: typing.Optional[models.aws_lambda.RuntimeManagementModeDef] = pydantic.Field(None, description="Sets the runtime management configuration for a function's version. Default: Auto\n")
    security_groups: typing.Optional[typing.Sequence[typing.Union[models.aws_ec2.SecurityGroupDef]]] = pydantic.Field(None, description="The list of security groups to associate with the Lambda's network interfaces. Only used if 'vpc' is supplied. Default: - If the function is placed within a VPC and a security group is not specified, either by this or securityGroup prop, a dedicated security group will be created for this function.\n")
    snap_start: typing.Optional[models.aws_lambda.SnapStartConfDef] = pydantic.Field(None, description='Enable SnapStart for Lambda Function. SnapStart is currently supported only for Java 11, 17 runtime Default: - No snapstart\n')
    timeout: typing.Optional[models.DurationDef] = pydantic.Field(None, description="The function execution time (in seconds) after which Lambda terminates the function. Because the execution time affects cost, set this value based on the function's expected execution time. Default: Duration.seconds(3)\n")
    tracing: typing.Optional[aws_cdk.aws_lambda.Tracing] = pydantic.Field(None, description='Enable AWS X-Ray Tracing for Lambda Function. Default: Tracing.Disabled\n')
    vpc: typing.Optional[typing.Union[models.aws_ec2.VpcDef]] = pydantic.Field(None, description='VPC network to place Lambda network interfaces. Specify this if the Lambda function needs to access resources in a VPC. This is required when ``vpcSubnets`` is specified. Default: - Function is not placed within a VPC.\n')
    vpc_subnets: typing.Union[models.aws_ec2.SubnetSelectionDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Where to place the network interfaces within the VPC. This requires ``vpc`` to be specified in order for interfaces to actually be placed in the subnets. If ``vpc`` is not specify, this will raise an error. Note: Internet access for Lambda Functions requires a NAT Gateway, so picking public subnets is not allowed (unless ``allowPublicSubnet`` is set to ``true``). Default: - the Vpc default strategy if not specified\n')
    aws_sdk_connection_reuse: typing.Optional[bool] = pydantic.Field(None, description='Whether to automatically reuse TCP connections when working with the AWS SDK for JavaScript. This sets the ``AWS_NODEJS_CONNECTION_REUSE_ENABLED`` environment variable to ``1``. Default: true\n')
    bundling: typing.Union[models.aws_lambda_nodejs.BundlingOptionsDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Bundling options. Default: - use default bundling options: no minify, no sourcemap, all modules are bundled.\n')
    deps_lock_file_path: typing.Optional[str] = pydantic.Field(None, description='The path to the dependencies lock file (``yarn.lock``, ``pnpm-lock.yaml`` or ``package-lock.json``). This will be used as the source for the volume mounted in the Docker container. Modules specified in ``nodeModules`` will be installed using the right installer (``yarn``, ``pnpm`` or ``npm``) along with this lock file. Default: - the path is found by walking up parent directories searching for a ``yarn.lock``, ``pnpm-lock.yaml`` or ``package-lock.json`` file\n')
    entry: typing.Optional[str] = pydantic.Field(None, description="Path to the entry file (JavaScript or TypeScript). Default: - Derived from the name of the defining file and the construct's id. If the ``NodejsFunction`` is defined in ``stack.ts`` with ``my-handler`` as id (``new NodejsFunction(this, 'my-handler')``), the construct will look at ``stack.my-handler.ts`` and ``stack.my-handler.js``.\n")
    handler: typing.Optional[str] = pydantic.Field(None, description='The name of the exported handler in the entry file. The handler is prefixed with ``index.`` unless the specified handler value contains a ``.``, in which case it is used as-is. Default: handler\n')
    project_root: typing.Optional[str] = pydantic.Field(None, description='The path to the directory containing project config files (``package.json`` or ``tsconfig.json``). Default: - the directory containing the ``depsLockFilePath``\n')
    runtime: typing.Optional[models.aws_lambda.RuntimeDef] = pydantic.Field(None, description='The runtime environment. Only runtimes of the Node.js family are supported. Default: Runtime.NODEJS_18_X\n\n:exampleMetadata: infused\n\nExample::\n\n    nodejs.NodejsFunction(self, "my-handler",\n        bundling=nodejs.BundlingOptions(\n            network="host",\n            security_opt="no-new-privileges",\n            user="user:group",\n            volumes_from=["777f7dc92da7"],\n            volumes=[DockerVolume(host_path="/host-path", container_path="/container-path")]\n        )\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_event_age', 'on_failure', 'on_success', 'retry_attempts', 'adot_instrumentation', 'allow_all_outbound', 'allow_public_subnet', 'architecture', 'code_signing_config', 'current_version_options', 'dead_letter_queue', 'dead_letter_queue_enabled', 'dead_letter_topic', 'description', 'environment', 'environment_encryption', 'ephemeral_storage_size', 'events', 'filesystem', 'function_name', 'initial_policy', 'insights_version', 'layers', 'log_retention', 'log_retention_retry_options', 'log_retention_role', 'memory_size', 'params_and_secrets', 'profiling', 'profiling_group', 'reserved_concurrent_executions', 'role', 'runtime_management_mode', 'security_groups', 'snap_start', 'timeout', 'tracing', 'vpc', 'vpc_subnets', 'aws_sdk_connection_reuse', 'bundling', 'deps_lock_file_path', 'entry', 'handler', 'project_root', 'runtime']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_lambda_nodejs.NodejsFunctionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_lambda_nodejs.Charset
# skipping emum

#  autogenerated from aws_cdk.aws_lambda_nodejs.LogLevel
# skipping emum

#  autogenerated from aws_cdk.aws_lambda_nodejs.OutputFormat
# skipping emum

#  autogenerated from aws_cdk.aws_lambda_nodejs.SourceMapMode
# skipping emum

#  autogenerated from aws_cdk.aws_lambda_nodejs.ICommandHooks
#  skipping Interface

class ModuleModel(pydantic.BaseModel):
    NodejsFunction: typing.Optional[dict[str, models.aws_lambda_nodejs.NodejsFunctionDef]] = pydantic.Field(None)
    BundlingOptions: typing.Optional[dict[str, models.aws_lambda_nodejs.BundlingOptionsDef]] = pydantic.Field(None)
    NodejsFunctionProps: typing.Optional[dict[str, models.aws_lambda_nodejs.NodejsFunctionPropsDef]] = pydantic.Field(None)
    ...

import models
