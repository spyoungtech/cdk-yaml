from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_cloudtrail.InsightType
class InsightTypeDef(BaseClass):
    value: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='-')
    _init_params: typing.ClassVar[list[str]] = ['value']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.InsightType'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.Trail
class TrailDef(BaseConstruct):
    bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='The Amazon S3 bucket. Default: - if not supplied a bucket will be created with all the correct permisions\n')
    cloud_watch_log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='Log Group to which CloudTrail to push logs to. Ignored if sendToCloudWatchLogs is set to false. Default: - a new log group is created and used.\n')
    cloud_watch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='How long to retain logs in CloudWatchLogs. Ignored if sendToCloudWatchLogs is false or if cloudWatchLogGroup is set. Default: logs.RetentionDays.ONE_YEAR\n')
    enable_file_validation: typing.Optional[bool] = pydantic.Field(None, description='To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection. You can use the AWS CLI to validate the files in the location where CloudTrail delivered them. Default: true\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS Key Management Service (AWS KMS) key ID that you want to use to encrypt CloudTrail logs. Default: - No encryption.\n')
    include_global_service_events: typing.Optional[bool] = pydantic.Field(None, description='For most services, events are recorded in the region where the action occurred. For global services such as AWS Identity and Access Management (IAM), AWS STS, Amazon CloudFront, and Route 53, events are delivered to any trail that includes global services, and are logged as occurring in US East (N. Virginia) Region. Default: true\n')
    insight_types: typing.Optional[typing.Sequence[models.aws_cloudtrail.InsightTypeDef]] = pydantic.Field(None, description='A JSON string that contains the insight types you want to log on a trail. Default: - No Value.\n')
    is_multi_region_trail: typing.Optional[bool] = pydantic.Field(None, description='Whether or not this trail delivers log files from multiple regions to a single S3 bucket for a single account. Default: true\n')
    is_organization_trail: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the trail is applied to all accounts in an organization in AWS Organizations, or only for the current AWS account. If this is set to true then the current account *must* be the management account. If it is not, then CloudFormation will throw an error. If this is set to true and the current account is a management account for an organization in AWS Organizations, the trail will be created in all AWS accounts that belong to the organization. If this is set to false, the trail will remain in the current AWS account but be deleted from all member accounts in the organization. Default: - false\n')
    management_events: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails. Only events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group. This method sets the management configuration for this trail. Management events provide insight into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Management events can also include non-API events that occur in your account. For example, when a user logs in to your account, CloudTrail logs the ConsoleLogin event. Default: ReadWriteType.ALL\n')
    s3_key_prefix: typing.Optional[str] = pydantic.Field(None, description='An Amazon S3 object key prefix that precedes the name of all log files. Default: - No prefix.\n')
    send_to_cloud_watch_logs: typing.Optional[bool] = pydantic.Field(None, description='If CloudTrail pushes logs to CloudWatch Logs in addition to S3. Disabled for cost out of the box. Default: false\n')
    sns_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='SNS topic that is notified when new log files are published. Default: - No notifications.\n')
    trail_name: typing.Optional[str] = pydantic.Field(None, description='The name of the trail. We recommend customers do not set an explicit name. Default: - AWS CloudFormation generated name.')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'cloud_watch_log_group', 'cloud_watch_logs_retention', 'enable_file_validation', 'encryption_key', 'include_global_service_events', 'insight_types', 'is_multi_region_trail', 'is_organization_trail', 'management_events', 's3_key_prefix', 'send_to_cloud_watch_logs', 'sns_topic', 'trail_name']
    _method_names: typing.ClassVar[list[str]] = ['add_event_selector', 'add_lambda_event_selector', 'add_s3_event_selector', 'apply_removal_policy', 'log_all_lambda_data_events', 'log_all_s3_data_events']
    _classmethod_names: typing.ClassVar[list[str]] = ['on_event']
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.Trail'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = ['on_event']
    ...


    on_event: typing.Optional[TrailDefOnEventParams] = pydantic.Field(None, description="Create an event rule for when an event is recorded by any Trail in the account.\nNote that the event doesn't necessarily have to come from this Trail, it can\nbe captured from any one.\n\nBe sure to filter the event further down using an event pattern.")
    resource_config: typing.Optional[TrailDefConfig] = pydantic.Field(None)


class TrailDefConfig(pydantic.BaseModel):
    add_event_selector: typing.Optional[list[TrailDefAddEventSelectorParams]] = pydantic.Field(None, description='When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails.\nOnly events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group.\n\nThis method adds an Event Selector for filtering events that match either S3 or Lambda function operations.\n\nData events: These events provide insight into the resource operations performed on or within a resource.\nThese are also known as data plane operations.')
    add_lambda_event_selector: typing.Optional[list[TrailDefAddLambdaEventSelectorParams]] = pydantic.Field(None, description='When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails.\nOnly events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group.\n\nThis method adds a Lambda Data Event Selector for filtering events that match Lambda function operations.\n\nData events: These events provide insight into the resource operations performed on or within a resource.\nThese are also known as data plane operations.')
    add_s3_event_selector: typing.Optional[list[TrailDefAddS3EventSelectorParams]] = pydantic.Field(None, description='When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails.\nOnly events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group.\n\nThis method adds an S3 Data Event Selector for filtering events that match S3 operations.\n\nData events: These events provide insight into the resource operations performed on or within a resource.\nThese are also known as data plane operations.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    log_all_lambda_data_events: typing.Optional[list[TrailDefLogAllLambdaDataEventsParams]] = pydantic.Field(None, description='Log all Lambda data events for all lambda functions the account.')
    log_all_s3_data_events: typing.Optional[list[TrailDefLogAllS3DataEventsParams]] = pydantic.Field(None, description='Log all S3 data events for all objects for all buckets in the account.')

class TrailDefAddEventSelectorParams(pydantic.BaseModel):
    data_resource_type: aws_cdk.aws_cloudtrail.DataResourceType = pydantic.Field(..., description='-\n')
    data_resource_values: typing.Sequence[str] = pydantic.Field(..., description='the list of data resource ARNs to include in logging (maximum 250 entries).\n')
    exclude_management_event_sources: typing.Optional[typing.Sequence[aws_cdk.aws_cloudtrail.ManagementEventSources]] = pydantic.Field(None, description='An optional list of service event sources from which you do not want management events to be logged on your trail. Default: []\n')
    include_management_events: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the event selector includes management events for the trail. Default: true\n')
    read_write_type: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='Specifies whether to log read-only events, write-only events, or all events. Default: ReadWriteType.All')
    ...

class TrailDefAddLambdaEventSelectorParams(pydantic.BaseModel):
    handlers: typing.Sequence[typing.Union[models.aws_lambda.FunctionBaseDef, models.aws_lambda.QualifiedFunctionBaseDef, models.aws_lambda.AliasDef, models.aws_lambda.DockerImageFunctionDef, models.aws_lambda.FunctionDef, models.aws_lambda.SingletonFunctionDef, models.aws_lambda.VersionDef, models.aws_lambda_nodejs.NodejsFunctionDef, models.triggers.TriggerFunctionDef]] = pydantic.Field(..., description='the list of lambda function handlers whose data events should be logged (maximum 250 entries).\n')
    exclude_management_event_sources: typing.Optional[typing.Sequence[aws_cdk.aws_cloudtrail.ManagementEventSources]] = pydantic.Field(None, description='An optional list of service event sources from which you do not want management events to be logged on your trail. Default: []\n')
    include_management_events: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the event selector includes management events for the trail. Default: true\n')
    read_write_type: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='Specifies whether to log read-only events, write-only events, or all events. Default: ReadWriteType.All')
    ...

class TrailDefAddS3EventSelectorParams(pydantic.BaseModel):
    s3_selector: typing.Sequence[typing.Union[models.aws_cloudtrail.S3EventSelectorDef, dict[str, typing.Any]]] = pydantic.Field(..., description='the list of S3 bucket with optional prefix to include in logging (maximum 250 entries).\n')
    exclude_management_event_sources: typing.Optional[typing.Sequence[aws_cdk.aws_cloudtrail.ManagementEventSources]] = pydantic.Field(None, description='An optional list of service event sources from which you do not want management events to be logged on your trail. Default: []\n')
    include_management_events: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the event selector includes management events for the trail. Default: true\n')
    read_write_type: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='Specifies whether to log read-only events, write-only events, or all events. Default: ReadWriteType.All')
    ...

class TrailDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: aws_cdk.RemovalPolicy = pydantic.Field(..., description='-')
    ...

class TrailDefLogAllLambdaDataEventsParams(pydantic.BaseModel):
    exclude_management_event_sources: typing.Optional[typing.Sequence[aws_cdk.aws_cloudtrail.ManagementEventSources]] = pydantic.Field(None, description='An optional list of service event sources from which you do not want management events to be logged on your trail. Default: []\n')
    include_management_events: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the event selector includes management events for the trail. Default: true\n')
    read_write_type: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='Specifies whether to log read-only events, write-only events, or all events. Default: ReadWriteType.All\n\n:default: false\n\n:see: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\n')
    ...

class TrailDefLogAllS3DataEventsParams(pydantic.BaseModel):
    exclude_management_event_sources: typing.Optional[typing.Sequence[aws_cdk.aws_cloudtrail.ManagementEventSources]] = pydantic.Field(None, description='An optional list of service event sources from which you do not want management events to be logged on your trail. Default: []\n')
    include_management_events: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the event selector includes management events for the trail. Default: true\n')
    read_write_type: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='Specifies whether to log read-only events, write-only events, or all events. Default: ReadWriteType.All\n\n:default: false\n\n:see: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html\n')
    ...

class TrailDefOnEventParams(pydantic.BaseModel):
    scope: models.constructs.ConstructDef = pydantic.Field(..., description='-\n')
    id: str = pydantic.Field(..., description='-\n')
    target: typing.Optional[typing.Union[models.aws_events_targets.ApiDestinationDef, models.aws_events_targets.ApiGatewayDef, models.aws_events_targets.AwsApiDef, models.aws_events_targets.BatchJobDef, models.aws_events_targets.CloudWatchLogGroupDef, models.aws_events_targets.CodeBuildProjectDef, models.aws_events_targets.CodePipelineDef, models.aws_events_targets.EcsTaskDef, models.aws_events_targets.EventBusDef, models.aws_events_targets.KinesisFirehoseStreamDef, models.aws_events_targets.KinesisStreamDef, models.aws_events_targets.LambdaFunctionDef, models.aws_events_targets.SfnStateMachineDef, models.aws_events_targets.SnsTopicDef, models.aws_events_targets.SqsQueueDef]] = pydantic.Field(None, description='The target to register for the event. Default: - No target is added to the rule. Use ``addTarget()`` to add a target.\n')
    cross_stack_scope: typing.Optional[models.constructs.ConstructDef] = pydantic.Field(None, description='The scope to use if the source of the rule and its target are in different Stacks (but in the same account & region). This helps dealing with cycles that often arise in these situations. Default: - none (the main scope will be used, even for cross-stack Events)\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the rule's purpose. Default: - No description\n")
    event_pattern: typing.Union[models.aws_events.EventPatternDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Additional restrictions for the event to route to the specified target. The method that generates the rule probably imposes some type of event filtering. The filtering implied by what you pass here is added on top of that filtering. Default: - No additional filtering based on an event pattern.\n')
    rule_name: typing.Optional[str] = pydantic.Field(None, description='A name for the rule. Default: AWS CloudFormation generates a unique physical ID.')
    ...


#  autogenerated from aws_cdk.aws_cloudtrail.AddEventSelectorOptions
class AddEventSelectorOptionsDef(BaseStruct):
    exclude_management_event_sources: typing.Optional[typing.Sequence[aws_cdk.aws_cloudtrail.ManagementEventSources]] = pydantic.Field(None, description='An optional list of service event sources from which you do not want management events to be logged on your trail. Default: []\n')
    include_management_events: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the event selector includes management events for the trail. Default: true\n')
    read_write_type: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='Specifies whether to log read-only events, write-only events, or all events. Default: ReadWriteType.All\n\n:exampleMetadata: infused\n\nExample::\n\n    import aws_cdk.aws_cloudtrail as cloudtrail\n\n    # source_bucket: s3.Bucket\n\n    source_output = codepipeline.Artifact()\n    key = "some/key.zip"\n    trail = cloudtrail.Trail(self, "CloudTrail")\n    trail.add_s3_event_selector([cloudtrail.S3EventSelector(\n        bucket=source_bucket,\n        object_prefix=key\n    )],\n        read_write_type=cloudtrail.ReadWriteType.WRITE_ONLY\n    )\n    source_action = codepipeline_actions.S3SourceAction(\n        action_name="S3Source",\n        bucket_key=key,\n        bucket=source_bucket,\n        output=source_output,\n        trigger=codepipeline_actions.S3Trigger.EVENTS\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['exclude_management_event_sources', 'include_management_events', 'read_write_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.AddEventSelectorOptions'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnChannel.DestinationProperty
class CfnChannel_DestinationPropertyDef(BaseStruct):
    location: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='For channels used for a CloudTrail Lake integration, the location is the ARN of an event data store that receives events from a channel. For service-linked channels, the location is the name of the AWS service.\n')
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of destination for events arriving from a channel. For channels used for a CloudTrail Lake integration, the value is ``EventDataStore`` . For service-linked channels, the value is ``AWS_SERVICE`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-channel-destination.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    destination_property = cloudtrail.CfnChannel.DestinationProperty(\n        location="location",\n        type="type"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['location', 'type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnChannel.DestinationProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnEventDataStore.AdvancedEventSelectorProperty
class CfnEventDataStore_AdvancedEventSelectorPropertyDef(BaseStruct):
    field_selectors: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnEventDataStore_AdvancedFieldSelectorPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Contains all selector statements in an advanced event selector.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='An optional, descriptive name for an advanced event selector, such as "Log data events for only two S3 buckets".\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-eventdatastore-advancedeventselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    advanced_event_selector_property = cloudtrail.CfnEventDataStore.AdvancedEventSelectorProperty(\n        field_selectors=[cloudtrail.CfnEventDataStore.AdvancedFieldSelectorProperty(\n            field="field",\n\n            # the properties below are optional\n            ends_with=["endsWith"],\n            equal_to=["equalTo"],\n            not_ends_with=["notEndsWith"],\n            not_equals=["notEquals"],\n            not_starts_with=["notStartsWith"],\n            starts_with=["startsWith"]\n        )],\n\n        # the properties below are optional\n        name="name"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['field_selectors', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnEventDataStore.AdvancedEventSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnEventDataStore.AdvancedFieldSelectorProperty
class CfnEventDataStore_AdvancedFieldSelectorPropertyDef(BaseStruct):
    field: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="A field in a CloudTrail event record on which to filter events to be logged. For event data stores for AWS Config configuration items, Audit Manager evidence, or non- AWS events, the field is used only for selecting events as filtering is not supported. For CloudTrail event records, supported fields include ``readOnly`` , ``eventCategory`` , ``eventSource`` (for management events), ``eventName`` , ``resources.type`` , and ``resources.ARN`` . For event data stores for AWS Config configuration items, Audit Manager evidence, or non- AWS events, the only supported field is ``eventCategory`` . - *``readOnly``* - Optional. Can be set to ``Equals`` a value of ``true`` or ``false`` . If you do not add this field, CloudTrail logs both ``read`` and ``write`` events. A value of ``true`` logs only ``read`` events. A value of ``false`` logs only ``write`` events. - *``eventSource``* - For filtering management events only. This can be set to ``NotEquals`` ``kms.amazonaws.com`` or ``NotEquals`` ``rdsdata.amazonaws.com`` . - *``eventName``* - Can use any operator. You can use it to ﬁlter in or ﬁlter out any data event logged to CloudTrail, such as ``PutBucket`` or ``GetSnapshotBlock`` . You can have multiple values for this ﬁeld, separated by commas. - *``eventCategory``* - This is required and must be set to ``Equals`` . - For CloudTrail event records, the value must be ``Management`` or ``Data`` . - For AWS Config configuration items, the value must be ``ConfigurationItem`` . - For Audit Manager evidence, the value must be ``Evidence`` . - For non- AWS events, the value must be ``ActivityAuditLog`` . - *``resources.type``* - This ﬁeld is required for CloudTrail data events. ``resources.type`` can only use the ``Equals`` operator, and the value can be one of the following: - ``AWS::DynamoDB::Table`` - ``AWS::Lambda::Function`` - ``AWS::S3::Object`` - ``AWS::CloudTrail::Channel`` - ``AWS::CodeWhisperer::Customization`` - ``AWS::CodeWhisperer::Profile`` - ``AWS::Cognito::IdentityPool`` - ``AWS::DynamoDB::Stream`` - ``AWS::EC2::Snapshot`` - ``AWS::EMRWAL::Workspace`` - ``AWS::FinSpace::Environment`` - ``AWS::Glue::Table`` - ``AWS::GuardDuty::Detector`` - ``AWS::KendraRanking::ExecutionPlan`` - ``AWS::KinesisVideo::Stream`` - ``AWS::ManagedBlockchain::Network`` - ``AWS::ManagedBlockchain::Node`` - ``AWS::MedicalImaging::Datastore`` - ``AWS::PCAConnectorAD::Connector`` - ``AWS::SageMaker::Endpoint`` - ``AWS::SageMaker::ExperimentTrialComponent`` - ``AWS::SageMaker::FeatureGroup`` - ``AWS::SNS::PlatformEndpoint`` - ``AWS::SNS::Topic`` - ``AWS::S3::AccessPoint`` - ``AWS::S3ObjectLambda::AccessPoint`` - ``AWS::S3Outposts::Object`` - ``AWS::SSMMessages::ControlChannel`` - ``AWS::Timestream::Database`` - ``AWS::Timestream::Table`` - ``AWS::VerifiedPermissions::PolicyStore`` You can have only one ``resources.type`` ﬁeld per selector. To log data events on more than one resource type, add another selector. - *``resources.ARN``* - You can use any operator with ``resources.ARN`` , but if you use ``Equals`` or ``NotEquals`` , the value must exactly match the ARN of a valid resource of the type you've speciﬁed in the template as the value of resources.type. For example, if resources.type equals ``AWS::S3::Object`` , the ARN must be in one of the following formats. To log all data events for all objects in a specific S3 bucket, use the ``StartsWith`` operator, and include only the bucket ARN as the matching value. The trailing slash is intentional; do not exclude it. Replace the text between less than and greater than symbols (<>) with resource-specific information. - ``arn:<partition>:s3:::<bucket_name>/`` - ``arn:<partition>:s3:::<bucket_name>/<object_path>/`` When resources.type equals ``AWS::DynamoDB::Table`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:dynamodb:<region>:<account_ID>:table/<table_name>`` When resources.type equals ``AWS::Lambda::Function`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:lambda:<region>:<account_ID>:function:<function_name>`` When resources.type equals ``AWS::CloudTrail::Channel`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:cloudtrail:<region>:<account_ID>:channel/<channel_UUID>`` When resources.type equals ``AWS::CodeWhisperer::Customization`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:codewhisperer:<region>:<account_ID>:customization/<customization_ID>`` When resources.type equals ``AWS::CodeWhisperer::Profile`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:codewhisperer:<region>:<account_ID>:profile/<profile_ID>`` When resources.type equals ``AWS::Cognito::IdentityPool`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:cognito-identity:<region>:<account_ID>:identitypool/<identity_pool_ID>`` When ``resources.type`` equals ``AWS::DynamoDB::Stream`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:dynamodb:<region>:<account_ID>:table/<table_name>/stream/<date_time>`` When ``resources.type`` equals ``AWS::EC2::Snapshot`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:ec2:<region>::snapshot/<snapshot_ID>`` When ``resources.type`` equals ``AWS::EMRWAL::Workspace`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:emrwal:<region>::workspace/<workspace_name>`` When ``resources.type`` equals ``AWS::FinSpace::Environment`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:finspace:<region>:<account_ID>:environment/<environment_ID>`` When ``resources.type`` equals ``AWS::Glue::Table`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:glue:<region>:<account_ID>:table/<database_name>/<table_name>`` When ``resources.type`` equals ``AWS::GuardDuty::Detector`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:guardduty:<region>:<account_ID>:detector/<detector_ID>`` When ``resources.type`` equals ``AWS::KendraRanking::ExecutionPlan`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:kendra-ranking:<region>:<account_ID>:rescore-execution-plan/<rescore_execution_plan_ID>`` When ``resources.type`` equals ``AWS::KinesisVideo::Stream`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:kinesisvideo:<region>:<account_ID>:stream/<stream_name/<creation_time>`` When ``resources.type`` equals ``AWS::ManagedBlockchain::Network`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:managedblockchain:::networks/<network_name>`` When ``resources.type`` equals ``AWS::ManagedBlockchain::Node`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:managedblockchain:<region>:<account_ID>:nodes/<node_ID>`` When ``resources.type`` equals ``AWS::MedicalImaging::Datastore`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:medical-imaging:<region>:<account_ID>:datastore/<data_store_ID>`` When ``resources.type`` equals ``AWS::PCAConnectorAD::Connector`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:pca-connector-ad:<region>:<account_ID>:connector/<connector_ID>`` When ``resources.type`` equals ``AWS::SageMaker::Endpoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sagemaker:<region>:<account_ID>:endpoint/<endpoint_name>`` When ``resources.type`` equals ``AWS::SageMaker::ExperimentTrialComponent`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sagemaker:<region>:<account_ID>:experiment-trial-component/<experiment_trial_component_name>`` When ``resources.type`` equals ``AWS::SageMaker::FeatureGroup`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sagemaker:<region>:<account_ID>:feature-group/<feature_group_name>`` When ``resources.type`` equals ``AWS::SNS::PlatformEndpoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sns:<region>:<account_ID>:endpoint/<endpoint_type>/<endpoint_name>/<endpoint_ID>`` When ``resources.type`` equals ``AWS::SNS::Topic`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sns:<region>:<account_ID>:<topic_name>`` When ``resources.type`` equals ``AWS::S3::AccessPoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in one of the following formats. To log events on all objects in an S3 access point, we recommend that you use only the access point ARN, don’t include the object path, and use the ``StartsWith`` or ``NotStartsWith`` operators. - ``arn:<partition>:s3:<region>:<account_ID>:accesspoint/<access_point_name>`` - ``arn:<partition>:s3:<region>:<account_ID>:accesspoint/<access_point_name>/object/<object_path>`` When ``resources.type`` equals ``AWS::S3ObjectLambda::AccessPoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:s3-object-lambda:<region>:<account_ID>:accesspoint/<access_point_name>`` When ``resources.type`` equals ``AWS::S3Outposts::Object`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:s3-outposts:<region>:<account_ID>:<object_path>`` When ``resources.type`` equals ``AWS::SSMMessages::ControlChannel`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:ssmmessages:<region>:<account_ID>:control-channel/<channel_ID>`` When ``resources.type`` equals ``AWS::Timestream::Database`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:timestream:<region>:<account_ID>:database/<database_name>`` When ``resources.type`` equals ``AWS::Timestream::Table`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:timestream:<region>:<account_ID>:database/<database_name>/table/<table_name>`` When resources.type equals ``AWS::VerifiedPermissions::PolicyStore`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:verifiedpermissions:<region>:<account_ID>:policy-store/<policy_store_UUID>``\n")
    ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that includes events that match the last few characters of the event record field specified as the value of ``Field`` .\n')
    equal_to: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that includes events that match the exact value of the event record field specified as the value of ``Field`` . This is the only valid operator that you can use with the ``readOnly`` , ``eventCategory`` , and ``resources.type`` fields.\n')
    not_ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that excludes events that match the last few characters of the event record field specified as the value of ``Field`` .\n')
    not_equals: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that excludes events that match the exact value of the event record field specified as the value of ``Field`` .\n')
    not_starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that excludes events that match the first few characters of the event record field specified as the value of ``Field`` .\n')
    starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that includes events that match the first few characters of the event record field specified as the value of ``Field`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-eventdatastore-advancedfieldselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    advanced_field_selector_property = cloudtrail.CfnEventDataStore.AdvancedFieldSelectorProperty(\n        field="field",\n\n        # the properties below are optional\n        ends_with=["endsWith"],\n        equal_to=["equalTo"],\n        not_ends_with=["notEndsWith"],\n        not_equals=["notEquals"],\n        not_starts_with=["notStartsWith"],\n        starts_with=["startsWith"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['field', 'ends_with', 'equal_to', 'not_ends_with', 'not_equals', 'not_starts_with', 'starts_with']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnEventDataStore.AdvancedFieldSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnTrail.AdvancedEventSelectorProperty
class CfnTrail_AdvancedEventSelectorPropertyDef(BaseStruct):
    field_selectors: typing.Union[_REQUIRED_INIT_PARAM, models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_AdvancedFieldSelectorPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(REQUIRED_INIT_PARAM, description='Contains all selector statements in an advanced event selector.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='An optional, descriptive name for an advanced event selector, such as "Log data events for only two S3 buckets".\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-trail-advancedeventselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    advanced_event_selector_property = cloudtrail.CfnTrail.AdvancedEventSelectorProperty(\n        field_selectors=[cloudtrail.CfnTrail.AdvancedFieldSelectorProperty(\n            field="field",\n\n            # the properties below are optional\n            ends_with=["endsWith"],\n            equal_to=["equalTo"],\n            not_ends_with=["notEndsWith"],\n            not_equals=["notEquals"],\n            not_starts_with=["notStartsWith"],\n            starts_with=["startsWith"]\n        )],\n\n        # the properties below are optional\n        name="name"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['field_selectors', 'name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnTrail.AdvancedEventSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnTrail.AdvancedFieldSelectorProperty
class CfnTrail_AdvancedFieldSelectorPropertyDef(BaseStruct):
    field: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description="A field in a CloudTrail event record on which to filter events to be logged. For event data stores for AWS Config configuration items, Audit Manager evidence, or non- AWS events, the field is used only for selecting events as filtering is not supported. For CloudTrail event records, supported fields include ``readOnly`` , ``eventCategory`` , ``eventSource`` (for management events), ``eventName`` , ``resources.type`` , and ``resources.ARN`` . For event data stores for AWS Config configuration items, Audit Manager evidence, or non- AWS events, the only supported field is ``eventCategory`` . - *``readOnly``* - Optional. Can be set to ``Equals`` a value of ``true`` or ``false`` . If you do not add this field, CloudTrail logs both ``read`` and ``write`` events. A value of ``true`` logs only ``read`` events. A value of ``false`` logs only ``write`` events. - *``eventSource``* - For filtering management events only. This can be set to ``NotEquals`` ``kms.amazonaws.com`` or ``NotEquals`` ``rdsdata.amazonaws.com`` . - *``eventName``* - Can use any operator. You can use it to ﬁlter in or ﬁlter out any data event logged to CloudTrail, such as ``PutBucket`` or ``GetSnapshotBlock`` . You can have multiple values for this ﬁeld, separated by commas. - *``eventCategory``* - This is required and must be set to ``Equals`` . - For CloudTrail event records, the value must be ``Management`` or ``Data`` . - For AWS Config configuration items, the value must be ``ConfigurationItem`` . - For Audit Manager evidence, the value must be ``Evidence`` . - For non- AWS events, the value must be ``ActivityAuditLog`` . - *``resources.type``* - This ﬁeld is required for CloudTrail data events. ``resources.type`` can only use the ``Equals`` operator, and the value can be one of the following: - ``AWS::DynamoDB::Table`` - ``AWS::Lambda::Function`` - ``AWS::S3::Object`` - ``AWS::CloudTrail::Channel`` - ``AWS::CodeWhisperer::Customization`` - ``AWS::CodeWhisperer::Profile`` - ``AWS::Cognito::IdentityPool`` - ``AWS::DynamoDB::Stream`` - ``AWS::EC2::Snapshot`` - ``AWS::EMRWAL::Workspace`` - ``AWS::FinSpace::Environment`` - ``AWS::Glue::Table`` - ``AWS::GuardDuty::Detector`` - ``AWS::KendraRanking::ExecutionPlan`` - ``AWS::KinesisVideo::Stream`` - ``AWS::ManagedBlockchain::Network`` - ``AWS::ManagedBlockchain::Node`` - ``AWS::MedicalImaging::Datastore`` - ``AWS::PCAConnectorAD::Connector`` - ``AWS::SageMaker::Endpoint`` - ``AWS::SageMaker::ExperimentTrialComponent`` - ``AWS::SageMaker::FeatureGroup`` - ``AWS::SNS::PlatformEndpoint`` - ``AWS::SNS::Topic`` - ``AWS::S3::AccessPoint`` - ``AWS::S3ObjectLambda::AccessPoint`` - ``AWS::S3Outposts::Object`` - ``AWS::SSMMessages::ControlChannel`` - ``AWS::Timestream::Database`` - ``AWS::Timestream::Table`` - ``AWS::VerifiedPermissions::PolicyStore`` You can have only one ``resources.type`` ﬁeld per selector. To log data events on more than one resource type, add another selector. - *``resources.ARN``* - You can use any operator with ``resources.ARN`` , but if you use ``Equals`` or ``NotEquals`` , the value must exactly match the ARN of a valid resource of the type you've speciﬁed in the template as the value of resources.type. For example, if resources.type equals ``AWS::S3::Object`` , the ARN must be in one of the following formats. To log all data events for all objects in a specific S3 bucket, use the ``StartsWith`` operator, and include only the bucket ARN as the matching value. The trailing slash is intentional; do not exclude it. Replace the text between less than and greater than symbols (<>) with resource-specific information. - ``arn:<partition>:s3:::<bucket_name>/`` - ``arn:<partition>:s3:::<bucket_name>/<object_path>/`` When resources.type equals ``AWS::DynamoDB::Table`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:dynamodb:<region>:<account_ID>:table/<table_name>`` When resources.type equals ``AWS::Lambda::Function`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:lambda:<region>:<account_ID>:function:<function_name>`` When resources.type equals ``AWS::CloudTrail::Channel`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:cloudtrail:<region>:<account_ID>:channel/<channel_UUID>`` When resources.type equals ``AWS::CodeWhisperer::Customization`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:codewhisperer:<region>:<account_ID>:customization/<customization_ID>`` When resources.type equals ``AWS::CodeWhisperer::Profile`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:codewhisperer:<region>:<account_ID>:profile/<profile_ID>`` When resources.type equals ``AWS::Cognito::IdentityPool`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:cognito-identity:<region>:<account_ID>:identitypool/<identity_pool_ID>`` When ``resources.type`` equals ``AWS::DynamoDB::Stream`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:dynamodb:<region>:<account_ID>:table/<table_name>/stream/<date_time>`` When ``resources.type`` equals ``AWS::EC2::Snapshot`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:ec2:<region>::snapshot/<snapshot_ID>`` When ``resources.type`` equals ``AWS::EMRWAL::Workspace`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:emrwal:<region>::workspace/<workspace_name>`` When ``resources.type`` equals ``AWS::FinSpace::Environment`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:finspace:<region>:<account_ID>:environment/<environment_ID>`` When ``resources.type`` equals ``AWS::Glue::Table`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:glue:<region>:<account_ID>:table/<database_name>/<table_name>`` When ``resources.type`` equals ``AWS::GuardDuty::Detector`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:guardduty:<region>:<account_ID>:detector/<detector_ID>`` When ``resources.type`` equals ``AWS::KendraRanking::ExecutionPlan`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:kendra-ranking:<region>:<account_ID>:rescore-execution-plan/<rescore_execution_plan_ID>`` When ``resources.type`` equals ``AWS::KinesisVideo::Stream`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:kinesisvideo:<region>:<account_ID>:stream/<stream_name/<creation_time>`` When ``resources.type`` equals ``AWS::ManagedBlockchain::Network`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:managedblockchain:::networks/<network_name>`` When ``resources.type`` equals ``AWS::ManagedBlockchain::Node`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:managedblockchain:<region>:<account_ID>:nodes/<node_ID>`` When ``resources.type`` equals ``AWS::MedicalImaging::Datastore`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:medical-imaging:<region>:<account_ID>:datastore/<data_store_ID>`` When ``resources.type`` equals ``AWS::PCAConnectorAD::Connector`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:pca-connector-ad:<region>:<account_ID>:connector/<connector_ID>`` When ``resources.type`` equals ``AWS::SageMaker::Endpoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sagemaker:<region>:<account_ID>:endpoint/<endpoint_name>`` When ``resources.type`` equals ``AWS::SageMaker::ExperimentTrialComponent`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sagemaker:<region>:<account_ID>:experiment-trial-component/<experiment_trial_component_name>`` When ``resources.type`` equals ``AWS::SageMaker::FeatureGroup`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sagemaker:<region>:<account_ID>:feature-group/<feature_group_name>`` When ``resources.type`` equals ``AWS::SNS::PlatformEndpoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sns:<region>:<account_ID>:endpoint/<endpoint_type>/<endpoint_name>/<endpoint_ID>`` When ``resources.type`` equals ``AWS::SNS::Topic`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:sns:<region>:<account_ID>:<topic_name>`` When ``resources.type`` equals ``AWS::S3::AccessPoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in one of the following formats. To log events on all objects in an S3 access point, we recommend that you use only the access point ARN, don’t include the object path, and use the ``StartsWith`` or ``NotStartsWith`` operators. - ``arn:<partition>:s3:<region>:<account_ID>:accesspoint/<access_point_name>`` - ``arn:<partition>:s3:<region>:<account_ID>:accesspoint/<access_point_name>/object/<object_path>`` When ``resources.type`` equals ``AWS::S3ObjectLambda::AccessPoint`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:s3-object-lambda:<region>:<account_ID>:accesspoint/<access_point_name>`` When ``resources.type`` equals ``AWS::S3Outposts::Object`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:s3-outposts:<region>:<account_ID>:<object_path>`` When ``resources.type`` equals ``AWS::SSMMessages::ControlChannel`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:ssmmessages:<region>:<account_ID>:control-channel/<channel_ID>`` When ``resources.type`` equals ``AWS::Timestream::Database`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:timestream:<region>:<account_ID>:database/<database_name>`` When ``resources.type`` equals ``AWS::Timestream::Table`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:timestream:<region>:<account_ID>:database/<database_name>/table/<table_name>`` When resources.type equals ``AWS::VerifiedPermissions::PolicyStore`` , and the operator is set to ``Equals`` or ``NotEquals`` , the ARN must be in the following format: - ``arn:<partition>:verifiedpermissions:<region>:<account_ID>:policy-store/<policy_store_UUID>``\n")
    ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that includes events that match the last few characters of the event record field specified as the value of ``Field`` .\n')
    equal_to: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that includes events that match the exact value of the event record field specified as the value of ``Field`` . This is the only valid operator that you can use with the ``readOnly`` , ``eventCategory`` , and ``resources.type`` fields.\n')
    not_ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that excludes events that match the last few characters of the event record field specified as the value of ``Field`` .\n')
    not_equals: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that excludes events that match the exact value of the event record field specified as the value of ``Field`` .\n')
    not_starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that excludes events that match the first few characters of the event record field specified as the value of ``Field`` .\n')
    starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An operator that includes events that match the first few characters of the event record field specified as the value of ``Field`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-trail-advancedfieldselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    advanced_field_selector_property = cloudtrail.CfnTrail.AdvancedFieldSelectorProperty(\n        field="field",\n\n        # the properties below are optional\n        ends_with=["endsWith"],\n        equal_to=["equalTo"],\n        not_ends_with=["notEndsWith"],\n        not_equals=["notEquals"],\n        not_starts_with=["notStartsWith"],\n        starts_with=["startsWith"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['field', 'ends_with', 'equal_to', 'not_ends_with', 'not_equals', 'not_starts_with', 'starts_with']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnTrail.AdvancedFieldSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnTrail.DataResourceProperty
class CfnTrail_DataResourcePropertyDef(BaseStruct):
    type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The resource type in which you want to log data events. You can specify the following *basic* event selector resource types: - ``AWS::DynamoDB::Table`` - ``AWS::Lambda::Function`` - ``AWS::S3::Object`` The following resource types are also available through *advanced* event selectors. Basic event selector resource types are valid in advanced event selectors, but advanced event selector resource types are not valid in basic event selectors. For more information, see `AdvancedFieldSelector <https://docs.aws.amazon.com/awscloudtrail/latest/APIReference/API_AdvancedFieldSelector.html>`_ . - ``AWS::CloudTrail::Channel`` - ``AWS::CodeWhisperer::Customization`` - ``AWS::CodeWhisperer::Profile`` - ``AWS::Cognito::IdentityPool`` - ``AWS::DynamoDB::Stream`` - ``AWS::EC2::Snapshot`` - ``AWS::EMRWAL::Workspace`` - ``AWS::FinSpace::Environment`` - ``AWS::Glue::Table`` - ``AWS::GuardDuty::Detector`` - ``AWS::KendraRanking::ExecutionPlan`` - ``AWS::KinesisVideo::Stream`` - ``AWS::ManagedBlockchain::Network`` - ``AWS::ManagedBlockchain::Node`` - ``AWS::MedicalImaging::Datastore`` - ``AWS::PCAConnectorAD::Connector`` - ``AWS::SageMaker::Endpoint`` - ``AWS::SageMaker::ExperimentTrialComponent`` - ``AWS::SageMaker::FeatureGroup`` - ``AWS::SNS::PlatformEndpoint`` - ``AWS::SNS::Topic`` - ``AWS::S3::AccessPoint`` - ``AWS::S3ObjectLambda::AccessPoint`` - ``AWS::S3Outposts::Object`` - ``AWS::SSMMessages::ControlChannel`` - ``AWS::Timestream::Database`` - ``AWS::Timestream::Table`` - ``AWS::VerifiedPermissions::PolicyStore``\n')
    values: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An array of Amazon Resource Name (ARN) strings or partial ARN strings for the specified objects. - To log data events for all objects in all S3 buckets in your AWS account , specify the prefix as ``arn:aws:s3`` . .. epigraph:: This also enables logging of data event activity performed by any user or role in your AWS account , even if that activity is performed on a bucket that belongs to another AWS account . - To log data events for all objects in an S3 bucket, specify the bucket and an empty object prefix such as ``arn:aws:s3:::bucket-1/`` . The trail logs data events for all objects in this S3 bucket. - To log data events for specific objects, specify the S3 bucket and object prefix such as ``arn:aws:s3:::bucket-1/example-images`` . The trail logs data events for objects in this S3 bucket that match the prefix. - To log data events for all Lambda functions in your AWS account , specify the prefix as ``arn:aws:lambda`` . .. epigraph:: This also enables logging of ``Invoke`` activity performed by any user or role in your AWS account , even if that activity is performed on a function that belongs to another AWS account . - To log data events for a specific Lambda function, specify the function ARN. .. epigraph:: Lambda function ARNs are exact. For example, if you specify a function ARN *arn:aws:lambda:us-west-2:111111111111:function:helloworld* , data events will only be logged for *arn:aws:lambda:us-west-2:111111111111:function:helloworld* . They will not be logged for *arn:aws:lambda:us-west-2:111111111111:function:helloworld2* . - To log data events for all DynamoDB tables in your AWS account , specify the prefix as ``arn:aws:dynamodb`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-trail-dataresource.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    data_resource_property = cloudtrail.CfnTrail.DataResourceProperty(\n        type="type",\n\n        # the properties below are optional\n        values=["values"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['type', 'values']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnTrail.DataResourceProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnTrail.EventSelectorProperty
class CfnTrail_EventSelectorPropertyDef(BaseStruct):
    data_resources: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_DataResourcePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='CloudTrail supports data event logging for Amazon S3 objects, AWS Lambda functions, and Amazon DynamoDB tables with basic event selectors. You can specify up to 250 resources for an individual event selector, but the total number of data resources cannot exceed 250 across all event selectors in a trail. This limit does not apply if you configure resource logging for all data events. For more information, see `Data Events <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html>`_ and `Limits in AWS CloudTrail <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/WhatIsCloudTrail-Limits.html>`_ in the *AWS CloudTrail User Guide* .\n')
    exclude_management_event_sources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='An optional list of service event sources from which you do not want management events to be logged on your trail. In this release, the list can be empty (disables the filter), or it can filter out AWS Key Management Service or Amazon RDS Data API events by containing ``kms.amazonaws.com`` or ``rdsdata.amazonaws.com`` . By default, ``ExcludeManagementEventSources`` is empty, and AWS KMS and Amazon RDS Data API events are logged to your trail. You can exclude management event sources only in Regions that support the event source.\n')
    include_management_events: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specify if you want your event selector to include management events for your trail. For more information, see `Management Events <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-management-events-with-cloudtrail.html>`_ in the *AWS CloudTrail User Guide* . By default, the value is ``true`` . The first copy of management events is free. You are charged for additional copies of management events that you are logging on any subsequent trail in the same Region. For more information about CloudTrail pricing, see `AWS CloudTrail Pricing <https://docs.aws.amazon.com/cloudtrail/pricing/>`_ .\n')
    read_write_type: typing.Optional[str] = pydantic.Field(None, description='Specify if you want your trail to log read-only events, write-only events, or all. For example, the EC2 ``GetConsoleOutput`` is a read-only API operation and ``RunInstances`` is a write-only API operation. By default, the value is ``All`` .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-trail-eventselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    event_selector_property = cloudtrail.CfnTrail.EventSelectorProperty(\n        data_resources=[cloudtrail.CfnTrail.DataResourceProperty(\n            type="type",\n\n            # the properties below are optional\n            values=["values"]\n        )],\n        exclude_management_event_sources=["excludeManagementEventSources"],\n        include_management_events=False,\n        read_write_type="readWriteType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_resources', 'exclude_management_event_sources', 'include_management_events', 'read_write_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnTrail.EventSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnTrail.InsightSelectorProperty
class CfnTrail_InsightSelectorPropertyDef(BaseStruct):
    insight_type: typing.Optional[str] = pydantic.Field(None, description='The type of Insights events to log on a trail. ``ApiCallRateInsight`` and ``ApiErrorRateInsight`` are valid Insight types. The ``ApiCallRateInsight`` Insights type analyzes write-only management API calls that are aggregated per minute against a baseline API call volume. The ``ApiErrorRateInsight`` Insights type analyzes management API calls that result in error codes. The error is shown if the API call is unsuccessful.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-cloudtrail-trail-insightselector.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    insight_selector_property = cloudtrail.CfnTrail.InsightSelectorProperty(\n        insight_type="insightType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['insight_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnTrail.InsightSelectorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.S3EventSelector
class S3EventSelectorDef(BaseStruct):
    bucket: typing.Union[_REQUIRED_INIT_PARAM, models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef] = pydantic.Field(REQUIRED_INIT_PARAM, description='S3 bucket.\n')
    object_prefix: typing.Optional[str] = pydantic.Field(None, description='Data events for objects whose key matches this prefix will be logged. Default: - all objects\n\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n    from aws_cdk import aws_s3 as s3\n\n    # bucket: s3.Bucket\n\n    s3_event_selector = cloudtrail.S3EventSelector(\n        bucket=bucket,\n\n        # the properties below are optional\n        object_prefix="objectPrefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'object_prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.S3EventSelector'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[S3EventSelectorDefConfig] = pydantic.Field(None)


class S3EventSelectorDefConfig(pydantic.BaseModel):
    bucket_config: typing.Optional[models._interface_methods.AwsS3IBucketDefConfig] = pydantic.Field(None)


#  autogenerated from aws_cdk.aws_cloudtrail.TrailProps
class TrailPropsDef(BaseStruct):
    bucket: typing.Optional[typing.Union[models.aws_s3.BucketBaseDef, models.aws_s3.BucketDef]] = pydantic.Field(None, description='The Amazon S3 bucket. Default: - if not supplied a bucket will be created with all the correct permisions\n')
    cloud_watch_log_group: typing.Optional[typing.Union[models.aws_logs.LogGroupDef]] = pydantic.Field(None, description='Log Group to which CloudTrail to push logs to. Ignored if sendToCloudWatchLogs is set to false. Default: - a new log group is created and used.\n')
    cloud_watch_logs_retention: typing.Optional[aws_cdk.aws_logs.RetentionDays] = pydantic.Field(None, description='How long to retain logs in CloudWatchLogs. Ignored if sendToCloudWatchLogs is false or if cloudWatchLogGroup is set. Default: logs.RetentionDays.ONE_YEAR\n')
    enable_file_validation: typing.Optional[bool] = pydantic.Field(None, description='To determine whether a log file was modified, deleted, or unchanged after CloudTrail delivered it, you can use CloudTrail log file integrity validation. This feature is built using industry standard algorithms: SHA-256 for hashing and SHA-256 with RSA for digital signing. This makes it computationally infeasible to modify, delete or forge CloudTrail log files without detection. You can use the AWS CLI to validate the files in the location where CloudTrail delivered them. Default: true\n')
    encryption_key: typing.Optional[typing.Union[models.aws_kms.KeyDef]] = pydantic.Field(None, description='The AWS Key Management Service (AWS KMS) key ID that you want to use to encrypt CloudTrail logs. Default: - No encryption.\n')
    include_global_service_events: typing.Optional[bool] = pydantic.Field(None, description='For most services, events are recorded in the region where the action occurred. For global services such as AWS Identity and Access Management (IAM), AWS STS, Amazon CloudFront, and Route 53, events are delivered to any trail that includes global services, and are logged as occurring in US East (N. Virginia) Region. Default: true\n')
    insight_types: typing.Optional[typing.Sequence[models.aws_cloudtrail.InsightTypeDef]] = pydantic.Field(None, description='A JSON string that contains the insight types you want to log on a trail. Default: - No Value.\n')
    is_multi_region_trail: typing.Optional[bool] = pydantic.Field(None, description='Whether or not this trail delivers log files from multiple regions to a single S3 bucket for a single account. Default: true\n')
    is_organization_trail: typing.Optional[bool] = pydantic.Field(None, description='Specifies whether the trail is applied to all accounts in an organization in AWS Organizations, or only for the current AWS account. If this is set to true then the current account *must* be the management account. If it is not, then CloudFormation will throw an error. If this is set to true and the current account is a management account for an organization in AWS Organizations, the trail will be created in all AWS accounts that belong to the organization. If this is set to false, the trail will remain in the current AWS account but be deleted from all member accounts in the organization. Default: - false\n')
    management_events: typing.Optional[aws_cdk.aws_cloudtrail.ReadWriteType] = pydantic.Field(None, description='When an event occurs in your account, CloudTrail evaluates whether the event matches the settings for your trails. Only events that match your trail settings are delivered to your Amazon S3 bucket and Amazon CloudWatch Logs log group. This method sets the management configuration for this trail. Management events provide insight into management operations that are performed on resources in your AWS account. These are also known as control plane operations. Management events can also include non-API events that occur in your account. For example, when a user logs in to your account, CloudTrail logs the ConsoleLogin event. Default: ReadWriteType.ALL\n')
    s3_key_prefix: typing.Optional[str] = pydantic.Field(None, description='An Amazon S3 object key prefix that precedes the name of all log files. Default: - No prefix.\n')
    send_to_cloud_watch_logs: typing.Optional[bool] = pydantic.Field(None, description='If CloudTrail pushes logs to CloudWatch Logs in addition to S3. Disabled for cost out of the box. Default: false\n')
    sns_topic: typing.Optional[typing.Union[models.aws_sns.TopicBaseDef, models.aws_sns.TopicDef]] = pydantic.Field(None, description='SNS topic that is notified when new log files are published. Default: - No notifications.\n')
    trail_name: typing.Optional[str] = pydantic.Field(None, description='The name of the trail. We recommend customers do not set an explicit name. Default: - AWS CloudFormation generated name.\n\n:exampleMetadata: infused\n\nExample::\n\n    trail = cloudtrail.Trail(self, "CloudTrail",\n        # ...\n        management_events=cloudtrail.ReadWriteType.READ_ONLY\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bucket', 'cloud_watch_log_group', 'cloud_watch_logs_retention', 'enable_file_validation', 'encryption_key', 'include_global_service_events', 'insight_types', 'is_multi_region_trail', 'is_organization_trail', 'management_events', 's3_key_prefix', 'send_to_cloud_watch_logs', 'sns_topic', 'trail_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.TrailProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.DataResourceType
# skipping emum

#  autogenerated from aws_cdk.aws_cloudtrail.ManagementEventSources
# skipping emum

#  autogenerated from aws_cdk.aws_cloudtrail.ReadWriteType
# skipping emum

#  autogenerated from aws_cdk.aws_cloudtrail.CfnChannel
class CfnChannelDef(BaseCfnResource):
    destinations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnChannel_DestinationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more event data stores to which events arriving through a channel will be logged.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the channel.\n')
    source: typing.Optional[str] = pydantic.Field(None, description='The name of the partner or external event source. You cannot change this name after you create the channel. A maximum of one channel is allowed per source. A source can be either ``Custom`` for all valid non- AWS events, or the name of a partner event source. For information about the source names for available partners, see `Additional information about integration partners <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/query-event-data-store-integration.html#cloudtrail-lake-partner-information>`_ in the CloudTrail User Guide.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of tags.')
    _init_params: typing.ClassVar[list[str]] = ['destinations', 'name', 'source', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['DestinationProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnChannel'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnChannelDefConfig] = pydantic.Field(None)


class CfnChannelDefConfig(pydantic.BaseModel):
    DestinationProperty: typing.Optional[list[CfnChannelDefDestinationpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnChannelDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnChannelDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnChannelDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnChannelDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnChannelDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnChannelDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnChannelDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnChannelDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnChannelDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnChannelDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnChannelDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnChannelDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnChannelDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnChannelDefDestinationpropertyParams(pydantic.BaseModel):
    location: str = pydantic.Field(..., description='')
    type: str = pydantic.Field(..., description='')
    ...

class CfnChannelDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnChannelDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnChannelDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnChannelDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnChannelDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnChannelDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnChannelDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnChannelDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnChannelDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnChannelDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnChannelDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnChannelDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnChannelDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnChannelDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_cloudtrail.CfnEventDataStore
class CfnEventDataStoreDef(BaseCfnResource):
    advanced_event_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnEventDataStore_AdvancedEventSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The advanced event selectors to use to select the events for the data store. You can configure up to five advanced event selectors for each event data store. For more information about how to use advanced event selectors to log CloudTrail events, see `Log events by using advanced event selectors <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html#creating-data-event-selectors-advanced>`_ in the CloudTrail User Guide. For more information about how to use advanced event selectors to include AWS Config configuration items in your event data store, see `Create an event data store for AWS Config configuration items <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/query-lake-cli.html#lake-cli-create-eds-config>`_ in the CloudTrail User Guide. For more information about how to use advanced event selectors to include non- AWS events in your event data store, see `Create an integration to log events from outside AWS <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/query-lake-cli.html#lake-cli-create-integration>`_ in the CloudTrail User Guide.\n')
    ingestion_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the event data store should start ingesting live events. The default is true.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the AWS KMS key ID to use to encrypt the events delivered by CloudTrail. The value can be an alias name prefixed by ``alias/`` , a fully specified ARN to an alias, a fully specified ARN to a key, or a globally unique identifier. .. epigraph:: Disabling or deleting the KMS key, or removing CloudTrail permissions on the key, prevents CloudTrail from logging events to the event data store, and prevents users from querying the data in the event data store that was encrypted with the key. After you associate an event data store with a KMS key, the KMS key cannot be removed or changed. Before you disable or delete a KMS key that you are using with an event data store, delete or back up your event data store. CloudTrail also supports AWS KMS multi-Region keys. For more information about multi-Region keys, see `Using multi-Region keys <https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html>`_ in the *AWS Key Management Service Developer Guide* . Examples: - ``alias/MyAliasName`` - ``arn:aws:kms:us-east-2:123456789012:alias/MyAliasName`` - ``arn:aws:kms:us-east-2:123456789012:key/12345678-1234-1234-1234-123456789012`` - ``12345678-1234-1234-1234-123456789012``\n')
    multi_region_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the event data store includes events from all Regions, or only from the Region in which the event data store is created.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the event data store.\n')
    organization_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether an event data store collects events logged for an organization in AWS Organizations .\n')
    retention_period: typing.Union[int, float, None] = pydantic.Field(None, description='The retention period of the event data store, in days. You can set a retention period of up to 2557 days, the equivalent of seven years.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of tags.\n')
    termination_protection_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether termination protection is enabled for the event data store. If termination protection is enabled, you cannot delete the event data store until termination protection is disabled.')
    _init_params: typing.ClassVar[list[str]] = ['advanced_event_selectors', 'ingestion_enabled', 'kms_key_id', 'multi_region_enabled', 'name', 'organization_enabled', 'retention_period', 'tags', 'termination_protection_enabled']
    _method_names: typing.ClassVar[list[str]] = ['AdvancedEventSelectorProperty', 'AdvancedFieldSelectorProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnEventDataStore'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnEventDataStoreDefConfig] = pydantic.Field(None)


class CfnEventDataStoreDefConfig(pydantic.BaseModel):
    AdvancedEventSelectorProperty: typing.Optional[list[CfnEventDataStoreDefAdvancedeventselectorpropertyParams]] = pydantic.Field(None, description='')
    AdvancedFieldSelectorProperty: typing.Optional[list[CfnEventDataStoreDefAdvancedfieldselectorpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnEventDataStoreDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnEventDataStoreDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnEventDataStoreDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnEventDataStoreDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnEventDataStoreDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnEventDataStoreDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnEventDataStoreDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnEventDataStoreDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnEventDataStoreDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnEventDataStoreDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnEventDataStoreDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnEventDataStoreDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnEventDataStoreDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnEventDataStoreDefAdvancedeventselectorpropertyParams(pydantic.BaseModel):
    field_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnEventDataStore_AdvancedFieldSelectorPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEventDataStoreDefAdvancedfieldselectorpropertyParams(pydantic.BaseModel):
    field: str = pydantic.Field(..., description='')
    ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    equal_to: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    not_ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    not_equals: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    not_starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnEventDataStoreDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnEventDataStoreDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventDataStoreDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnEventDataStoreDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventDataStoreDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnEventDataStoreDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnEventDataStoreDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnEventDataStoreDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnEventDataStoreDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnEventDataStoreDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventDataStoreDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnEventDataStoreDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnEventDataStoreDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventDataStoreDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_cloudtrail.CfnResourcePolicy
class CfnResourcePolicyDef(BaseCfnResource):
    resource_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the CloudTrail channel attached to the resource-based policy. The following is the format of a resource ARN: ``arn:aws:cloudtrail:us-east-2:123456789012:channel/MyChannel`` .\n')
    resource_policy: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A JSON-formatted string for an AWS resource-based policy. The following are requirements for the resource policy: - Contains only one action: cloudtrail-data:PutAuditEvents - Contains at least one statement. The policy can have a maximum of 20 statements. - Each statement contains at least one principal. A statement can have a maximum of 50 principals.')
    _init_params: typing.ClassVar[list[str]] = ['resource_arn', 'resource_policy']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnResourcePolicy'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnResourcePolicyDefConfig] = pydantic.Field(None)


class CfnResourcePolicyDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[CfnResourcePolicyDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnResourcePolicyDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnResourcePolicyDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnResourcePolicyDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnResourcePolicyDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnResourcePolicyDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnResourcePolicyDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnResourcePolicyDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnResourcePolicyDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnResourcePolicyDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnResourcePolicyDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnResourcePolicyDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnResourcePolicyDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnResourcePolicyDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnResourcePolicyDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnResourcePolicyDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnResourcePolicyDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnResourcePolicyDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnResourcePolicyDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnResourcePolicyDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnResourcePolicyDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnResourcePolicyDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnResourcePolicyDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnResourcePolicyDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnResourcePolicyDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnResourcePolicyDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnResourcePolicyDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_cloudtrail.CfnTrail
class CfnTrailDef(BaseCfnResource):
    is_logging: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Whether the CloudTrail trail is currently logging AWS API calls.\n')
    s3_bucket_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the name of the Amazon S3 bucket designated for publishing log files. See `Amazon S3 Bucket Naming Requirements <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create_trail_naming_policy.html>`_ .\n')
    advanced_event_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_AdvancedEventSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the settings for advanced event selectors. You can add advanced event selectors, and conditions for your advanced event selectors, up to a maximum of 500 values for all conditions and selectors on a trail. You can use either ``AdvancedEventSelectors`` or ``EventSelectors`` , but not both. If you apply ``AdvancedEventSelectors`` to a trail, any existing ``EventSelectors`` are overwritten. For more information about advanced event selectors, see `Logging data events <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html>`_ in the *AWS CloudTrail User Guide* .\n')
    cloud_watch_logs_log_group_arn: typing.Optional[str] = pydantic.Field(None, description='Specifies a log group name using an Amazon Resource Name (ARN), a unique identifier that represents the log group to which CloudTrail logs are delivered. You must use a log group that exists in your account. Not required unless you specify ``CloudWatchLogsRoleArn`` . .. epigraph:: Only the management account can configure a CloudWatch Logs log group for an organization trail.\n')
    cloud_watch_logs_role_arn: typing.Optional[str] = pydantic.Field(None, description="Specifies the role for the CloudWatch Logs endpoint to assume to write to a user's log group. You must use a role that exists in your account.\n")
    enable_log_file_validation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether log file validation is enabled. The default is false. .. epigraph:: When you disable log file integrity validation, the chain of digest files is broken after one hour. CloudTrail does not create digest files for log files that were delivered during a period in which log file integrity validation was disabled. For example, if you enable log file integrity validation at noon on January 1, disable it at noon on January 2, and re-enable it at noon on January 10, digest files will not be created for the log files delivered from noon on January 2 to noon on January 10. The same applies whenever you stop CloudTrail logging or delete a trail.\n')
    event_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_EventSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Use event selectors to further specify the management and data event settings for your trail. By default, trails created without specific event selectors will be configured to log all read and write management events, and no data events. When an event occurs in your account, CloudTrail evaluates the event selector for all trails. For each trail, if the event matches any event selector, the trail processes and logs the event. If the event doesn't match any event selector, the trail doesn't log the event. You can configure up to five event selectors for a trail. You cannot apply both event selectors and advanced event selectors to a trail.\n")
    include_global_service_events: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the trail is publishing events from global services such as IAM to the log files.\n')
    insight_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_InsightSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A JSON string that contains the insight types you want to log on a trail. ``ApiCallRateInsight`` and ``ApiErrorRateInsight`` are valid Insight types. The ``ApiCallRateInsight`` Insights type analyzes write-only management API calls that are aggregated per minute against a baseline API call volume. The ``ApiErrorRateInsight`` Insights type analyzes management API calls that result in error codes. The error is shown if the API call is unsuccessful.\n')
    is_multi_region_trail: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the trail applies only to the current Region or to all Regions. The default is false. If the trail exists only in the current Region and this value is set to true, shadow trails (replications of the trail) will be created in the other Regions. If the trail exists in all Regions and this value is set to false, the trail will remain in the Region where it was created, and its shadow trails in other Regions will be deleted. As a best practice, consider using trails that log events in all Regions.\n')
    is_organization_trail: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the trail is applied to all accounts in an organization in AWS Organizations , or only for the current AWS account . The default is false, and cannot be true unless the call is made on behalf of an AWS account that is the management account for an organization in AWS Organizations . If the trail is not an organization trail and this is set to ``true`` , the trail will be created in all AWS accounts that belong to the organization. If the trail is an organization trail and this is set to ``false`` , the trail will remain in the current AWS account but be deleted from all member accounts in the organization. .. epigraph:: Only the management account for the organization can convert an organization trail to a non-organization trail, or convert a non-organization trail to an organization trail.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the AWS KMS key ID to use to encrypt the logs delivered by CloudTrail. The value can be an alias name prefixed by "alias/", a fully specified ARN to an alias, a fully specified ARN to a key, or a globally unique identifier. CloudTrail also supports AWS KMS multi-Region keys. For more information about multi-Region keys, see `Using multi-Region keys <https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html>`_ in the *AWS Key Management Service Developer Guide* . Examples: - alias/MyAliasName - arn:aws:kms:us-east-2:123456789012:alias/MyAliasName - arn:aws:kms:us-east-2:123456789012:key/12345678-1234-1234-1234-123456789012 - 12345678-1234-1234-1234-123456789012\n')
    s3_key_prefix: typing.Optional[str] = pydantic.Field(None, description='Specifies the Amazon S3 key prefix that comes after the name of the bucket you have designated for log file delivery. For more information, see `Finding Your CloudTrail Log Files <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html>`_ . The maximum length is 200 characters.\n')
    sns_topic_name: typing.Optional[str] = pydantic.Field(None, description='Specifies the name of the Amazon SNS topic defined for notification of log file delivery. The maximum length is 256 characters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A custom set of tags (key-value pairs) for this trail.\n')
    trail_name: typing.Optional[str] = pydantic.Field(None, description='Specifies the name of the trail. The name must meet the following requirements:. - Contain only ASCII letters (a-z, A-Z), numbers (0-9), periods (.), underscores (_), or dashes (-) - Start with a letter or number, and end with a letter or number - Be between 3 and 128 characters - Have no adjacent periods, underscores or dashes. Names like ``my-_namespace`` and ``my--namespace`` are not valid. - Not be in IP address format (for example, 192.168.5.4)')
    _init_params: typing.ClassVar[list[str]] = ['is_logging', 's3_bucket_name', 'advanced_event_selectors', 'cloud_watch_logs_log_group_arn', 'cloud_watch_logs_role_arn', 'enable_log_file_validation', 'event_selectors', 'include_global_service_events', 'insight_selectors', 'is_multi_region_trail', 'is_organization_trail', 'kms_key_id', 's3_key_prefix', 'sns_topic_name', 'tags', 'trail_name']
    _method_names: typing.ClassVar[list[str]] = ['AdvancedEventSelectorProperty', 'AdvancedFieldSelectorProperty', 'DataResourceProperty', 'EventSelectorProperty', 'InsightSelectorProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnTrail'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[CfnTrailDefConfig] = pydantic.Field(None)


class CfnTrailDefConfig(pydantic.BaseModel):
    AdvancedEventSelectorProperty: typing.Optional[list[CfnTrailDefAdvancedeventselectorpropertyParams]] = pydantic.Field(None, description='')
    AdvancedFieldSelectorProperty: typing.Optional[list[CfnTrailDefAdvancedfieldselectorpropertyParams]] = pydantic.Field(None, description='')
    DataResourceProperty: typing.Optional[list[CfnTrailDefDataresourcepropertyParams]] = pydantic.Field(None, description='')
    EventSelectorProperty: typing.Optional[list[CfnTrailDefEventselectorpropertyParams]] = pydantic.Field(None, description='')
    InsightSelectorProperty: typing.Optional[list[CfnTrailDefInsightselectorpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[CfnTrailDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[CfnTrailDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[CfnTrailDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[CfnTrailDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[CfnTrailDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[CfnTrailDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[CfnTrailDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[CfnTrailDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[CfnTrailDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[CfnTrailDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[CfnTrailDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[CfnTrailDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[CfnTrailDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnTrailDefAdvancedeventselectorpropertyParams(pydantic.BaseModel):
    field_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_AdvancedFieldSelectorPropertyDef, dict[str, typing.Any]]]] = pydantic.Field(..., description='')
    name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTrailDefAdvancedfieldselectorpropertyParams(pydantic.BaseModel):
    field: str = pydantic.Field(..., description='')
    ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    equal_to: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    not_ends_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    not_equals: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    not_starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    starts_with: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnTrailDefDataresourcepropertyParams(pydantic.BaseModel):
    type: str = pydantic.Field(..., description='')
    values: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnTrailDefEventselectorpropertyParams(pydantic.BaseModel):
    data_resources: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_DataResourcePropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='')
    exclude_management_event_sources: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    include_management_events: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    read_write_type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTrailDefInsightselectorpropertyParams(pydantic.BaseModel):
    insight_type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnTrailDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnTrailDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnTrailDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnTrailDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnTrailDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnTrailDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnTrailDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnTrailDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnTrailDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnTrailDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnTrailDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnTrailDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnTrailDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnTrailDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_cloudtrail.CfnChannelProps
class CfnChannelPropsDef(BaseCfnProperty):
    destinations: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnChannel_DestinationPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='One or more event data stores to which events arriving through a channel will be logged.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the channel.\n')
    source: typing.Optional[str] = pydantic.Field(None, description='The name of the partner or external event source. You cannot change this name after you create the channel. A maximum of one channel is allowed per source. A source can be either ``Custom`` for all valid non- AWS events, or the name of a partner event source. For information about the source names for available partners, see `Additional information about integration partners <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/query-event-data-store-integration.html#cloudtrail-lake-partner-information>`_ in the CloudTrail User Guide.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of tags.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudtrail-channel.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    cfn_channel_props = cloudtrail.CfnChannelProps(\n        destinations=[cloudtrail.CfnChannel.DestinationProperty(\n            location="location",\n            type="type"\n        )],\n        name="name",\n        source="source",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['destinations', 'name', 'source', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnChannelProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnEventDataStoreProps
class CfnEventDataStorePropsDef(BaseCfnProperty):
    advanced_event_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnEventDataStore_AdvancedEventSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='The advanced event selectors to use to select the events for the data store. You can configure up to five advanced event selectors for each event data store. For more information about how to use advanced event selectors to log CloudTrail events, see `Log events by using advanced event selectors <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html#creating-data-event-selectors-advanced>`_ in the CloudTrail User Guide. For more information about how to use advanced event selectors to include AWS Config configuration items in your event data store, see `Create an event data store for AWS Config configuration items <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/query-lake-cli.html#lake-cli-create-eds-config>`_ in the CloudTrail User Guide. For more information about how to use advanced event selectors to include non- AWS events in your event data store, see `Create an integration to log events from outside AWS <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/query-lake-cli.html#lake-cli-create-integration>`_ in the CloudTrail User Guide.\n')
    ingestion_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the event data store should start ingesting live events. The default is true.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the AWS KMS key ID to use to encrypt the events delivered by CloudTrail. The value can be an alias name prefixed by ``alias/`` , a fully specified ARN to an alias, a fully specified ARN to a key, or a globally unique identifier. .. epigraph:: Disabling or deleting the KMS key, or removing CloudTrail permissions on the key, prevents CloudTrail from logging events to the event data store, and prevents users from querying the data in the event data store that was encrypted with the key. After you associate an event data store with a KMS key, the KMS key cannot be removed or changed. Before you disable or delete a KMS key that you are using with an event data store, delete or back up your event data store. CloudTrail also supports AWS KMS multi-Region keys. For more information about multi-Region keys, see `Using multi-Region keys <https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html>`_ in the *AWS Key Management Service Developer Guide* . Examples: - ``alias/MyAliasName`` - ``arn:aws:kms:us-east-2:123456789012:alias/MyAliasName`` - ``arn:aws:kms:us-east-2:123456789012:key/12345678-1234-1234-1234-123456789012`` - ``12345678-1234-1234-1234-123456789012``\n')
    multi_region_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the event data store includes events from all Regions, or only from the Region in which the event data store is created.\n')
    name: typing.Optional[str] = pydantic.Field(None, description='The name of the event data store.\n')
    organization_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether an event data store collects events logged for an organization in AWS Organizations .\n')
    retention_period: typing.Union[int, float, None] = pydantic.Field(None, description='The retention period of the event data store, in days. You can set a retention period of up to 2557 days, the equivalent of seven years.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A list of tags.\n')
    termination_protection_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether termination protection is enabled for the event data store. If termination protection is enabled, you cannot delete the event data store until termination protection is disabled.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudtrail-eventdatastore.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    cfn_event_data_store_props = cloudtrail.CfnEventDataStoreProps(\n        advanced_event_selectors=[cloudtrail.CfnEventDataStore.AdvancedEventSelectorProperty(\n            field_selectors=[cloudtrail.CfnEventDataStore.AdvancedFieldSelectorProperty(\n                field="field",\n\n                # the properties below are optional\n                ends_with=["endsWith"],\n                equal_to=["equalTo"],\n                not_ends_with=["notEndsWith"],\n                not_equals=["notEquals"],\n                not_starts_with=["notStartsWith"],\n                starts_with=["startsWith"]\n            )],\n\n            # the properties below are optional\n            name="name"\n        )],\n        ingestion_enabled=False,\n        kms_key_id="kmsKeyId",\n        multi_region_enabled=False,\n        name="name",\n        organization_enabled=False,\n        retention_period=123,\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        termination_protection_enabled=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['advanced_event_selectors', 'ingestion_enabled', 'kms_key_id', 'multi_region_enabled', 'name', 'organization_enabled', 'retention_period', 'tags', 'termination_protection_enabled']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnEventDataStoreProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnResourcePolicyProps
class CfnResourcePolicyPropsDef(BaseCfnProperty):
    resource_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the CloudTrail channel attached to the resource-based policy. The following is the format of a resource ARN: ``arn:aws:cloudtrail:us-east-2:123456789012:channel/MyChannel`` .\n')
    resource_policy: typing.Union[typing.Any, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='A JSON-formatted string for an AWS resource-based policy. The following are requirements for the resource policy: - Contains only one action: cloudtrail-data:PutAuditEvents - Contains at least one statement. The policy can have a maximum of 20 statements. - Each statement contains at least one principal. A statement can have a maximum of 50 principals.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudtrail-resourcepolicy.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    # resource_policy: Any\n\n    cfn_resource_policy_props = cloudtrail.CfnResourcePolicyProps(\n        resource_arn="resourceArn",\n        resource_policy=resource_policy\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['resource_arn', 'resource_policy']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnResourcePolicyProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_cloudtrail.CfnTrailProps
class CfnTrailPropsDef(BaseCfnProperty):
    is_logging: typing.Union[_REQUIRED_INIT_PARAM, bool, models.UnsupportedResource] = pydantic.Field(REQUIRED_INIT_PARAM, description='Whether the CloudTrail trail is currently logging AWS API calls.\n')
    s3_bucket_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the name of the Amazon S3 bucket designated for publishing log files. See `Amazon S3 Bucket Naming Requirements <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create_trail_naming_policy.html>`_ .\n')
    advanced_event_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_AdvancedEventSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Specifies the settings for advanced event selectors. You can add advanced event selectors, and conditions for your advanced event selectors, up to a maximum of 500 values for all conditions and selectors on a trail. You can use either ``AdvancedEventSelectors`` or ``EventSelectors`` , but not both. If you apply ``AdvancedEventSelectors`` to a trail, any existing ``EventSelectors`` are overwritten. For more information about advanced event selectors, see `Logging data events <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/logging-data-events-with-cloudtrail.html>`_ in the *AWS CloudTrail User Guide* .\n')
    cloud_watch_logs_log_group_arn: typing.Optional[str] = pydantic.Field(None, description='Specifies a log group name using an Amazon Resource Name (ARN), a unique identifier that represents the log group to which CloudTrail logs are delivered. You must use a log group that exists in your account. Not required unless you specify ``CloudWatchLogsRoleArn`` . .. epigraph:: Only the management account can configure a CloudWatch Logs log group for an organization trail.\n')
    cloud_watch_logs_role_arn: typing.Optional[str] = pydantic.Field(None, description="Specifies the role for the CloudWatch Logs endpoint to assume to write to a user's log group. You must use a role that exists in your account.\n")
    enable_log_file_validation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether log file validation is enabled. The default is false. .. epigraph:: When you disable log file integrity validation, the chain of digest files is broken after one hour. CloudTrail does not create digest files for log files that were delivered during a period in which log file integrity validation was disabled. For example, if you enable log file integrity validation at noon on January 1, disable it at noon on January 2, and re-enable it at noon on January 10, digest files will not be created for the log files delivered from noon on January 2 to noon on January 10. The same applies whenever you stop CloudTrail logging or delete a trail.\n')
    event_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_EventSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description="Use event selectors to further specify the management and data event settings for your trail. By default, trails created without specific event selectors will be configured to log all read and write management events, and no data events. When an event occurs in your account, CloudTrail evaluates the event selector for all trails. For each trail, if the event matches any event selector, the trail processes and logs the event. If the event doesn't match any event selector, the trail doesn't log the event. You can configure up to five event selectors for a trail. You cannot apply both event selectors and advanced event selectors to a trail.\n")
    include_global_service_events: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the trail is publishing events from global services such as IAM to the log files.\n')
    insight_selectors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_cloudtrail.CfnTrail_InsightSelectorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='A JSON string that contains the insight types you want to log on a trail. ``ApiCallRateInsight`` and ``ApiErrorRateInsight`` are valid Insight types. The ``ApiCallRateInsight`` Insights type analyzes write-only management API calls that are aggregated per minute against a baseline API call volume. The ``ApiErrorRateInsight`` Insights type analyzes management API calls that result in error codes. The error is shown if the API call is unsuccessful.\n')
    is_multi_region_trail: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the trail applies only to the current Region or to all Regions. The default is false. If the trail exists only in the current Region and this value is set to true, shadow trails (replications of the trail) will be created in the other Regions. If the trail exists in all Regions and this value is set to false, the trail will remain in the Region where it was created, and its shadow trails in other Regions will be deleted. As a best practice, consider using trails that log events in all Regions.\n')
    is_organization_trail: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies whether the trail is applied to all accounts in an organization in AWS Organizations , or only for the current AWS account . The default is false, and cannot be true unless the call is made on behalf of an AWS account that is the management account for an organization in AWS Organizations . If the trail is not an organization trail and this is set to ``true`` , the trail will be created in all AWS accounts that belong to the organization. If the trail is an organization trail and this is set to ``false`` , the trail will remain in the current AWS account but be deleted from all member accounts in the organization. .. epigraph:: Only the management account for the organization can convert an organization trail to a non-organization trail, or convert a non-organization trail to an organization trail.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the AWS KMS key ID to use to encrypt the logs delivered by CloudTrail. The value can be an alias name prefixed by "alias/", a fully specified ARN to an alias, a fully specified ARN to a key, or a globally unique identifier. CloudTrail also supports AWS KMS multi-Region keys. For more information about multi-Region keys, see `Using multi-Region keys <https://docs.aws.amazon.com/kms/latest/developerguide/multi-region-keys-overview.html>`_ in the *AWS Key Management Service Developer Guide* . Examples: - alias/MyAliasName - arn:aws:kms:us-east-2:123456789012:alias/MyAliasName - arn:aws:kms:us-east-2:123456789012:key/12345678-1234-1234-1234-123456789012 - 12345678-1234-1234-1234-123456789012\n')
    s3_key_prefix: typing.Optional[str] = pydantic.Field(None, description='Specifies the Amazon S3 key prefix that comes after the name of the bucket you have designated for log file delivery. For more information, see `Finding Your CloudTrail Log Files <https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-find-log-files.html>`_ . The maximum length is 200 characters.\n')
    sns_topic_name: typing.Optional[str] = pydantic.Field(None, description='Specifies the name of the Amazon SNS topic defined for notification of log file delivery. The maximum length is 256 characters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='A custom set of tags (key-value pairs) for this trail.\n')
    trail_name: typing.Optional[str] = pydantic.Field(None, description='Specifies the name of the trail. The name must meet the following requirements:. - Contain only ASCII letters (a-z, A-Z), numbers (0-9), periods (.), underscores (_), or dashes (-) - Start with a letter or number, and end with a letter or number - Be between 3 and 128 characters - Have no adjacent periods, underscores or dashes. Names like ``my-_namespace`` and ``my--namespace`` are not valid. - Not be in IP address format (for example, 192.168.5.4)\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-cloudtrail-trail.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_cloudtrail as cloudtrail\n\n    cfn_trail_props = cloudtrail.CfnTrailProps(\n        is_logging=False,\n        s3_bucket_name="s3BucketName",\n\n        # the properties below are optional\n        advanced_event_selectors=[cloudtrail.CfnTrail.AdvancedEventSelectorProperty(\n            field_selectors=[cloudtrail.CfnTrail.AdvancedFieldSelectorProperty(\n                field="field",\n\n                # the properties below are optional\n                ends_with=["endsWith"],\n                equal_to=["equalTo"],\n                not_ends_with=["notEndsWith"],\n                not_equals=["notEquals"],\n                not_starts_with=["notStartsWith"],\n                starts_with=["startsWith"]\n            )],\n\n            # the properties below are optional\n            name="name"\n        )],\n        cloud_watch_logs_log_group_arn="cloudWatchLogsLogGroupArn",\n        cloud_watch_logs_role_arn="cloudWatchLogsRoleArn",\n        enable_log_file_validation=False,\n        event_selectors=[cloudtrail.CfnTrail.EventSelectorProperty(\n            data_resources=[cloudtrail.CfnTrail.DataResourceProperty(\n                type="type",\n\n                # the properties below are optional\n                values=["values"]\n            )],\n            exclude_management_event_sources=["excludeManagementEventSources"],\n            include_management_events=False,\n            read_write_type="readWriteType"\n        )],\n        include_global_service_events=False,\n        insight_selectors=[cloudtrail.CfnTrail.InsightSelectorProperty(\n            insight_type="insightType"\n        )],\n        is_multi_region_trail=False,\n        is_organization_trail=False,\n        kms_key_id="kmsKeyId",\n        s3_key_prefix="s3KeyPrefix",\n        sns_topic_name="snsTopicName",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        trail_name="trailName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['is_logging', 's3_bucket_name', 'advanced_event_selectors', 'cloud_watch_logs_log_group_arn', 'cloud_watch_logs_role_arn', 'enable_log_file_validation', 'event_selectors', 'include_global_service_events', 'insight_selectors', 'is_multi_region_trail', 'is_organization_trail', 'kms_key_id', 's3_key_prefix', 'sns_topic_name', 'tags', 'trail_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_cloudtrail.CfnTrailProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




import models

class ModuleModel(pydantic.BaseModel):
    InsightType: typing.Optional[dict[str, InsightTypeDef]] = pydantic.Field(None)
    Trail: typing.Optional[dict[str, TrailDef]] = pydantic.Field(None)
    AddEventSelectorOptions: typing.Optional[dict[str, AddEventSelectorOptionsDef]] = pydantic.Field(None)
    CfnChannel_DestinationProperty: typing.Optional[dict[str, CfnChannel_DestinationPropertyDef]] = pydantic.Field(None)
    CfnEventDataStore_AdvancedEventSelectorProperty: typing.Optional[dict[str, CfnEventDataStore_AdvancedEventSelectorPropertyDef]] = pydantic.Field(None)
    CfnEventDataStore_AdvancedFieldSelectorProperty: typing.Optional[dict[str, CfnEventDataStore_AdvancedFieldSelectorPropertyDef]] = pydantic.Field(None)
    CfnTrail_AdvancedEventSelectorProperty: typing.Optional[dict[str, CfnTrail_AdvancedEventSelectorPropertyDef]] = pydantic.Field(None)
    CfnTrail_AdvancedFieldSelectorProperty: typing.Optional[dict[str, CfnTrail_AdvancedFieldSelectorPropertyDef]] = pydantic.Field(None)
    CfnTrail_DataResourceProperty: typing.Optional[dict[str, CfnTrail_DataResourcePropertyDef]] = pydantic.Field(None)
    CfnTrail_EventSelectorProperty: typing.Optional[dict[str, CfnTrail_EventSelectorPropertyDef]] = pydantic.Field(None)
    CfnTrail_InsightSelectorProperty: typing.Optional[dict[str, CfnTrail_InsightSelectorPropertyDef]] = pydantic.Field(None)
    S3EventSelector: typing.Optional[dict[str, S3EventSelectorDef]] = pydantic.Field(None)
    TrailProps: typing.Optional[dict[str, TrailPropsDef]] = pydantic.Field(None)
    CfnChannel: typing.Optional[dict[str, CfnChannelDef]] = pydantic.Field(None)
    CfnEventDataStore: typing.Optional[dict[str, CfnEventDataStoreDef]] = pydantic.Field(None)
    CfnResourcePolicy: typing.Optional[dict[str, CfnResourcePolicyDef]] = pydantic.Field(None)
    CfnTrail: typing.Optional[dict[str, CfnTrailDef]] = pydantic.Field(None)
    CfnChannelProps: typing.Optional[dict[str, CfnChannelPropsDef]] = pydantic.Field(None)
    CfnEventDataStoreProps: typing.Optional[dict[str, CfnEventDataStorePropsDef]] = pydantic.Field(None)
    CfnResourcePolicyProps: typing.Optional[dict[str, CfnResourcePolicyPropsDef]] = pydantic.Field(None)
    CfnTrailProps: typing.Optional[dict[str, CfnTrailPropsDef]] = pydantic.Field(None)
    ...
