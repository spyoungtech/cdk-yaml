from __future__ import annotations
import typing
import aws_cdk
import constructs
import pydantic
import datetime
from ._base import BaseConstruct, BaseClass, BaseStruct, BaseCfnResource, BaseCfnProperty, ConnectableMixin, BaseMethodParams, GenericApplyRemovalPolicyParams, REQUIRED_INIT_PARAM, _REQUIRED_INIT_PARAM

#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.DocDbSettingsProperty
class CfnEndpoint_DocDbSettingsPropertyDef(BaseStruct):
    docs_to_investigate: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates the number of documents to preview to determine the document organization. Use this setting when ``NestingLevel`` is set to ``"one"`` . Must be a positive value greater than ``0`` . Default value is ``1000`` .\n')
    extract_doc_id: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies the document ID. Use this setting when ``NestingLevel`` is set to ``"none"`` . Default value is ``"false"`` .\n')
    nesting_level: typing.Optional[str] = pydantic.Field(None, description='Specifies either document or table mode. Default value is ``"none"`` . Specify ``"none"`` to use document mode. Specify ``"one"`` to use table mode.\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the DocumentDB endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the DocumentDB endpoint connection details.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-docdbsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    doc_db_settings_property = dms.CfnEndpoint.DocDbSettingsProperty(\n        docs_to_investigate=123,\n        extract_doc_id=False,\n        nesting_level="nestingLevel",\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['docs_to_investigate', 'extract_doc_id', 'nesting_level', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.DocDbSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.DynamoDbSettingsProperty
class CfnEndpoint_DynamoDbSettingsPropertyDef(BaseStruct):
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the ``iam:PassRole`` action.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-dynamodbsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    dynamo_db_settings_property = dms.CfnEndpoint.DynamoDbSettingsProperty(\n        service_access_role_arn="serviceAccessRoleArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['service_access_role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.DynamoDbSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.ElasticsearchSettingsProperty
class CfnEndpoint_ElasticsearchSettingsPropertyDef(BaseStruct):
    endpoint_uri: typing.Optional[str] = pydantic.Field(None, description="The endpoint for the OpenSearch cluster. AWS DMS uses HTTPS if a transport protocol (either HTTP or HTTPS) isn't specified.\n")
    error_retry_duration: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum number of seconds for which DMS retries failed API requests to the OpenSearch cluster.\n')
    full_load_error_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum percentage of records that can fail to be written before a full load operation stops. To avoid early failure, this counter is only effective after 1,000 records are transferred. OpenSearch also has the concept of error monitoring during the last 10 minutes of an Observation Window. If transfer of all records fail in the last 10 minutes, the full load operation stops.\n')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the ``iam:PassRole`` action.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-elasticsearchsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    elasticsearch_settings_property = dms.CfnEndpoint.ElasticsearchSettingsProperty(\n        endpoint_uri="endpointUri",\n        error_retry_duration=123,\n        full_load_error_percentage=123,\n        service_access_role_arn="serviceAccessRoleArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['endpoint_uri', 'error_retry_duration', 'full_load_error_percentage', 'service_access_role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.ElasticsearchSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.GcpMySQLSettingsProperty
class CfnEndpoint_GcpMySQLSettingsPropertyDef(BaseStruct):
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description='Specifies a script to run immediately after AWS DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails. For this parameter, provide the code of the script itself, not the name of a file containing the script.\n')
    clean_source_metadata_on_mismatch: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Adjusts the behavior of AWS DMS when migrating from an SQL Server source database that is hosted as part of an Always On availability group cluster. If you need AWS DMS to poll all the nodes in the Always On cluster for transaction backups, set this attribute to ``false`` .\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description="Database name for the endpoint. For a MySQL source or target endpoint, don't explicitly specify the database using the ``DatabaseName`` request parameter on either the ``CreateEndpoint`` or ``ModifyEndpoint`` API call. Specifying ``DatabaseName`` when you create or modify a MySQL endpoint replicates all the task tables to this single database. For MySQL endpoints, you specify the database only when you specify the schema in the table-mapping rules of the AWS DMS task.\n")
    events_poll_interval: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies how often to check the binary log for new changes/events when the database is idle. The default is five seconds. Example: ``eventsPollInterval=5;`` In the example, AWS DMS checks for changes in the binary logs every five seconds.\n')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database. Example: ``maxFileSize=512``\n')
    parallel_load_threads: typing.Union[int, float, None] = pydantic.Field(None, description='Improves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread. The default is one. Example: ``parallelLoadThreads=1``\n')
    password: typing.Optional[str] = pydantic.Field(None, description='Endpoint connection password.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port used by the endpoint database.\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret.`` The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the MySQL endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the MySQL endpoint connection details.\n')
    server_name: typing.Optional[str] = pydantic.Field(None, description='The MySQL host name.\n')
    server_timezone: typing.Optional[str] = pydantic.Field(None, description="Specifies the time zone for the source MySQL database. Don't enclose time zones in single quotation marks. Example: ``serverTimezone=US/Pacific;``\n")
    username: typing.Optional[str] = pydantic.Field(None, description='Endpoint connection user name.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-gcpmysqlsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    gcp_my_sQLSettings_property = dms.CfnEndpoint.GcpMySQLSettingsProperty(\n        after_connect_script="afterConnectScript",\n        clean_source_metadata_on_mismatch=False,\n        database_name="databaseName",\n        events_poll_interval=123,\n        max_file_size=123,\n        parallel_load_threads=123,\n        password="password",\n        port=123,\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        server_name="serverName",\n        server_timezone="serverTimezone",\n        username="username"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['after_connect_script', 'clean_source_metadata_on_mismatch', 'database_name', 'events_poll_interval', 'max_file_size', 'parallel_load_threads', 'password', 'port', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id', 'server_name', 'server_timezone', 'username']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.GcpMySQLSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.IbmDb2SettingsProperty
class CfnEndpoint_IbmDb2SettingsPropertyDef(BaseStruct):
    current_lsn: typing.Optional[str] = pydantic.Field(None, description='For ongoing replication (CDC), use CurrentLSN to specify a log sequence number (LSN) where you want the replication to start.\n')
    keep_csv_files: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If true, AWS DMS saves any .csv files to the Db2 LUW target that were used to replicate data. DMS uses these files for analysis and troubleshooting. The default value is false.\n')
    load_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time (in milliseconds) before AWS DMS times out operations performed by DMS on the Db2 target. The default value is 1200 (20 minutes).\n')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum size (in KB) of .csv files used to transfer data to Db2 LUW.\n')
    max_k_bytes_per_read: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum number of bytes per read, as a NUMBER value. The default is 64 KB.\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value ofthe AWS Secrets Manager secret that allows access to the Db2 LUW endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the IBMDB2 endpoint connection details.\n')
    set_data_capture_changes: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Enables ongoing replication (CDC) as a BOOLEAN value. The default is true.\n')
    write_buffer_size: typing.Union[int, float, None] = pydantic.Field(None, description='The size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk on the DMS replication instance. The default value is 1024 (1 MB).\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-ibmdb2settings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    ibm_db2_settings_property = dms.CfnEndpoint.IbmDb2SettingsProperty(\n        current_lsn="currentLsn",\n        keep_csv_files=False,\n        load_timeout=123,\n        max_file_size=123,\n        max_kBytes_per_read=123,\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        set_data_capture_changes=False,\n        write_buffer_size=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['current_lsn', 'keep_csv_files', 'load_timeout', 'max_file_size', 'max_k_bytes_per_read', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id', 'set_data_capture_changes', 'write_buffer_size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.IbmDb2SettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.KafkaSettingsProperty
class CfnEndpoint_KafkaSettingsPropertyDef(BaseStruct):
    broker: typing.Optional[str] = pydantic.Field(None, description='A comma-separated list of one or more broker locations in your Kafka cluster that host your Kafka instance. Specify each broker location in the form ``*broker-hostname-or-ip* : *port*`` . For example, ``"ec2-12-345-678-901.compute-1.amazonaws.com:2345"`` . For more information and examples of specifying a list of broker locations, see `Using Apache Kafka as a target for AWS Database Migration Service <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    include_control_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Shows detailed control information for table definition, column definition, and table and column changes in the Kafka message output. The default is ``false`` .\n')
    include_null_and_empty: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Include NULL and empty columns for records migrated to the endpoint. The default is ``false`` .\n')
    include_partition_value: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Shows the partition value within the Kafka message output unless the partition type is ``schema-table-type`` . The default is ``false`` .\n')
    include_table_alter_operations: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Includes any data definition language (DDL) operations that change the table in the control data, such as ``rename-table`` , ``drop-table`` , ``add-column`` , ``drop-column`` , and ``rename-column`` . The default is ``false`` .\n')
    include_transaction_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for ``transaction_id`` , previous ``transaction_id`` , and ``transaction_record_id`` (the record offset within a transaction). The default is ``false`` .\n')
    message_format: typing.Optional[str] = pydantic.Field(None, description='The output format for the records created on the endpoint. The message format is ``JSON`` (default) or ``JSON_UNFORMATTED`` (a single line with no tab).\n')
    message_max_bytes: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size in bytes for records created on the endpoint The default is 1,000,000.\n')
    no_hex_prefix: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Set this optional parameter to ``true`` to avoid adding a '0x' prefix to raw data in hexadecimal format. For example, by default, AWS DMS adds a '0x' prefix to the LOB column type in hexadecimal format moving from an Oracle source to a Kafka target. Use the ``NoHexPrefix`` endpoint setting to enable migration of RAW data type columns without adding the '0x' prefix.\n")
    partition_include_schema_table: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Prefixes schema and table names to partition values, when the partition type is ``primary-key-type`` . Doing this increases data distribution among Kafka partitions. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same partition, which causes throttling. The default is ``false`` .\n')
    sasl_password: typing.Optional[str] = pydantic.Field(None, description='The secure password that you created when you first set up your Amazon MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n')
    sasl_user_name: typing.Optional[str] = pydantic.Field(None, description='The secure user name you created when you first set up your Amazon MSK cluster to validate a client identity and make an encrypted connection between server and client using SASL-SSL authentication.\n')
    security_protocol: typing.Optional[str] = pydantic.Field(None, description='Set secure connection to a Kafka target endpoint using Transport Layer Security (TLS). Options include ``ssl-encryption`` , ``ssl-authentication`` , and ``sasl-ssl`` . ``sasl-ssl`` requires ``SaslUsername`` and ``SaslPassword`` .\n')
    ssl_ca_certificate_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the private certificate authority (CA) cert that AWS DMS uses to securely connect to your Kafka target endpoint.\n')
    ssl_client_certificate_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the client certificate used to securely connect to a Kafka target endpoint.\n')
    ssl_client_key_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the client private key used to securely connect to a Kafka target endpoint.\n')
    ssl_client_key_password: typing.Optional[str] = pydantic.Field(None, description='The password for the client private key used to securely connect to a Kafka target endpoint.\n')
    topic: typing.Optional[str] = pydantic.Field(None, description='The topic to which you migrate the data. If you don\'t specify a topic, AWS DMS specifies ``"kafka-default-topic"`` as the migration topic.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-kafkasettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    kafka_settings_property = dms.CfnEndpoint.KafkaSettingsProperty(\n        broker="broker",\n        include_control_details=False,\n        include_null_and_empty=False,\n        include_partition_value=False,\n        include_table_alter_operations=False,\n        include_transaction_details=False,\n        message_format="messageFormat",\n        message_max_bytes=123,\n        no_hex_prefix=False,\n        partition_include_schema_table=False,\n        sasl_password="saslPassword",\n        sasl_user_name="saslUserName",\n        security_protocol="securityProtocol",\n        ssl_ca_certificate_arn="sslCaCertificateArn",\n        ssl_client_certificate_arn="sslClientCertificateArn",\n        ssl_client_key_arn="sslClientKeyArn",\n        ssl_client_key_password="sslClientKeyPassword",\n        topic="topic"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['broker', 'include_control_details', 'include_null_and_empty', 'include_partition_value', 'include_table_alter_operations', 'include_transaction_details', 'message_format', 'message_max_bytes', 'no_hex_prefix', 'partition_include_schema_table', 'sasl_password', 'sasl_user_name', 'security_protocol', 'ssl_ca_certificate_arn', 'ssl_client_certificate_arn', 'ssl_client_key_arn', 'ssl_client_key_password', 'topic']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.KafkaSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.KinesisSettingsProperty
class CfnEndpoint_KinesisSettingsPropertyDef(BaseStruct):
    include_control_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Shows detailed control information for table definition, column definition, and table and column changes in the Kinesis message output. The default is ``false`` .\n')
    include_null_and_empty: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Include NULL and empty columns for records migrated to the endpoint. The default is ``false`` .\n')
    include_partition_value: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Shows the partition value within the Kinesis message output, unless the partition type is ``schema-table-type`` . The default is ``false`` .\n')
    include_table_alter_operations: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Includes any data definition language (DDL) operations that change the table in the control data, such as ``rename-table`` , ``drop-table`` , ``add-column`` , ``drop-column`` , and ``rename-column`` . The default is ``false`` .\n')
    include_transaction_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Provides detailed transaction information from the source database. This information includes a commit timestamp, a log position, and values for ``transaction_id`` , previous ``transaction_id`` , and ``transaction_record_id`` (the record offset within a transaction). The default is ``false`` .\n')
    message_format: typing.Optional[str] = pydantic.Field(None, description='The output format for the records created on the endpoint. The message format is ``JSON`` (default) or ``JSON_UNFORMATTED`` (a single line with no tab).\n')
    no_hex_prefix: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Set this optional parameter to ``true`` to avoid adding a '0x' prefix to raw data in hexadecimal format. For example, by default, AWS DMS adds a '0x' prefix to the LOB column type in hexadecimal format moving from an Oracle source to an Amazon Kinesis target. Use the ``NoHexPrefix`` endpoint setting to enable migration of RAW data type columns without adding the '0x' prefix.\n")
    partition_include_schema_table: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Prefixes schema and table names to partition values, when the partition type is ``primary-key-type`` . Doing this increases data distribution among Kinesis shards. For example, suppose that a SysBench schema has thousands of tables and each table has only limited range for a primary key. In this case, the same primary key is sent from thousands of tables to the same shard, which causes throttling. The default is ``false`` .\n')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the IAM role that AWS DMS uses to write to the Kinesis data stream. The role must allow the ``iam:PassRole`` action.\n')
    stream_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the Amazon Kinesis Data Streams endpoint.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-kinesissettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    kinesis_settings_property = dms.CfnEndpoint.KinesisSettingsProperty(\n        include_control_details=False,\n        include_null_and_empty=False,\n        include_partition_value=False,\n        include_table_alter_operations=False,\n        include_transaction_details=False,\n        message_format="messageFormat",\n        no_hex_prefix=False,\n        partition_include_schema_table=False,\n        service_access_role_arn="serviceAccessRoleArn",\n        stream_arn="streamArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['include_control_details', 'include_null_and_empty', 'include_partition_value', 'include_table_alter_operations', 'include_transaction_details', 'message_format', 'no_hex_prefix', 'partition_include_schema_table', 'service_access_role_arn', 'stream_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.KinesisSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.MicrosoftSqlServerSettingsProperty
class CfnEndpoint_MicrosoftSqlServerSettingsPropertyDef(BaseStruct):
    bcp_packet_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of the packets (in bytes) used to transfer data using BCP.\n')
    control_tables_file_group: typing.Optional[str] = pydantic.Field(None, description='Specifies a file group for the AWS DMS internal tables. When the replication task starts, all the internal AWS DMS control tables (awsdms_ apply_exception, awsdms_apply, awsdms_changes) are created for the specified file group.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='Database name for the endpoint.\n')
    force_lob_lookup: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Forces LOB lookup on inline LOB.\n')
    password: typing.Optional[str] = pydantic.Field(None, description='Endpoint connection password.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='Endpoint TCP port.\n')
    query_single_always_on_node: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Cleans and recreates table metadata information on the replication instance when a mismatch occurs. An example is a situation where running an alter DDL statement on a table might result in different information about the table cached in the replication instance.\n')
    read_backup_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="When this attribute is set to ``Y`` , AWS DMS only reads changes from transaction log backups and doesn't read from the active transaction log file during ongoing replication. Setting this parameter to ``Y`` enables you to control active transaction log file growth during full load and ongoing replication tasks. However, it can add some source latency to ongoing replication.\n")
    safeguard_policy: typing.Optional[str] = pydantic.Field(None, description="Use this attribute to minimize the need to access the backup log and enable AWS DMS to prevent truncation using one of the following two methods. *Start transactions in the database:* This is the default method. When this method is used, AWS DMS prevents TLOG truncation by mimicking a transaction in the database. As long as such a transaction is open, changes that appear after the transaction started aren't truncated. If you need Microsoft Replication to be enabled in your database, then you must choose this method. *Exclusively use sp_repldone within a single task* : When this method is used, AWS DMS reads the changes and then uses sp_repldone to mark the TLOG transactions as ready for truncation. Although this method doesn't involve any transactional activities, it can only be used when Microsoft Replication isn't running. Also, when using this method, only one AWS DMS task can access the database at any given time. Therefore, if you need to run parallel AWS DMS tasks against the same database, use the default method.\n")
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the SQL Server endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the MicrosoftSQLServer endpoint connection details.\n')
    server_name: typing.Optional[str] = pydantic.Field(None, description='Fully qualified domain name of the endpoint. For an Amazon RDS SQL Server instance, this is the output of `DescribeDBInstances <https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_DescribeDBInstances.html>`_ , in the ``[Endpoint](https://docs.aws.amazon.com/AmazonRDS/latest/APIReference/API_Endpoint.html) .Address`` field.\n')
    tlog_access_mode: typing.Optional[str] = pydantic.Field(None, description='Indicates the mode used to fetch CDC data.\n')
    trim_space_in_char: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Use the ``TrimSpaceInChar`` source endpoint setting to right-trim data on CHAR and NCHAR data types during migration. Setting ``TrimSpaceInChar`` does not left-trim data. The default value is ``true`` .\n')
    use_bcp_full_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Use this to attribute to transfer data for full-load operations using BCP. When the target table contains an identity column that does not exist in the source table, you must disable the use BCP for loading table option.\n')
    username: typing.Optional[str] = pydantic.Field(None, description='Endpoint connection user name.\n')
    use_third_party_backup_device: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this attribute is set to ``Y`` , DMS processes third-party transaction log backups if they are created in native format.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-microsoftsqlserversettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    microsoft_sql_server_settings_property = dms.CfnEndpoint.MicrosoftSqlServerSettingsProperty(\n        bcp_packet_size=123,\n        control_tables_file_group="controlTablesFileGroup",\n        database_name="databaseName",\n        force_lob_lookup=False,\n        password="password",\n        port=123,\n        query_single_always_on_node=False,\n        read_backup_only=False,\n        safeguard_policy="safeguardPolicy",\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        server_name="serverName",\n        tlog_access_mode="tlogAccessMode",\n        trim_space_in_char=False,\n        use_bcp_full_load=False,\n        username="username",\n        use_third_party_backup_device=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['bcp_packet_size', 'control_tables_file_group', 'database_name', 'force_lob_lookup', 'password', 'port', 'query_single_always_on_node', 'read_backup_only', 'safeguard_policy', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id', 'server_name', 'tlog_access_mode', 'trim_space_in_char', 'use_bcp_full_load', 'username', 'use_third_party_backup_device']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.MicrosoftSqlServerSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.MongoDbSettingsProperty
class CfnEndpoint_MongoDbSettingsPropertyDef(BaseStruct):
    auth_mechanism: typing.Optional[str] = pydantic.Field(None, description='The authentication mechanism you use to access the MongoDB source endpoint. For the default value, in MongoDB version 2.x, ``"default"`` is ``"mongodb_cr"`` . For MongoDB version 3.x or later, ``"default"`` is ``"scram_sha_1"`` . This setting isn\'t used when ``AuthType`` is set to ``"no"`` .\n')
    auth_source: typing.Optional[str] = pydantic.Field(None, description='The MongoDB database name. This setting isn\'t used when ``AuthType`` is set to ``"no"`` . The default is ``"admin"`` .\n')
    auth_type: typing.Optional[str] = pydantic.Field(None, description='The authentication type you use to access the MongoDB source endpoint. When set to ``"no"`` , user name and password parameters are not used and can be empty.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description='The database name on the MongoDB source endpoint.\n')
    docs_to_investigate: typing.Optional[str] = pydantic.Field(None, description='Indicates the number of documents to preview to determine the document organization. Use this setting when ``NestingLevel`` is set to ``"one"`` . Must be a positive value greater than ``0`` . Default value is ``1000`` .\n')
    extract_doc_id: typing.Optional[str] = pydantic.Field(None, description='Specifies the document ID. Use this setting when ``NestingLevel`` is set to ``"none"`` . Default value is ``"false"`` .\n')
    nesting_level: typing.Optional[str] = pydantic.Field(None, description='Specifies either document or table mode. Default value is ``"none"`` . Specify ``"none"`` to use document mode. Specify ``"one"`` to use table mode.\n')
    password: typing.Optional[str] = pydantic.Field(None, description='The password for the user account you use to access the MongoDB source endpoint.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port value for the MongoDB source endpoint.\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the MongoDB endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the MongoDB endpoint connection details.\n')
    server_name: typing.Optional[str] = pydantic.Field(None, description='The name of the server on the MongoDB source endpoint.\n')
    username: typing.Optional[str] = pydantic.Field(None, description='The user name you use to access the MongoDB source endpoint.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-mongodbsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    mongo_db_settings_property = dms.CfnEndpoint.MongoDbSettingsProperty(\n        auth_mechanism="authMechanism",\n        auth_source="authSource",\n        auth_type="authType",\n        database_name="databaseName",\n        docs_to_investigate="docsToInvestigate",\n        extract_doc_id="extractDocId",\n        nesting_level="nestingLevel",\n        password="password",\n        port=123,\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        server_name="serverName",\n        username="username"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth_mechanism', 'auth_source', 'auth_type', 'database_name', 'docs_to_investigate', 'extract_doc_id', 'nesting_level', 'password', 'port', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id', 'server_name', 'username']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.MongoDbSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.MySqlSettingsProperty
class CfnEndpoint_MySqlSettingsPropertyDef(BaseStruct):
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description='Specifies a script to run immediately after AWS DMS connects to the endpoint. The migration task continues running regardless if the SQL statement succeeds or fails. For this parameter, provide the code of the script itself, not the name of a file containing the script.\n')
    clean_source_metadata_on_mismatch: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Cleans and recreates table metadata information on the replication instance when a mismatch occurs. For example, in a situation where running an alter DDL on the table could result in different information about the table cached in the replication instance.\n')
    events_poll_interval: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies how often to check the binary log for new changes/events when the database is idle. The default is five seconds. Example: ``eventsPollInterval=5;`` In the example, AWS DMS checks for changes in the binary logs every five seconds.\n')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum size (in KB) of any .csv file used to transfer data to a MySQL-compatible database. Example: ``maxFileSize=512``\n')
    parallel_load_threads: typing.Union[int, float, None] = pydantic.Field(None, description='Improves performance when loading data into the MySQL-compatible target database. Specifies how many threads to use to load the data into the MySQL-compatible target database. Setting a large number of threads can have an adverse effect on database performance, because a separate connection is required for each thread. The default is one. Example: ``parallelLoadThreads=1``\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the MySQL endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the MySQL endpoint connection details.\n')
    server_timezone: typing.Optional[str] = pydantic.Field(None, description='Specifies the time zone for the source MySQL database. Example: ``serverTimezone=US/Pacific;`` Note: Do not enclose time zones in single quotes.\n')
    target_db_type: typing.Optional[str] = pydantic.Field(None, description='Specifies where to migrate source tables on the target, either to a single database or multiple databases. If you specify ``SPECIFIC_DATABASE`` , specify the database name using the ``DatabaseName`` parameter of the ``Endpoint`` object. Example: ``targetDbType=MULTIPLE_DATABASES``\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-mysqlsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    my_sql_settings_property = dms.CfnEndpoint.MySqlSettingsProperty(\n        after_connect_script="afterConnectScript",\n        clean_source_metadata_on_mismatch=False,\n        events_poll_interval=123,\n        max_file_size=123,\n        parallel_load_threads=123,\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        server_timezone="serverTimezone",\n        target_db_type="targetDbType"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['after_connect_script', 'clean_source_metadata_on_mismatch', 'events_poll_interval', 'max_file_size', 'parallel_load_threads', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id', 'server_timezone', 'target_db_type']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.MySqlSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.NeptuneSettingsProperty
class CfnEndpoint_NeptuneSettingsPropertyDef(BaseStruct):
    error_retry_duration: typing.Union[int, float, None] = pydantic.Field(None, description='The number of milliseconds for AWS DMS to wait to retry a bulk-load of migrated graph data to the Neptune target database before raising an error. The default is 250.\n')
    iam_auth_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If you want IAM authorization enabled for this endpoint, set this parameter to ``true`` . Then attach the appropriate IAM policy document to your service role specified by ``ServiceAccessRoleArn`` . The default is ``false`` .\n')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size in kilobytes of migrated graph data stored in a .csv file before AWS DMS bulk-loads the data to the Neptune target database. The default is 1,048,576 KB. If the bulk load is successful, AWS DMS clears the bucket, ready to store the next batch of migrated graph data.\n')
    max_retry_count: typing.Union[int, float, None] = pydantic.Field(None, description='The number of times for AWS DMS to retry a bulk load of migrated graph data to the Neptune target database before raising an error. The default is 5.\n')
    s3_bucket_folder: typing.Optional[str] = pydantic.Field(None, description='A folder path where you want AWS DMS to store migrated graph data in the S3 bucket specified by ``S3BucketName``.\n')
    s3_bucket_name: typing.Optional[str] = pydantic.Field(None, description='The name of the Amazon S3 bucket where AWS DMS can temporarily store migrated graph data in .csv files before bulk-loading it to the Neptune target database. AWS DMS maps the SQL source data to graph data before storing it in these .csv files.\n')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the service role that you created for the Neptune target endpoint. The role must allow the ``iam:PassRole`` action. For more information, see `Creating an IAM Service Role for Accessing Amazon Neptune as a Target <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Neptune.html#CHAP_Target.Neptune.ServiceRole>`_ in the *AWS Database Migration Service User Guide* .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-neptunesettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    neptune_settings_property = dms.CfnEndpoint.NeptuneSettingsProperty(\n        error_retry_duration=123,\n        iam_auth_enabled=False,\n        max_file_size=123,\n        max_retry_count=123,\n        s3_bucket_folder="s3BucketFolder",\n        s3_bucket_name="s3BucketName",\n        service_access_role_arn="serviceAccessRoleArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['error_retry_duration', 'iam_auth_enabled', 'max_file_size', 'max_retry_count', 's3_bucket_folder', 's3_bucket_name', 'service_access_role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.NeptuneSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.OracleSettingsProperty
class CfnEndpoint_OracleSettingsPropertyDef(BaseStruct):
    access_alternate_directly: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to ``false`` in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to not access redo logs through any specified path prefix replacement using direct file access.\n')
    additional_archived_log_dest_id: typing.Union[int, float, None] = pydantic.Field(None, description="Set this attribute with ``ArchivedLogDestId`` in a primary/ standby setup. This attribute is useful in the case of a switchover. In this case, AWS DMS needs to know which destination to get archive redo logs from to read changes. This need arises because the previous primary instance is now a standby instance after switchover. Although AWS DMS supports the use of the Oracle ``RESETLOGS`` option to open the database, never use ``RESETLOGS`` unless necessary. For additional information about ``RESETLOGS`` , see `RMAN Data Repair Concepts <https://docs.aws.amazon.com/https://docs.oracle.com/en/database/oracle/oracle-database/19/bradv/rman-data-repair-concepts.html#GUID-1805CCF7-4AF2-482D-B65A-998192F89C2B>`_ in the *Oracle Database Backup and Recovery User's Guide* .\n")
    add_supplemental_logging: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to set up table-level supplemental logging for the Oracle database. This attribute enables PRIMARY KEY supplemental logging on all tables selected for a migration task. If you use this option, you still need to enable database-level supplemental logging.\n')
    allow_select_nested_tables: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to ``true`` to enable replication of Oracle tables containing columns that are nested tables or defined types.\n')
    archived_log_dest_id: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the ID of the destination for the archived redo logs. This value should be the same as a number in the dest_id column of the v$archived_log view. If you work with an additional redo log destination, use the ``AdditionalArchivedLogDestId`` option to specify the additional destination ID. Doing this improves performance by ensuring that the correct logs are accessed from the outset.\n')
    archived_logs_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When this field is set to ``Y`` , AWS DMS only accesses the archived redo logs. If the archived redo logs are stored on Automatic Storage Management (ASM) only, the AWS DMS user account needs to be granted ASM privileges.\n')
    asm_password: typing.Optional[str] = pydantic.Field(None, description='For an Oracle source endpoint, your Oracle Automatic Storage Management (ASM) password. You can set this value from the ``*asm_user_password*`` value. You set this value as part of the comma-separated value that you set to the ``Password`` request parameter when you create the endpoint to access transaction logs using Binary Reader. For more information, see `Configuration for change data capture (CDC) on an Oracle source database <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.CDC.Configuration>`_ .\n')
    asm_server: typing.Optional[str] = pydantic.Field(None, description='For an Oracle source endpoint, your ASM server address. You can set this value from the ``asm_server`` value. You set ``asm_server`` as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see `Configuration for change data capture (CDC) on an Oracle source database <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.CDC.Configuration>`_ .\n')
    asm_user: typing.Optional[str] = pydantic.Field(None, description='For an Oracle source endpoint, your ASM user name. You can set this value from the ``asm_user`` value. You set ``asm_user`` as part of the extra connection attribute string to access an Oracle server with Binary Reader that uses ASM. For more information, see `Configuration for change data capture (CDC) on an Oracle source database <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.CDC.Configuration>`_ .\n')
    char_length_semantics: typing.Optional[str] = pydantic.Field(None, description='Specifies whether the length of a character column is in bytes or in characters. To indicate that the character column length is in characters, set this attribute to ``CHAR`` . Otherwise, the character column length is in bytes. Example: ``charLengthSemantics=CHAR;``\n')
    direct_path_no_log: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When set to ``true`` , this attribute helps to increase the commit rate on the Oracle target database by writing directly to tables and not writing a trail to database logs.\n')
    direct_path_parallel_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When set to ``true`` , this attribute specifies a parallel load when ``useDirectPathFullLoad`` is set to ``Y`` . This attribute also only applies when you use the AWS DMS parallel load feature. Note that the target table cannot have any constraints or indexes.\n')
    enable_homogenous_tablespace: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to enable homogenous tablespace replication and create existing tables or indexes under the same tablespace on the target.\n')
    extra_archived_log_dest_ids: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[int, float]], None] = pydantic.Field(None, description="Specifies the IDs of one more destinations for one or more archived redo logs. These IDs are the values of the ``dest_id`` column in the ``v$archived_log`` view. Use this setting with the ``archivedLogDestId`` extra connection attribute in a primary-to-single setup or a primary-to-multiple-standby setup. This setting is useful in a switchover when you use an Oracle Data Guard database as a source. In this case, AWS DMS needs information about what destination to get archive redo logs from to read changes. AWS DMS needs this because after the switchover the previous primary is a standby instance. For example, in a primary-to-single standby setup you might apply the following settings. ``archivedLogDestId=1; ExtraArchivedLogDestIds=[2]`` In a primary-to-multiple-standby setup, you might apply the following settings. ``archivedLogDestId=1; ExtraArchivedLogDestIds=[2,3,4]`` Although AWS DMS supports the use of the Oracle ``RESETLOGS`` option to open the database, never use ``RESETLOGS`` unless it's necessary. For more information about ``RESETLOGS`` , see `RMAN Data Repair Concepts <https://docs.aws.amazon.com/https://docs.oracle.com/en/database/oracle/oracle-database/19/bradv/rman-data-repair-concepts.html#GUID-1805CCF7-4AF2-482D-B65A-998192F89C2B>`_ in the *Oracle Database Backup and Recovery User's Guide* .\n")
    fail_tasks_on_lob_truncation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When set to ``true`` , this attribute causes a task to fail if the actual size of an LOB column is greater than the specified ``LobMaxSize`` . If a task is set to limited LOB mode and this option is set to ``true`` , the task fails instead of truncating the LOB data.\n')
    number_datatype_scale: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the number scale. You can select a scale up to 38, or you can select FLOAT. By default, the NUMBER data type is converted to precision 38, scale 10. Example: ``numberDataTypeScale=12``\n')
    oracle_path_prefix: typing.Optional[str] = pydantic.Field(None, description='Set this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the default Oracle root used to access the redo logs.\n')
    parallel_asm_read_threads: typing.Union[int, float, None] = pydantic.Field(None, description='Set this attribute to change the number of threads that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 2 (the default) and 8 (the maximum). Use this attribute together with the ``readAheadBlocks`` attribute.\n')
    read_ahead_blocks: typing.Union[int, float, None] = pydantic.Field(None, description='Set this attribute to change the number of read-ahead blocks that DMS configures to perform a change data capture (CDC) load using Oracle Automatic Storage Management (ASM). You can specify an integer value between 1000 (the default) and 200,000 (the maximum).\n')
    read_table_space_name: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When set to ``true`` , this attribute supports tablespace replication.\n')
    replace_path_prefix: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to true in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This setting tells DMS instance to replace the default Oracle root with the specified ``usePathPrefix`` setting to access the redo logs.\n')
    retry_interval: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the number of seconds that the system waits before resending a query. Example: ``retryInterval=6;``\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the Oracle endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_oracle_asm_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="Required only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the ``SecretsManagerOracleAsmSecret`` . This ``SecretsManagerOracleAsmSecret`` has the secret value that allows access to the Oracle ASM of the endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerOracleAsmSecretId`` . Or you can specify clear-text values for ``AsmUser`` , ``AsmPassword`` , and ``AsmServerName`` . You can't specify both. For more information on creating this ``SecretsManagerOracleAsmSecret`` , the corresponding ``SecretsManagerOracleAsmAccessRoleArn`` , and the ``SecretsManagerOracleAsmSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_oracle_asm_secret_id: typing.Optional[str] = pydantic.Field(None, description='Required only if your Oracle endpoint uses Advanced Storage Manager (ASM). The full ARN, partial ARN, or display name of the ``SecretsManagerOracleAsmSecret`` that contains the Oracle ASM connection details for the Oracle endpoint.\n')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the Oracle endpoint connection details.\n')
    security_db_encryption: typing.Optional[str] = pydantic.Field(None, description='For an Oracle source endpoint, the transparent data encryption (TDE) password required by AWM DMS to access Oracle redo logs encrypted by TDE using Binary Reader. It is also the ``*TDE_Password*`` part of the comma-separated value you set to the ``Password`` request parameter when you create the endpoint. The ``SecurityDbEncryptian`` setting is related to this ``SecurityDbEncryptionName`` setting. For more information, see `Supported encryption methods for using Oracle as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.Encryption>`_ in the *AWS Database Migration Service User Guide* .\n')
    security_db_encryption_name: typing.Optional[str] = pydantic.Field(None, description='For an Oracle source endpoint, the name of a key used for the transparent data encryption (TDE) of the columns and tablespaces in an Oracle source database that is encrypted using TDE. The key value is the value of the ``SecurityDbEncryption`` setting. For more information on setting the key name value of ``SecurityDbEncryptionName`` , see the information and example for setting the ``securityDbEncryptionName`` extra connection attribute in `Supported encryption methods for using Oracle as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.Encryption>`_ in the *AWS Database Migration Service User Guide* .\n')
    spatial_data_option_to_geo_json_function_name: typing.Optional[str] = pydantic.Field(None, description='Use this attribute to convert ``SDO_GEOMETRY`` to ``GEOJSON`` format. By default, DMS calls the ``SDO2GEOJSON`` custom function if present and accessible. Or you can create your own custom function that mimics the operation of ``SDOGEOJSON`` and set ``SpatialDataOptionToGeoJsonFunctionName`` to call it instead.\n')
    standby_delay_time: typing.Union[int, float, None] = pydantic.Field(None, description='Use this attribute to specify a time in minutes for the delay in standby sync. If the source is an Oracle Active Data Guard standby database, use this attribute to specify the time lag between primary and standby databases. In AWS DMS , you can create an Oracle CDC task that uses an Active Data Guard standby instance as a source for replicating ongoing changes. Doing this eliminates the need to connect to an active database that might be in production.\n')
    use_alternate_folder_for_online: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to ``true`` in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This tells the DMS instance to use any specified prefix replacement to access all online redo logs.\n')
    use_b_file: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to Y to capture change data using the Binary Reader utility. Set ``UseLogminerReader`` to N to set this attribute to Y. To use Binary Reader with Amazon RDS for Oracle as the source, you set additional attributes. For more information about using this setting with Oracle Automatic Storage Management (ASM), see `Using Oracle LogMiner or AWS DMS Binary Reader for CDC <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.CDC>`_ .\n')
    use_direct_path_full_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to Y to have AWS DMS use a direct path full load. Specify this value to use the direct path protocol in the Oracle Call Interface (OCI). By using this OCI protocol, you can bulk-load Oracle target tables during a full load.\n')
    use_logminer_reader: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Set this attribute to Y to capture change data using the Oracle LogMiner utility (the default). Set this attribute to N if you want to access the redo logs as a binary file. When you set ``UseLogminerReader`` to N, also set ``UseBfile`` to Y. For more information on this setting and using Oracle ASM, see `Using Oracle LogMiner or AWS DMS Binary Reader for CDC <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.CDC>`_ in the *AWS DMS User Guide* .\n')
    use_path_prefix: typing.Optional[str] = pydantic.Field(None, description='Set this string attribute to the required value in order to use the Binary Reader to capture change data for an Amazon RDS for Oracle as the source. This value specifies the path prefix used to replace the default Oracle root to access the redo logs.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-oraclesettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    oracle_settings_property = dms.CfnEndpoint.OracleSettingsProperty(\n        access_alternate_directly=False,\n        additional_archived_log_dest_id=123,\n        add_supplemental_logging=False,\n        allow_select_nested_tables=False,\n        archived_log_dest_id=123,\n        archived_logs_only=False,\n        asm_password="asmPassword",\n        asm_server="asmServer",\n        asm_user="asmUser",\n        char_length_semantics="charLengthSemantics",\n        direct_path_no_log=False,\n        direct_path_parallel_load=False,\n        enable_homogenous_tablespace=False,\n        extra_archived_log_dest_ids=[123],\n        fail_tasks_on_lob_truncation=False,\n        number_datatype_scale=123,\n        oracle_path_prefix="oraclePathPrefix",\n        parallel_asm_read_threads=123,\n        read_ahead_blocks=123,\n        read_table_space_name=False,\n        replace_path_prefix=False,\n        retry_interval=123,\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_oracle_asm_access_role_arn="secretsManagerOracleAsmAccessRoleArn",\n        secrets_manager_oracle_asm_secret_id="secretsManagerOracleAsmSecretId",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        security_db_encryption="securityDbEncryption",\n        security_db_encryption_name="securityDbEncryptionName",\n        spatial_data_option_to_geo_json_function_name="spatialDataOptionToGeoJsonFunctionName",\n        standby_delay_time=123,\n        use_alternate_folder_for_online=False,\n        use_bFile=False,\n        use_direct_path_full_load=False,\n        use_logminer_reader=False,\n        use_path_prefix="usePathPrefix"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['access_alternate_directly', 'additional_archived_log_dest_id', 'add_supplemental_logging', 'allow_select_nested_tables', 'archived_log_dest_id', 'archived_logs_only', 'asm_password', 'asm_server', 'asm_user', 'char_length_semantics', 'direct_path_no_log', 'direct_path_parallel_load', 'enable_homogenous_tablespace', 'extra_archived_log_dest_ids', 'fail_tasks_on_lob_truncation', 'number_datatype_scale', 'oracle_path_prefix', 'parallel_asm_read_threads', 'read_ahead_blocks', 'read_table_space_name', 'replace_path_prefix', 'retry_interval', 'secrets_manager_access_role_arn', 'secrets_manager_oracle_asm_access_role_arn', 'secrets_manager_oracle_asm_secret_id', 'secrets_manager_secret_id', 'security_db_encryption', 'security_db_encryption_name', 'spatial_data_option_to_geo_json_function_name', 'standby_delay_time', 'use_alternate_folder_for_online', 'use_b_file', 'use_direct_path_full_load', 'use_logminer_reader', 'use_path_prefix']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.OracleSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.PostgreSqlSettingsProperty
class CfnEndpoint_PostgreSqlSettingsPropertyDef(BaseStruct):
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description="For use with change data capture (CDC) only, this attribute has AWS DMS bypass foreign keys and user triggers to reduce the time it takes to bulk load data. Example: ``afterConnectScript=SET session_replication_role='replica'``\n")
    babelfish_database_name: typing.Optional[str] = pydantic.Field(None, description='The Babelfish for Aurora PostgreSQL database name for the endpoint.\n')
    capture_ddls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="To capture DDL events, AWS DMS creates various artifacts in the PostgreSQL database when the task starts. You can later remove these artifacts. If this value is set to ``True`` , you don't have to create tables or triggers on the source database.\n")
    database_mode: typing.Optional[str] = pydantic.Field(None, description="Specifies the default behavior of the replication's handling of PostgreSQL- compatible endpoints that require some additional configuration, such as Babelfish endpoints.\n")
    ddl_artifacts_schema: typing.Optional[str] = pydantic.Field(None, description='The schema in which the operational DDL database artifacts are created. The default value is ``public`` . Example: ``ddlArtifactsSchema=xyzddlschema;``\n')
    execute_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='Sets the client statement timeout for the PostgreSQL instance, in seconds. The default value is 60 seconds. Example: ``executeTimeout=100;``\n')
    fail_tasks_on_lob_truncation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When set to ``true`` , this value causes a task to fail if the actual size of a LOB column is greater than the specified ``LobMaxSize`` . The default value is ``false`` . If task is set to Limited LOB mode and this option is set to true, the task fails instead of truncating the LOB data.\n')
    heartbeat_enable: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='The write-ahead log (WAL) heartbeat feature mimics a dummy transaction. By doing this, it prevents idle logical replication slots from holding onto old WAL logs, which can result in storage full situations on the source. This heartbeat keeps ``restart_lsn`` moving and prevents storage full scenarios. The default value is ``false`` .\n')
    heartbeat_frequency: typing.Union[int, float, None] = pydantic.Field(None, description='Sets the WAL heartbeat frequency (in minutes). The default value is 5 minutes.\n')
    heartbeat_schema: typing.Optional[str] = pydantic.Field(None, description='Sets the schema in which the heartbeat artifacts are created. The default value is ``public`` .\n')
    map_boolean_as_boolean: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When true, lets PostgreSQL migrate the boolean type as boolean. By default, PostgreSQL migrates booleans as ``varchar(5)`` . You must set this setting on both the source and target endpoints for it to take effect. The default value is ``false`` .\n')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='Specifies the maximum size (in KB) of any .csv file used to transfer data to PostgreSQL. The default value is 32,768 KB (32 MB). Example: ``maxFileSize=512``\n')
    plugin_name: typing.Optional[str] = pydantic.Field(None, description='Specifies the plugin to use to create a replication slot. The default value is ``pglogical`` .\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the PostgreSQL endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the PostgreSQL endpoint connection details.\n')
    slot_name: typing.Optional[str] = pydantic.Field(None, description='Sets the name of a previously created logical replication slot for a change data capture (CDC) load of the PostgreSQL source instance. When used with the ``CdcStartPosition`` request parameter for the AWS DMS API , this attribute also makes it possible to use native CDC start points. DMS verifies that the specified logical replication slot exists before starting the CDC load task. It also verifies that the task was created with a valid setting of ``CdcStartPosition`` . If the specified slot doesn\'t exist or the task doesn\'t have a valid ``CdcStartPosition`` setting, DMS raises an error. For more information about setting the ``CdcStartPosition`` request parameter, see `Determining a CDC native start point <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Task.CDC.html#CHAP_Task.CDC.StartPoint.Native>`_ in the *AWS Database Migration Service User Guide* . For more information about using ``CdcStartPosition`` , see `CreateReplicationTask <https://docs.aws.amazon.com/dms/latest/APIReference/API_CreateReplicationTask.html>`_ , `StartReplicationTask <https://docs.aws.amazon.com/dms/latest/APIReference/API_StartReplicationTask.html>`_ , and `ModifyReplicationTask <https://docs.aws.amazon.com/dms/latest/APIReference/API_ModifyReplicationTask.html>`_ .\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-postgresqlsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    postgre_sql_settings_property = dms.CfnEndpoint.PostgreSqlSettingsProperty(\n        after_connect_script="afterConnectScript",\n        babelfish_database_name="babelfishDatabaseName",\n        capture_ddls=False,\n        database_mode="databaseMode",\n        ddl_artifacts_schema="ddlArtifactsSchema",\n        execute_timeout=123,\n        fail_tasks_on_lob_truncation=False,\n        heartbeat_enable=False,\n        heartbeat_frequency=123,\n        heartbeat_schema="heartbeatSchema",\n        map_boolean_as_boolean=False,\n        max_file_size=123,\n        plugin_name="pluginName",\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        slot_name="slotName"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['after_connect_script', 'babelfish_database_name', 'capture_ddls', 'database_mode', 'ddl_artifacts_schema', 'execute_timeout', 'fail_tasks_on_lob_truncation', 'heartbeat_enable', 'heartbeat_frequency', 'heartbeat_schema', 'map_boolean_as_boolean', 'max_file_size', 'plugin_name', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id', 'slot_name']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.PostgreSqlSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.RedisSettingsProperty
class CfnEndpoint_RedisSettingsPropertyDef(BaseStruct):
    auth_password: typing.Optional[str] = pydantic.Field(None, description='The password provided with the ``auth-role`` and ``auth-token`` options of the ``AuthType`` setting for a Redis target endpoint.\n')
    auth_type: typing.Optional[str] = pydantic.Field(None, description='The type of authentication to perform when connecting to a Redis target. Options include ``none`` , ``auth-token`` , and ``auth-role`` . The ``auth-token`` option requires an ``AuthPassword`` value to be provided. The ``auth-role`` option requires ``AuthUserName`` and ``AuthPassword`` values to be provided.\n')
    auth_user_name: typing.Optional[str] = pydantic.Field(None, description='The user name provided with the ``auth-role`` option of the ``AuthType`` setting for a Redis target endpoint.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='Transmission Control Protocol (TCP) port for the endpoint.\n')
    server_name: typing.Optional[str] = pydantic.Field(None, description='Fully qualified domain name of the endpoint.\n')
    ssl_ca_certificate_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the certificate authority (CA) that DMS uses to connect to your Redis target endpoint.\n')
    ssl_security_protocol: typing.Optional[str] = pydantic.Field(None, description='The connection to a Redis target endpoint using Transport Layer Security (TLS). Valid values include ``plaintext`` and ``ssl-encryption`` . The default is ``ssl-encryption`` . The ``ssl-encryption`` option makes an encrypted connection. Optionally, you can identify an Amazon Resource Name (ARN) for an SSL certificate authority (CA) using the ``SslCaCertificateArn`` setting. If an ARN isn\'t given for a CA, DMS uses the Amazon root CA. The ``plaintext`` option doesn\'t provide Transport Layer Security (TLS) encryption for traffic between endpoint and database.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-redissettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    redis_settings_property = dms.CfnEndpoint.RedisSettingsProperty(\n        auth_password="authPassword",\n        auth_type="authType",\n        auth_user_name="authUserName",\n        port=123,\n        server_name="serverName",\n        ssl_ca_certificate_arn="sslCaCertificateArn",\n        ssl_security_protocol="sslSecurityProtocol"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['auth_password', 'auth_type', 'auth_user_name', 'port', 'server_name', 'ssl_ca_certificate_arn', 'ssl_security_protocol']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.RedisSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.RedshiftSettingsProperty
class CfnEndpoint_RedshiftSettingsPropertyDef(BaseStruct):
    accept_any_date: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A value that indicates to allow any date format, including invalid formats such as 00/00/00 00:00:00, to be loaded without generating an error. You can choose ``true`` or ``false`` (the default). This parameter applies only to TIMESTAMP and DATE columns. Always use ACCEPTANYDATE with the DATEFORMAT parameter. If the date format for the data doesn't match the DATEFORMAT specification, Amazon Redshift inserts a NULL value into that field.\n")
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description='Code to run after connecting. This parameter should contain the code itself, not the name of a file containing the code.\n')
    bucket_folder: typing.Optional[str] = pydantic.Field(None, description='An S3 folder where the comma-separated-value (.csv) files are stored before being uploaded to the target Redshift cluster. For full load mode, AWS DMS converts source records into .csv files and loads them to the *BucketFolder/TableID* path. AWS DMS uses the Redshift ``COPY`` command to upload the .csv files to the target table. The files are deleted once the ``COPY`` operation has finished. For more information, see `COPY <https://docs.aws.amazon.com/redshift/latest/dg/r_COPY.html>`_ in the *Amazon Redshift Database Developer Guide* . For change-data-capture (CDC) mode, AWS DMS creates a *NetChanges* table, and loads the .csv files to this *BucketFolder/NetChangesTableID* path.\n')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description='The name of the intermediate S3 bucket used to store .csv files before uploading data to Redshift.\n')
    case_sensitive_names: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If Amazon Redshift is configured to support case sensitive schema names, set ``CaseSensitiveNames`` to ``true`` . The default is ``false`` .\n')
    comp_update: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="If you set ``CompUpdate`` to ``true`` Amazon Redshift applies automatic compression if the table is empty. This applies even if the table columns already have encodings other than ``RAW`` . If you set ``CompUpdate`` to ``false`` , automatic compression is disabled and existing column encodings aren't changed. The default is ``true`` .\n")
    connection_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='A value that sets the amount of time to wait (in milliseconds) before timing out, beginning from when you initially establish a connection.\n')
    date_format: typing.Optional[str] = pydantic.Field(None, description="The date format that you are using. Valid values are ``auto`` (case-sensitive), your date format string enclosed in quotes, or NULL. If this parameter is left unset (NULL), it defaults to a format of 'YYYY-MM-DD'. Using ``auto`` recognizes most strings, even some that aren't supported when you use a date format string. If your date and time values use formats different from each other, set this to ``auto`` .\n")
    empty_as_null: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that specifies whether AWS DMS should migrate empty CHAR and VARCHAR fields as NULL. A value of ``true`` sets empty CHAR and VARCHAR fields to null. The default is ``false`` .\n')
    encryption_mode: typing.Optional[str] = pydantic.Field(None, description='The type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either ``SSE_S3`` (the default) or ``SSE_KMS`` . .. epigraph:: For the ``ModifyEndpoint`` operation, you can change the existing value of the ``EncryptionMode`` parameter from ``SSE_KMS`` to ``SSE_S3`` . But you cant change the existing value from ``SSE_S3`` to ``SSE_KMS`` . To use ``SSE_S3`` , create an AWS Identity and Access Management (IAM) role with a policy that allows ``"arn:aws:s3:::*"`` to use the following actions: ``"s3:PutObject", "s3:ListBucket"``\n')
    explicit_ids: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='This setting is only valid for a full-load migration task. Set ``ExplicitIds`` to ``true`` to have tables with ``IDENTITY`` columns override their auto-generated values with explicit values loaded from the source data files used to populate the tables. The default is ``false`` .\n')
    file_transfer_upload_streams: typing.Union[int, float, None] = pydantic.Field(None, description='The number of threads used to upload a single file. This parameter accepts a value from 1 through 64. It defaults to 10. The number of parallel streams used to upload a single .csv file to an S3 bucket using S3 Multipart Upload. For more information, see `Multipart upload overview <https://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html>`_ . ``FileTransferUploadStreams`` accepts a value from 1 through 64. It defaults to 10.\n')
    load_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of time to wait (in milliseconds) before timing out of operations performed by AWS DMS on a Redshift cluster, such as Redshift COPY, INSERT, DELETE, and UPDATE.\n')
    map_boolean_as_boolean: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When true, lets Redshift migrate the boolean type as boolean. By default, Redshift migrates booleans as ``varchar(1)`` . You must set this setting on both the source and target endpoints for it to take effect.\n')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size (in KB) of any .csv file used to load data on an S3 bucket and transfer data to Amazon Redshift. It defaults to 1048576KB (1 GB).\n')
    remove_quotes: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that specifies to remove surrounding quotation marks from strings in the incoming data. All characters within the quotation marks, including delimiters, are retained. Choose ``true`` to remove quotation marks. The default is ``false`` .\n')
    replace_chars: typing.Optional[str] = pydantic.Field(None, description='A value that specifies to replaces the invalid characters specified in ``ReplaceInvalidChars`` , substituting the specified characters instead. The default is ``"?"`` .\n')
    replace_invalid_chars: typing.Optional[str] = pydantic.Field(None, description='A list of characters that you want to replace. Use with ``ReplaceChars`` .\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the Amazon Redshift endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the Amazon Redshift endpoint connection details.\n')
    server_side_encryption_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='The AWS KMS key ID. If you are using ``SSE_KMS`` for the ``EncryptionMode`` , provide this key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key.\n')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the IAM role that has access to the Amazon Redshift service. The role must allow the ``iam:PassRole`` action.\n')
    time_format: typing.Optional[str] = pydantic.Field(None, description="The time format that you want to use. Valid values are ``auto`` (case-sensitive), ``'timeformat_string'`` , ``'epochsecs'`` , or ``'epochmillisecs'`` . It defaults to 10. Using ``auto`` recognizes most strings, even some that aren't supported when you use a time format string. If your date and time values use formats different from each other, set this parameter to ``auto`` .\n")
    trim_blanks: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that specifies to remove the trailing white space characters from a VARCHAR string. This parameter applies only to columns with a VARCHAR data type. Choose ``true`` to remove unneeded white space. The default is ``false`` .\n')
    truncate_columns: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that specifies to truncate data in columns to the appropriate number of characters, so that the data fits in the column. This parameter applies only to columns with a VARCHAR or CHAR data type, and rows with a size of 4 MB or less. Choose ``true`` to truncate data. The default is ``false`` .\n')
    write_buffer_size: typing.Union[int, float, None] = pydantic.Field(None, description='The size (in KB) of the in-memory file write buffer used when generating .csv files on the local disk at the DMS replication instance. The default value is 1000 (buffer size is 1000KB).\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-redshiftsettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    redshift_settings_property = dms.CfnEndpoint.RedshiftSettingsProperty(\n        accept_any_date=False,\n        after_connect_script="afterConnectScript",\n        bucket_folder="bucketFolder",\n        bucket_name="bucketName",\n        case_sensitive_names=False,\n        comp_update=False,\n        connection_timeout=123,\n        date_format="dateFormat",\n        empty_as_null=False,\n        encryption_mode="encryptionMode",\n        explicit_ids=False,\n        file_transfer_upload_streams=123,\n        load_timeout=123,\n        map_boolean_as_boolean=False,\n        max_file_size=123,\n        remove_quotes=False,\n        replace_chars="replaceChars",\n        replace_invalid_chars="replaceInvalidChars",\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId",\n        server_side_encryption_kms_key_id="serverSideEncryptionKmsKeyId",\n        service_access_role_arn="serviceAccessRoleArn",\n        time_format="timeFormat",\n        trim_blanks=False,\n        truncate_columns=False,\n        write_buffer_size=123\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['accept_any_date', 'after_connect_script', 'bucket_folder', 'bucket_name', 'case_sensitive_names', 'comp_update', 'connection_timeout', 'date_format', 'empty_as_null', 'encryption_mode', 'explicit_ids', 'file_transfer_upload_streams', 'load_timeout', 'map_boolean_as_boolean', 'max_file_size', 'remove_quotes', 'replace_chars', 'replace_invalid_chars', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id', 'server_side_encryption_kms_key_id', 'service_access_role_arn', 'time_format', 'trim_blanks', 'truncate_columns', 'write_buffer_size']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.RedshiftSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.S3SettingsProperty
class CfnEndpoint_S3SettingsPropertyDef(BaseStruct):
    add_column_name: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='An optional parameter that, when set to ``true`` or ``y`` , you can use to add column name information to the .csv output file. The default value is ``false`` . Valid values are ``true`` , ``false`` , ``y`` , and ``n`` .\n')
    add_trailing_padding_character: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Use the S3 target endpoint setting ``AddTrailingPaddingCharacter`` to add padding on string data. The default value is ``false`` .\n')
    bucket_folder: typing.Optional[str] = pydantic.Field(None, description="An optional parameter to set a folder name in the S3 bucket. If provided, tables are created in the path ``*bucketFolder* / *schema_name* / *table_name* /`` . If this parameter isn't specified, the path used is ``*schema_name* / *table_name* /`` .\n")
    bucket_name: typing.Optional[str] = pydantic.Field(None, description='The name of the S3 bucket.\n')
    canned_acl_for_objects: typing.Optional[str] = pydantic.Field(None, description='A value that enables AWS DMS to specify a predefined (canned) access control list (ACL) for objects created in an Amazon S3 bucket as .csv or .parquet files. For more information about Amazon S3 canned ACLs, see `Canned ACL <https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl>`_ in the *Amazon S3 Developer Guide* . The default value is NONE. Valid values include NONE, PRIVATE, PUBLIC_READ, PUBLIC_READ_WRITE, AUTHENTICATED_READ, AWS_EXEC_READ, BUCKET_OWNER_READ, and BUCKET_OWNER_FULL_CONTROL.\n')
    cdc_inserts_and_updates: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A value that enables a change data capture (CDC) load to write INSERT and UPDATE operations to .csv or .parquet (columnar storage) output files. The default setting is ``false`` , but when ``CdcInsertsAndUpdates`` is set to ``true`` or ``y`` , only INSERTs and UPDATEs from the source database are migrated to the .csv or .parquet file. For .csv file format only, how these INSERTs and UPDATEs are recorded depends on the value of the ``IncludeOpForFullLoad`` parameter. If ``IncludeOpForFullLoad`` is set to ``true`` , the first field of every CDC record is set to either ``I`` or ``U`` to indicate INSERT and UPDATE operations at the source. But if ``IncludeOpForFullLoad`` is set to ``false`` , CDC records are written without an indication of INSERT or UPDATE operations at the source. For more information about how these settings work together, see `Indicating Source DB Operations in Migrated S3 Data <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring.InsertOps>`_ in the *AWS Database Migration Service User Guide* . .. epigraph:: AWS DMS supports the use of the ``CdcInsertsAndUpdates`` parameter in versions 3.3.1 and later. ``CdcInsertsOnly`` and ``CdcInsertsAndUpdates`` can't both be set to ``true`` for the same endpoint. Set either ``CdcInsertsOnly`` or ``CdcInsertsAndUpdates`` to ``true`` for the same endpoint, but not both.\n")
    cdc_inserts_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="A value that enables a change data capture (CDC) load to write only INSERT operations to .csv or columnar storage (.parquet) output files. By default (the ``false`` setting), the first field in a .csv or .parquet record contains the letter I (INSERT), U (UPDATE), or D (DELETE). These values indicate whether the row was inserted, updated, or deleted at the source database for a CDC load to the target. If ``CdcInsertsOnly`` is set to ``true`` or ``y`` , only INSERTs from the source database are migrated to the .csv or .parquet file. For .csv format only, how these INSERTs are recorded depends on the value of ``IncludeOpForFullLoad`` . If ``IncludeOpForFullLoad`` is set to ``true`` , the first field of every CDC record is set to I to indicate the INSERT operation at the source. If ``IncludeOpForFullLoad`` is set to ``false`` , every CDC record is written without a first field to indicate the INSERT operation at the source. For more information about how these settings work together, see `Indicating Source DB Operations in Migrated S3 Data <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring.InsertOps>`_ in the *AWS Database Migration Service User Guide* . .. epigraph:: AWS DMS supports the interaction described preceding between the ``CdcInsertsOnly`` and ``IncludeOpForFullLoad`` parameters in versions 3.1.4 and later. ``CdcInsertsOnly`` and ``CdcInsertsAndUpdates`` can't both be set to ``true`` for the same endpoint. Set either ``CdcInsertsOnly`` or ``CdcInsertsAndUpdates`` to ``true`` for the same endpoint, but not both.\n")
    cdc_max_batch_interval: typing.Union[int, float, None] = pydantic.Field(None, description='Maximum length of the interval, defined in seconds, after which to output a file to Amazon S3. When ``CdcMaxBatchInterval`` and ``CdcMinFileSize`` are both specified, the file write is triggered by whichever parameter condition is met first within an AWS DMS CloudFormation template. The default value is 60 seconds.\n')
    cdc_min_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='Minimum file size, defined in kilobytes, to reach for a file output to Amazon S3. When ``CdcMinFileSize`` and ``CdcMaxBatchInterval`` are both specified, the file write is triggered by whichever parameter condition is met first within an AWS DMS CloudFormation template. The default value is 32 MB.\n')
    cdc_path: typing.Optional[str] = pydantic.Field(None, description="Specifies the folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it's optional. If ``CdcPath`` is set, AWS DMS reads CDC files from this path and replicates the data changes to the target endpoint. For an S3 target if you set ```PreserveTransactions`` <https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-PreserveTransactions>`_ to ``true`` , AWS DMS verifies that you have set this parameter to a folder path on your S3 target where AWS DMS can save the transaction order for the CDC load. AWS DMS creates this CDC folder path in either your S3 target working directory or the S3 target location specified by ```BucketFolder`` <https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketFolder>`_ and ```BucketName`` <https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-BucketName>`_ . For example, if you specify ``CdcPath`` as ``MyChangedData`` , and you specify ``BucketName`` as ``MyTargetBucket`` but do not specify ``BucketFolder`` , AWS DMS creates the CDC folder path following: ``MyTargetBucket/MyChangedData`` . If you specify the same ``CdcPath`` , and you specify ``BucketName`` as ``MyTargetBucket`` and ``BucketFolder`` as ``MyTargetData`` , AWS DMS creates the CDC folder path following: ``MyTargetBucket/MyTargetData/MyChangedData`` . For more information on CDC including transaction order on an S3 target, see `Capturing data changes (CDC) including transaction order on the S3 target <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.EndpointSettings.CdcPath>`_ . .. epigraph:: This setting is supported in AWS DMS versions 3.4.2 and later.\n")
    compression_type: typing.Optional[str] = pydantic.Field(None, description="An optional parameter. When set to GZIP it enables the service to compress the target files. To allow the service to write the target files uncompressed, either set this parameter to NONE (the default) or don't specify the parameter at all. This parameter applies to both .csv and .parquet file formats.\n")
    csv_delimiter: typing.Optional[str] = pydantic.Field(None, description='The delimiter used to separate columns in the .csv file for both source and target. The default is a comma.\n')
    csv_no_sup_value: typing.Optional[str] = pydantic.Field(None, description='This setting only applies if your Amazon S3 output files during a change data capture (CDC) load are written in .csv format. If ```UseCsvNoSupValue`` <https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-UseCsvNoSupValue>`_ is set to true, specify a string value that you want AWS DMS to use for all columns not included in the supplemental log. If you do not specify a string value, AWS DMS uses the null value for these columns regardless of the ``UseCsvNoSupValue`` setting. .. epigraph:: This setting is supported in AWS DMS versions 3.4.1 and later.\n')
    csv_null_value: typing.Optional[str] = pydantic.Field(None, description='An optional parameter that specifies how AWS DMS treats null values. While handling the null value, you can use this parameter to pass a user-defined string as null when writing to the target. For example, when target columns are not nullable, you can use this option to differentiate between the empty string value and the null value. So, if you set this parameter value to the empty string ("" or \'\'), AWS DMS treats the empty string as the null value instead of ``NULL`` . The default value is ``NULL`` . Valid values include any valid string.\n')
    csv_row_delimiter: typing.Optional[str] = pydantic.Field(None, description='The delimiter used to separate rows in the .csv file for both source and target. The default is a carriage return ( ``\\n`` ).\n')
    data_format: typing.Optional[str] = pydantic.Field(None, description='The format of the data that you want to use for output. You can choose one of the following:. - ``csv`` : This is a row-based file format with comma-separated values (.csv). - ``parquet`` : Apache Parquet (.parquet) is a columnar storage file format that features efficient compression and provides faster query response.\n')
    data_page_size: typing.Union[int, float, None] = pydantic.Field(None, description='The size of one data page in bytes. This parameter defaults to 1024 * 1024 bytes (1 MiB). This number is used for .parquet file format only.\n')
    date_partition_delimiter: typing.Optional[str] = pydantic.Field(None, description='Specifies a date separating delimiter to use during folder partitioning. The default value is ``SLASH`` . Use this parameter when ``DatePartitionedEnabled`` is set to ``true`` .\n')
    date_partition_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When set to ``true`` , this parameter partitions S3 bucket folders based on transaction commit dates. The default value is ``false`` . For more information about date-based folder partitioning, see `Using date-based folder partitioning <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.DatePartitioning>`_ .\n')
    date_partition_sequence: typing.Optional[str] = pydantic.Field(None, description='Identifies the sequence of the date format to use during folder partitioning. The default value is ``YYYYMMDD`` . Use this parameter when ``DatePartitionedEnabled`` is set to ``true`` .\n')
    date_partition_timezone: typing.Optional[str] = pydantic.Field(None, description='When creating an S3 target endpoint, set ``DatePartitionTimezone`` to convert the current UTC time into a specified time zone. The conversion occurs when a date partition folder is created and a change data capture (CDC) file name is generated. The time zone format is Area/Location. Use this parameter when ``DatePartitionedEnabled`` is set to ``true`` , as shown in the following example. ``s3-settings=\'{"DatePartitionEnabled": true, "DatePartitionSequence": "YYYYMMDDHH", "DatePartitionDelimiter": "SLASH", "DatePartitionTimezone":" *Asia/Seoul* ", "BucketName": "dms-nattarat-test"}\'``\n')
    dict_page_size_limit: typing.Union[int, float, None] = pydantic.Field(None, description='The maximum size of an encoded dictionary page of a column. If the dictionary page exceeds this, this column is stored using an encoding type of ``PLAIN`` . This parameter defaults to 1024 * 1024 bytes (1 MiB), the maximum size of a dictionary page before it reverts to ``PLAIN`` encoding. This size is used for .parquet file format only.\n')
    enable_statistics: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that enables statistics for Parquet pages and row groups. Choose ``true`` to enable statistics, ``false`` to disable. Statistics include ``NULL`` , ``DISTINCT`` , ``MAX`` , and ``MIN`` values. This parameter defaults to ``true`` . This value is used for .parquet file format only.\n')
    encoding_type: typing.Optional[str] = pydantic.Field(None, description="The type of encoding that you're using:. - ``RLE_DICTIONARY`` uses a combination of bit-packing and run-length encoding to store repeated values more efficiently. This is the default. - ``PLAIN`` doesn't use encoding at all. Values are stored as they are. - ``PLAIN_DICTIONARY`` builds a dictionary of the values encountered in a given column. The dictionary is stored in a dictionary page for each column chunk.\n")
    encryption_mode: typing.Optional[str] = pydantic.Field(None, description='The type of server-side encryption that you want to use for your data. This encryption type is part of the endpoint settings or the extra connections attributes for Amazon S3. You can choose either ``SSE_S3`` (the default) or ``SSE_KMS`` . .. epigraph:: For the ``ModifyEndpoint`` operation, you can change the existing value of the ``EncryptionMode`` parameter from ``SSE_KMS`` to ``SSE_S3`` . But you cant change the existing value from ``SSE_S3`` to ``SSE_KMS`` . To use ``SSE_S3`` , you need an IAM role with permission to allow ``"arn:aws:s3:::dms-*"`` to use the following actions: - ``s3:CreateBucket`` - ``s3:ListBucket`` - ``s3:DeleteBucket`` - ``s3:GetBucketLocation`` - ``s3:GetObject`` - ``s3:PutObject`` - ``s3:DeleteObject`` - ``s3:GetObjectVersion`` - ``s3:GetBucketPolicy`` - ``s3:PutBucketPolicy`` - ``s3:DeleteBucketPolicy``\n')
    expected_bucket_owner: typing.Optional[str] = pydantic.Field(None, description='To specify a bucket owner and prevent sniping, you can use the ``ExpectedBucketOwner`` endpoint setting. Example: ``--s3-settings=\'{"ExpectedBucketOwner": " *AWS_Account_ID* "}\'`` When you make a request to test a connection or perform a migration, S3 checks the account ID of the bucket owner against the specified parameter.\n')
    external_table_definition: typing.Optional[str] = pydantic.Field(None, description='The external table definition. Conditional: If ``S3`` is used as a source then ``ExternalTableDefinition`` is required.\n')
    glue_catalog_generation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When true, allows AWS Glue to catalog your S3 bucket. Creating an AWS Glue catalog lets you use Athena to query your data.\n')
    ignore_header_rows: typing.Union[int, float, None] = pydantic.Field(None, description='When this value is set to 1, AWS DMS ignores the first row header in a .csv file. A value of 1 turns on the feature; a value of 0 turns off the feature. The default is 0.\n')
    include_op_for_full_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that enables a full load to write INSERT operations to the comma-separated value (.csv) output files only to indicate how the rows were added to the source database. .. epigraph:: AWS DMS supports the ``IncludeOpForFullLoad`` parameter in versions 3.1.4 and later. For full load, records can only be inserted. By default (the ``false`` setting), no information is recorded in these output files for a full load to indicate that the rows were inserted at the source database. If ``IncludeOpForFullLoad`` is set to ``true`` or ``y`` , the INSERT is recorded as an I annotation in the first field of the .csv file. This allows the format of your target records from a full load to be consistent with the target records from a CDC load. .. epigraph:: This setting works together with the ``CdcInsertsOnly`` and the ``CdcInsertsAndUpdates`` parameters for output to .csv files only. For more information about how these settings work together, see `Indicating Source DB Operations in Migrated S3 Data <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring.InsertOps>`_ in the *AWS Database Migration Service User Guide* .\n')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='A value that specifies the maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load. The default value is 1,048,576 KB (1 GB). Valid values include 1 to 1,048,576.\n')
    parquet_timestamp_in_millisecond: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that specifies the precision of any ``TIMESTAMP`` column values that are written to an Amazon S3 object file in .parquet format. .. epigraph:: AWS DMS supports the ``ParquetTimestampInMillisecond`` parameter in versions 3.1.4 and later. When ``ParquetTimestampInMillisecond`` is set to ``true`` or ``y`` , AWS DMS writes all ``TIMESTAMP`` columns in a .parquet formatted file with millisecond precision. Otherwise, DMS writes them with microsecond precision. Currently, Amazon Athena and AWS Glue can handle only millisecond precision for ``TIMESTAMP`` values. Set this parameter to ``true`` for S3 endpoint object files that are .parquet formatted only if you plan to query or process the data with Athena or AWS Glue . .. epigraph:: AWS DMS writes any ``TIMESTAMP`` column values written to an S3 file in .csv format with microsecond precision. Setting ``ParquetTimestampInMillisecond`` has no effect on the string format of the timestamp column value that is inserted by setting the ``TimestampColumnName`` parameter.\n')
    parquet_version: typing.Optional[str] = pydantic.Field(None, description='The version of the Apache Parquet format that you want to use: ``parquet_1_0`` (the default) or ``parquet_2_0`` .\n')
    preserve_transactions: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='If this setting is set to ``true`` , AWS DMS saves the transaction order for a change data capture (CDC) load on the Amazon S3 target specified by ```CdcPath`` <https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CdcPath>`_ . For more information, see `Capturing data changes (CDC) including transaction order on the S3 target <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.EndpointSettings.CdcPath>`_ . .. epigraph:: This setting is supported in AWS DMS versions 3.4.2 and later.\n')
    rfc4180: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='For an S3 source, when this value is set to ``true`` or ``y`` , each leading double quotation mark has to be followed by an ending double quotation mark. This formatting complies with RFC 4180. When this value is set to ``false`` or ``n`` , string literals are copied to the target as is. In this case, a delimiter (row or column) signals the end of the field. Thus, you can\'t use a delimiter as part of the string, because it signals the end of the value. For an S3 target, an optional parameter used to set behavior to comply with RFC 4180 for data migrated to Amazon S3 using .csv file format only. When this value is set to ``true`` or ``y`` using Amazon S3 as a target, if the data has quotation marks or newline characters in it, AWS DMS encloses the entire column with an additional pair of double quotation marks ("). Every quotation mark within the data is repeated twice. The default value is ``true`` . Valid values include ``true`` , ``false`` , ``y`` , and ``n`` .\n')
    row_group_length: typing.Union[int, float, None] = pydantic.Field(None, description='The number of rows in a row group. A smaller row group size provides faster reads. But as the number of row groups grows, the slower writes become. This parameter defaults to 10,000 rows. This number is used for .parquet file format only. If you choose a value larger than the maximum, ``RowGroupLength`` is set to the max row group length in bytes (64 * 1024 * 1024).\n')
    server_side_encryption_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='If you are using ``SSE_KMS`` for the ``EncryptionMode`` , provide the AWS KMS key ID. The key that you use needs an attached policy that enables IAM user permissions and allows use of the key. Here is a CLI example: ``aws dms create-endpoint --endpoint-identifier *value* --endpoint-type target --engine-name s3 --s3-settings ServiceAccessRoleArn= *value* ,BucketFolder= *value* ,BucketName= *value* ,EncryptionMode=SSE_KMS,ServerSideEncryptionKmsKeyId= *value*``\n')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='A required parameter that specifies the Amazon Resource Name (ARN) used by the service to access the IAM role. The role must allow the ``iam:PassRole`` action. It enables AWS DMS to read and write objects from an S3 bucket.\n')
    timestamp_column_name: typing.Optional[str] = pydantic.Field(None, description='A value that when nonblank causes AWS DMS to add a column with timestamp information to the endpoint data for an Amazon S3 target. .. epigraph:: AWS DMS supports the ``TimestampColumnName`` parameter in versions 3.1.4 and later. AWS DMS includes an additional ``STRING`` column in the .csv or .parquet object files of your migrated data when you set ``TimestampColumnName`` to a nonblank value. For a full load, each row of this timestamp column contains a timestamp for when the data was transferred from the source to the target by DMS. For a change data capture (CDC) load, each row of the timestamp column contains the timestamp for the commit of that row in the source database. The string format for this timestamp column value is ``yyyy-MM-dd HH:mm:ss.SSSSSS`` . By default, the precision of this value is in microseconds. For a CDC load, the rounding of the precision depends on the commit timestamp supported by DMS for the source database. When the ``AddColumnName`` parameter is set to ``true`` , DMS also includes a name for the timestamp column that you set with ``TimestampColumnName`` .\n')
    use_csv_no_sup_value: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="This setting applies if the S3 output files during a change data capture (CDC) load are written in .csv format. If this setting is set to ``true`` for columns not included in the supplemental log, AWS DMS uses the value specified by ```CsvNoSupValue`` <https://docs.aws.amazon.com/dms/latest/APIReference/API_S3Settings.html#DMS-Type-S3Settings-CsvNoSupValue>`_ . If this setting isn't set or is set to ``false`` , AWS DMS uses the null value for these columns. .. epigraph:: This setting is supported in AWS DMS versions 3.4.1 and later.\n")
    use_task_start_time_for_full_load_timestamp: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='When set to true, this parameter uses the task start time as the timestamp column value instead of the time data is written to target. For full load, when ``useTaskStartTimeForFullLoadTimestamp`` is set to ``true`` , each row of the timestamp column contains the task start time. For CDC loads, each row of the timestamp column contains the transaction commit time. When ``useTaskStartTimeForFullLoadTimestamp`` is set to ``false`` , the full load timestamp in the timestamp column increments with the time data arrives at the target.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-s3settings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    s3_settings_property = dms.CfnEndpoint.S3SettingsProperty(\n        add_column_name=False,\n        add_trailing_padding_character=False,\n        bucket_folder="bucketFolder",\n        bucket_name="bucketName",\n        canned_acl_for_objects="cannedAclForObjects",\n        cdc_inserts_and_updates=False,\n        cdc_inserts_only=False,\n        cdc_max_batch_interval=123,\n        cdc_min_file_size=123,\n        cdc_path="cdcPath",\n        compression_type="compressionType",\n        csv_delimiter="csvDelimiter",\n        csv_no_sup_value="csvNoSupValue",\n        csv_null_value="csvNullValue",\n        csv_row_delimiter="csvRowDelimiter",\n        data_format="dataFormat",\n        data_page_size=123,\n        date_partition_delimiter="datePartitionDelimiter",\n        date_partition_enabled=False,\n        date_partition_sequence="datePartitionSequence",\n        date_partition_timezone="datePartitionTimezone",\n        dict_page_size_limit=123,\n        enable_statistics=False,\n        encoding_type="encodingType",\n        encryption_mode="encryptionMode",\n        expected_bucket_owner="expectedBucketOwner",\n        external_table_definition="externalTableDefinition",\n        glue_catalog_generation=False,\n        ignore_header_rows=123,\n        include_op_for_full_load=False,\n        max_file_size=123,\n        parquet_timestamp_in_millisecond=False,\n        parquet_version="parquetVersion",\n        preserve_transactions=False,\n        rfc4180=False,\n        row_group_length=123,\n        server_side_encryption_kms_key_id="serverSideEncryptionKmsKeyId",\n        service_access_role_arn="serviceAccessRoleArn",\n        timestamp_column_name="timestampColumnName",\n        use_csv_no_sup_value=False,\n        use_task_start_time_for_full_load_timestamp=False\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['add_column_name', 'add_trailing_padding_character', 'bucket_folder', 'bucket_name', 'canned_acl_for_objects', 'cdc_inserts_and_updates', 'cdc_inserts_only', 'cdc_max_batch_interval', 'cdc_min_file_size', 'cdc_path', 'compression_type', 'csv_delimiter', 'csv_no_sup_value', 'csv_null_value', 'csv_row_delimiter', 'data_format', 'data_page_size', 'date_partition_delimiter', 'date_partition_enabled', 'date_partition_sequence', 'date_partition_timezone', 'dict_page_size_limit', 'enable_statistics', 'encoding_type', 'encryption_mode', 'expected_bucket_owner', 'external_table_definition', 'glue_catalog_generation', 'ignore_header_rows', 'include_op_for_full_load', 'max_file_size', 'parquet_timestamp_in_millisecond', 'parquet_version', 'preserve_transactions', 'rfc4180', 'row_group_length', 'server_side_encryption_kms_key_id', 'service_access_role_arn', 'timestamp_column_name', 'use_csv_no_sup_value', 'use_task_start_time_for_full_load_timestamp']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.S3SettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpoint.SybaseSettingsProperty
class CfnEndpoint_SybaseSettingsPropertyDef(BaseStruct):
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description="The full Amazon Resource Name (ARN) of the IAM role that specifies AWS DMS as the trusted entity and grants the required permissions to access the value in ``SecretsManagerSecret`` . The role must allow the ``iam:PassRole`` action. ``SecretsManagerSecret`` has the value of the AWS Secrets Manager secret that allows access to the SAP ASE endpoint. .. epigraph:: You can specify one of two sets of values for these permissions. You can specify the values for this setting and ``SecretsManagerSecretId`` . Or you can specify clear-text values for ``UserName`` , ``Password`` , ``ServerName`` , and ``Port`` . You can't specify both. For more information on creating this ``SecretsManagerSecret`` , the corresponding ``SecretsManagerAccessRoleArn`` , and the ``SecretsManagerSecretId`` that is required to access it, see `Using secrets to access AWS Database Migration Service resources <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#security-iam-secretsmanager>`_ in the *AWS Database Migration Service User Guide* .\n")
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The full ARN, partial ARN, or display name of the ``SecretsManagerSecret`` that contains the SAP SAE endpoint connection details.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-endpoint-sybasesettings.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    sybase_settings_property = dms.CfnEndpoint.SybaseSettingsProperty(\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['secrets_manager_access_role_arn', 'secrets_manager_secret_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint.SybaseSettingsProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnMigrationProject.DataProviderDescriptorProperty
class CfnMigrationProject_DataProviderDescriptorPropertyDef(BaseStruct):
    data_provider_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the data provider.\n')
    data_provider_identifier: typing.Optional[str] = pydantic.Field(None, description='')
    data_provider_name: typing.Optional[str] = pydantic.Field(None, description='The user-friendly name of the data provider.\n')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='The ARN of the role used to access AWS Secrets Manager.\n')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='The identifier of the AWS Secrets Manager Secret used to store access credentials for the data provider.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-migrationproject-dataproviderdescriptor.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    data_provider_descriptor_property = dms.CfnMigrationProject.DataProviderDescriptorProperty(\n        data_provider_arn="dataProviderArn",\n        data_provider_identifier="dataProviderIdentifier",\n        data_provider_name="dataProviderName",\n        secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n        secrets_manager_secret_id="secretsManagerSecretId"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['data_provider_arn', 'data_provider_identifier', 'data_provider_name', 'secrets_manager_access_role_arn', 'secrets_manager_secret_id']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnMigrationProject.DataProviderDescriptorProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnMigrationProject.SchemaConversionApplicationAttributesProperty
class CfnMigrationProject_SchemaConversionApplicationAttributesPropertyDef(BaseStruct):
    s3_bucket_path: typing.Optional[str] = pydantic.Field(None, description='')
    s3_bucket_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    _init_params: typing.ClassVar[list[str]] = ['s3_bucket_path', 's3_bucket_role_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnMigrationProject.SchemaConversionApplicationAttributesProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnReplicationConfig.ComputeConfigProperty
class CfnReplicationConfig_ComputeConfigPropertyDef(BaseStruct):
    max_capacity_units: typing.Union[_REQUIRED_INIT_PARAM, int, float] = pydantic.Field(REQUIRED_INIT_PARAM, description='Specifies the maximum value of the AWS DMS capacity units (DCUs) for which a given AWS DMS Serverless replication can be provisioned. A single DCU is 2GB of RAM, with 1 DCU as the minimum value allowed. The list of valid DCU values includes 1, 2, 4, 8, 16, 32, 64, 128, 192, 256, and 384. So, the maximum value that you can specify for AWS DMS Serverless is 384. The ``MaxCapacityUnits`` parameter is the only DCU parameter you are required to specify.\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The Availability Zone where the AWS DMS Serverless replication using this configuration will run. The default value is a random, system-chosen Availability Zone in the configuration\'s AWS Region , for example, ``"us-west-2"`` . You can\'t set this parameter if the ``MultiAZ`` parameter is set to ``true`` .\n')
    dns_name_servers: typing.Optional[str] = pydantic.Field(None, description='A list of custom DNS name servers supported for the AWS DMS Serverless replication to access your source or target database. This list overrides the default name servers supported by the AWS DMS Serverless replication. You can specify a comma-separated list of internet addresses for up to four DNS name servers. For example: ``"1.1.1.1,2.2.2.2,3.3.3.3,4.4.4.4"``\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="An AWS Key Management Service ( AWS KMS ) key Amazon Resource Name (ARN) that is used to encrypt the data during AWS DMS Serverless replication. If you don't specify a value for the ``KmsKeyId`` parameter, AWS DMS uses your default encryption key. AWS KMS creates the default encryption key for your Amazon Web Services account. Your AWS account has a different default encryption key for each AWS Region .\n")
    min_capacity_units: typing.Union[int, float, None] = pydantic.Field(None, description="Specifies the minimum value of the AWS DMS capacity units (DCUs) for which a given AWS DMS Serverless replication can be provisioned. A single DCU is 2GB of RAM, with 1 DCU as the minimum value allowed. The list of valid DCU values includes 1, 2, 4, 8, 16, 32, 64, 128, 192, 256, and 384. So, the minimum DCU value that you can specify for AWS DMS Serverless is 1. If you don't set this value, AWS DMS sets this parameter to the minimum DCU value allowed, 1. If there is no current source activity, AWS DMS scales down your replication until it reaches the value specified in ``MinCapacityUnits`` .\n")
    multi_az: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether the AWS DMS Serverless replication is a Multi-AZ deployment. You can't set the ``AvailabilityZone`` parameter if the ``MultiAZ`` parameter is set to ``true`` .\n")
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range during which system maintenance can occur for the AWS DMS Serverless replication, in Universal Coordinated Time (UTC). The format is ``ddd:hh24:mi-ddd:hh24:mi`` . The default is a 30-minute window selected at random from an 8-hour block of time per AWS Region . This maintenance occurs on a random day of the week. Valid values for days of the week include ``Mon`` , ``Tue`` , ``Wed`` , ``Thu`` , ``Fri`` , ``Sat`` , and ``Sun`` . Constraints include a minimum 30-minute window.\n')
    replication_subnet_group_id: typing.Optional[str] = pydantic.Field(None, description='Specifies a subnet group identifier to associate with the AWS DMS Serverless replication.\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Specifies the virtual private cloud (VPC) security group to use with the AWS DMS Serverless replication. The VPC security group must work with the VPC containing the replication.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-dms-replicationconfig-computeconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    compute_config_property = dms.CfnReplicationConfig.ComputeConfigProperty(\n        max_capacity_units=123,\n\n        # the properties below are optional\n        availability_zone="availabilityZone",\n        dns_name_servers="dnsNameServers",\n        kms_key_id="kmsKeyId",\n        min_capacity_units=123,\n        multi_az=False,\n        preferred_maintenance_window="preferredMaintenanceWindow",\n        replication_subnet_group_id="replicationSubnetGroupId",\n        vpc_security_group_ids=["vpcSecurityGroupIds"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['max_capacity_units', 'availability_zone', 'dns_name_servers', 'kms_key_id', 'min_capacity_units', 'multi_az', 'preferred_maintenance_window', 'replication_subnet_group_id', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationConfig.ComputeConfigProperty'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnCertificate
class CfnCertificateDef(BaseCfnResource):
    certificate_identifier: typing.Optional[str] = pydantic.Field(None, description="A customer-assigned name for the certificate. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen or contain two consecutive hyphens.\n")
    certificate_pem: typing.Optional[str] = pydantic.Field(None, description='The contents of a ``.pem`` file, which contains an X.509 certificate.\n')
    certificate_wallet: typing.Optional[str] = pydantic.Field(None, description='The location of an imported Oracle Wallet certificate for use with SSL. An example is: ``filebase64("${path.root}/rds-ca-2019-root.sso")``')
    _init_params: typing.ClassVar[list[str]] = ['certificate_identifier', 'certificate_pem', 'certificate_wallet']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnCertificate'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnCertificateDefConfig] = pydantic.Field(None)


class CfnCertificateDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnCertificateDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnCertificateDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnCertificateDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnCertificateDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnCertificateDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnCertificateDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnCertificateDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnCertificateDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnCertificateDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnCertificateDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnCertificateDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnCertificateDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnCertificateDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')

class CfnCertificateDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnCertificateDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCertificateDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnCertificateDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCertificateDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnCertificateDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnCertificateDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnCertificateDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnCertificateDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnCertificateDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnCertificateDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnCertificateDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnCertificateDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnCertificateDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnDataProvider
class CfnDataProviderDef(BaseCfnResource):
    engine: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of database engine for the data provider. Valid values include ``"aurora"`` , ``"aurora-postgresql"`` , ``"mysql"`` , ``"oracle"`` , ``"postgres"`` , ``"sqlserver"`` , ``redshift`` , ``mariadb`` , ``mongodb`` , and ``docdb`` . A value of ``"aurora"`` represents Amazon Aurora MySQL-Compatible Edition.\n')
    data_provider_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the data provider. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    data_provider_name: typing.Optional[str] = pydantic.Field(None, description='The name of the data provider.\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the data provider. Descriptions can have up to 31 characters. A description can contain only ASCII letters, digits, and hyphens ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter.\n")
    exact_settings: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='The property describes the exact settings which can be modified. Default: - false\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'data_provider_identifier', 'data_provider_name', 'description', 'exact_settings', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnDataProvider'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnDataProviderDefConfig] = pydantic.Field(None)


class CfnDataProviderDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnDataProviderDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnDataProviderDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnDataProviderDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnDataProviderDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnDataProviderDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnDataProviderDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnDataProviderDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnDataProviderDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnDataProviderDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnDataProviderDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnDataProviderDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnDataProviderDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnDataProviderDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnDataProviderDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnDataProviderDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDataProviderDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnDataProviderDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDataProviderDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnDataProviderDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnDataProviderDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnDataProviderDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnDataProviderDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnDataProviderDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnDataProviderDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnDataProviderDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnDataProviderDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnDataProviderDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnEndpoint
class CfnEndpointDef(BaseCfnResource):
    endpoint_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of endpoint. Valid values are ``source`` and ``target`` .\n')
    engine_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of engine for the endpoint, depending on the ``EndpointType`` value. *Valid values* : ``mysql`` | ``oracle`` | ``postgres`` | ``mariadb`` | ``aurora`` | ``aurora-postgresql`` | ``opensearch`` | ``redshift`` | ``redshift-serverless`` | ``s3`` | ``db2`` | ``azuredb`` | ``sybase`` | ``dynamodb`` | ``mongodb`` | ``kinesis`` | ``kafka`` | ``elasticsearch`` | ``docdb`` | ``sqlserver`` | ``neptune``\n')
    certificate_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the certificate.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description="The name of the endpoint database. For a MySQL source or target endpoint, don't specify ``DatabaseName`` . To migrate to a specific database, use this setting and ``targetDbType`` .\n")
    doc_db_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_DocDbSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target DocumentDB endpoint. For more information about other available settings, see `Using extra connections attributes with Amazon DocumentDB as a source <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.DocumentDB.html#CHAP_Source.DocumentDB.ECAs>`_ and `Using Amazon DocumentDB as a target for AWS Database Migration Service <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DocumentDB.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    dynamo_db_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_DynamoDbSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Amazon DynamoDB endpoint. For information about other available settings, see `Using object mapping to migrate data to DynamoDB <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html#CHAP_Target.DynamoDB.ObjectMapping>`_ in the *AWS Database Migration Service User Guide* .\n')
    elasticsearch_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_ElasticsearchSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target OpenSearch endpoint. For more information about the available settings, see `Extra connection attributes when using OpenSearch as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Elasticsearch.html#CHAP_Target.Elasticsearch.Configuration>`_ in the *AWS Database Migration Service User Guide* .\n')
    endpoint_identifier: typing.Optional[str] = pydantic.Field(None, description="The database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    extra_connection_attributes: typing.Optional[str] = pydantic.Field(None, description='Additional attributes associated with the connection. Each attribute is specified as a name-value pair associated by an equal sign (=). Multiple attributes are separated by a semicolon (;) with no additional white space. For information on the attributes available for connecting your source or target endpoint, see `Working with AWS DMS Endpoints <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Endpoints.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    gcp_my_sql_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_GcpMySQLSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source GCP MySQL endpoint. These settings are much the same as the settings for any MySQL-compatible endpoint. For more information, see `Extra connection attributes when using MySQL as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    ibm_db2_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_IbmDb2SettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source IBM Db2 LUW endpoint. For information about other available settings, see `Extra connection attributes when using Db2 LUW as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.DB2.html#CHAP_Source.DB2.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    kafka_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_KafkaSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Apache Kafka endpoint. For more information about other available settings, see `Using object mapping to migrate data to a Kafka topic <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html#CHAP_Target.Kafka.ObjectMapping>`_ in the *AWS Database Migration Service User Guide* .\n')
    kinesis_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_KinesisSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target endpoint for Amazon Kinesis Data Streams. For more information about other available settings, see `Using object mapping to migrate data to a Kinesis data stream <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kinesis.html#CHAP_Target.Kinesis.ObjectMapping>`_ in the *AWS Database Migration Service User Guide* .\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="An AWS KMS key identifier that is used to encrypt the connection parameters for the endpoint. If you don't specify a value for the ``KmsKeyId`` parameter, AWS DMS uses your default encryption key. AWS KMS creates the default encryption key for your AWS account . Your AWS account has a different default encryption key for each AWS Region .\n")
    microsoft_sql_server_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_MicrosoftSqlServerSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target Microsoft SQL Server endpoint. For information about other available settings, see `Extra connection attributes when using SQL Server as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.SQLServer.html#CHAP_Source.SQLServer.ConnectionAttrib>`_ and `Extra connection attributes when using SQL Server as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.SQLServer.html#CHAP_Target.SQLServer.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    mongo_db_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_MongoDbSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source MongoDB endpoint. For more information about the available settings, see `Using MongoDB as a target for AWS Database Migration Service <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MongoDB.html#CHAP_Source.MongoDB.Configuration>`_ in the *AWS Database Migration Service User Guide* .\n')
    my_sql_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_MySqlSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target MySQL endpoint. For information about other available settings, see `Extra connection attributes when using MySQL as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.ConnectionAttrib>`_ and `Extra connection attributes when using a MySQL-compatible database as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.MySQL.html#CHAP_Target.MySQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    neptune_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_NeptuneSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Amazon Neptune endpoint. For more information about the available settings, see `Specifying endpoint settings for Amazon Neptune as a target <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Neptune.html#CHAP_Target.Neptune.EndpointSettings>`_ in the *AWS Database Migration Service User Guide* .\n')
    oracle_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_OracleSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target Oracle endpoint. For information about other available settings, see `Extra connection attributes when using Oracle as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.ConnectionAttrib>`_ and `Extra connection attributes when using Oracle as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Oracle.html#CHAP_Target.Oracle.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    password: typing.Optional[str] = pydantic.Field(None, description='The password to be used to log in to the endpoint database.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port used by the endpoint database.\n')
    postgre_sql_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_PostgreSqlSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target PostgreSQL endpoint. For information about other available settings, see `Extra connection attributes when using PostgreSQL as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.ConnectionAttrib>`_ and `Extra connection attributes when using PostgreSQL as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.PostgreSQL.html#CHAP_Target.PostgreSQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    redis_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_RedisSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Redis endpoint. For information about other available settings, see `Specifying endpoint settings for Redis as a target <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redis.html#CHAP_Target.Redis.EndpointSettings>`_ in the *AWS Database Migration Service User Guide* .\n')
    redshift_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_RedshiftSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the Amazon Redshift endpoint. For more information about other available settings, see `Extra connection attributes when using Amazon Redshift as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description="A display name for the resource identifier at the end of the ``EndpointArn`` response parameter that is returned in the created ``Endpoint`` object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as ``Example-App-ARN1`` . For example, this value might result in the ``EndpointArn`` value ``arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1`` . If you don't specify a ``ResourceIdentifier`` value, AWS DMS generates a default identifier value for the end of ``EndpointArn`` .\n")
    s3_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_S3SettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target Amazon S3 endpoint. For more information about other available settings, see `Extra connection attributes when using Amazon S3 as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.S3.html#CHAP_Source.S3.Configuring>`_ and `Extra connection attributes when using Amazon S3 as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring>`_ in the *AWS Database Migration Service User Guide* .\n')
    server_name: typing.Optional[str] = pydantic.Field(None, description='The name of the server where the endpoint database resides.\n')
    ssl_mode: typing.Optional[str] = pydantic.Field(None, description='The Secure Sockets Layer (SSL) mode to use for the SSL connection. The default is ``none`` . .. epigraph:: When ``engine_name`` is set to S3, the only allowed value is ``none`` .\n')
    sybase_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_SybaseSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target SAP ASE endpoint. For information about other available settings, see `Extra connection attributes when using SAP ASE as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.SAP.html#CHAP_Source.SAP.ConnectionAttrib>`_ and `Extra connection attributes when using SAP ASE as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.SAP.html#CHAP_Target.SAP.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the endpoint.\n')
    username: typing.Optional[str] = pydantic.Field(None, description='The user name to be used to log in to the endpoint database.')
    _init_params: typing.ClassVar[list[str]] = ['endpoint_type', 'engine_name', 'certificate_arn', 'database_name', 'doc_db_settings', 'dynamo_db_settings', 'elasticsearch_settings', 'endpoint_identifier', 'extra_connection_attributes', 'gcp_my_sql_settings', 'ibm_db2_settings', 'kafka_settings', 'kinesis_settings', 'kms_key_id', 'microsoft_sql_server_settings', 'mongo_db_settings', 'my_sql_settings', 'neptune_settings', 'oracle_settings', 'password', 'port', 'postgre_sql_settings', 'redis_settings', 'redshift_settings', 'resource_identifier', 's3_settings', 'server_name', 'ssl_mode', 'sybase_settings', 'tags', 'username']
    _method_names: typing.ClassVar[list[str]] = ['DocDbSettingsProperty', 'DynamoDbSettingsProperty', 'ElasticsearchSettingsProperty', 'GcpMySQLSettingsProperty', 'IbmDb2SettingsProperty', 'KafkaSettingsProperty', 'KinesisSettingsProperty', 'MicrosoftSqlServerSettingsProperty', 'MongoDbSettingsProperty', 'MySqlSettingsProperty', 'NeptuneSettingsProperty', 'OracleSettingsProperty', 'PostgreSqlSettingsProperty', 'RedisSettingsProperty', 'RedshiftSettingsProperty', 'S3SettingsProperty', 'SybaseSettingsProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpoint'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnEndpointDefConfig] = pydantic.Field(None)


class CfnEndpointDefConfig(pydantic.BaseModel):
    DocDbSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefDocdbsettingspropertyParams]] = pydantic.Field(None, description='')
    DynamoDbSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefDynamodbsettingspropertyParams]] = pydantic.Field(None, description='')
    ElasticsearchSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefElasticsearchsettingspropertyParams]] = pydantic.Field(None, description='')
    GcpMySQLSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefGcpmysqlsettingspropertyParams]] = pydantic.Field(None, description='')
    IbmDb2SettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefIbmdb2SettingspropertyParams]] = pydantic.Field(None, description='')
    KafkaSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefKafkasettingspropertyParams]] = pydantic.Field(None, description='')
    KinesisSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefKinesissettingspropertyParams]] = pydantic.Field(None, description='')
    MicrosoftSqlServerSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefMicrosoftsqlserversettingspropertyParams]] = pydantic.Field(None, description='')
    MongoDbSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefMongodbsettingspropertyParams]] = pydantic.Field(None, description='')
    MySqlSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefMysqlsettingspropertyParams]] = pydantic.Field(None, description='')
    NeptuneSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefNeptunesettingspropertyParams]] = pydantic.Field(None, description='')
    OracleSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefOraclesettingspropertyParams]] = pydantic.Field(None, description='')
    PostgreSqlSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefPostgresqlsettingspropertyParams]] = pydantic.Field(None, description='')
    RedisSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefRedissettingspropertyParams]] = pydantic.Field(None, description='')
    RedshiftSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefRedshiftsettingspropertyParams]] = pydantic.Field(None, description='')
    S3SettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefS3SettingspropertyParams]] = pydantic.Field(None, description='')
    SybaseSettingsProperty: typing.Optional[list[models.aws_dms.CfnEndpointDefSybasesettingspropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnEndpointDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnEndpointDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnEndpointDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnEndpointDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnEndpointDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnEndpointDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnEndpointDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnEndpointDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnEndpointDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnEndpointDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnEndpointDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnEndpointDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnEndpointDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnEndpointDefDocdbsettingspropertyParams(pydantic.BaseModel):
    docs_to_investigate: typing.Union[int, float, None] = pydantic.Field(None, description='')
    extract_doc_id: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    nesting_level: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefDynamodbsettingspropertyParams(pydantic.BaseModel):
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefElasticsearchsettingspropertyParams(pydantic.BaseModel):
    endpoint_uri: typing.Optional[str] = pydantic.Field(None, description='')
    error_retry_duration: typing.Union[int, float, None] = pydantic.Field(None, description='')
    full_load_error_percentage: typing.Union[int, float, None] = pydantic.Field(None, description='')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefGcpmysqlsettingspropertyParams(pydantic.BaseModel):
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description='')
    clean_source_metadata_on_mismatch: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    database_name: typing.Optional[str] = pydantic.Field(None, description='')
    events_poll_interval: typing.Union[int, float, None] = pydantic.Field(None, description='')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    parallel_load_threads: typing.Union[int, float, None] = pydantic.Field(None, description='')
    password: typing.Optional[str] = pydantic.Field(None, description='')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    server_name: typing.Optional[str] = pydantic.Field(None, description='')
    server_timezone: typing.Optional[str] = pydantic.Field(None, description='')
    username: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefIbmdb2SettingspropertyParams(pydantic.BaseModel):
    current_lsn: typing.Optional[str] = pydantic.Field(None, description='')
    keep_csv_files: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    load_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    max_k_bytes_per_read: typing.Union[int, float, None] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    set_data_capture_changes: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    write_buffer_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefKafkasettingspropertyParams(pydantic.BaseModel):
    broker: typing.Optional[str] = pydantic.Field(None, description='')
    include_control_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_null_and_empty: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_partition_value: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_table_alter_operations: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_transaction_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    message_format: typing.Optional[str] = pydantic.Field(None, description='')
    message_max_bytes: typing.Union[int, float, None] = pydantic.Field(None, description='')
    no_hex_prefix: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    partition_include_schema_table: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    sasl_password: typing.Optional[str] = pydantic.Field(None, description='')
    sasl_user_name: typing.Optional[str] = pydantic.Field(None, description='')
    security_protocol: typing.Optional[str] = pydantic.Field(None, description='')
    ssl_ca_certificate_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ssl_client_certificate_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ssl_client_key_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ssl_client_key_password: typing.Optional[str] = pydantic.Field(None, description='')
    topic: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefKinesissettingspropertyParams(pydantic.BaseModel):
    include_control_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_null_and_empty: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_partition_value: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_table_alter_operations: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    include_transaction_details: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    message_format: typing.Optional[str] = pydantic.Field(None, description='')
    no_hex_prefix: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    partition_include_schema_table: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    stream_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefMicrosoftsqlserversettingspropertyParams(pydantic.BaseModel):
    bcp_packet_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    control_tables_file_group: typing.Optional[str] = pydantic.Field(None, description='')
    database_name: typing.Optional[str] = pydantic.Field(None, description='')
    force_lob_lookup: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    password: typing.Optional[str] = pydantic.Field(None, description='')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    query_single_always_on_node: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    read_backup_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    safeguard_policy: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    server_name: typing.Optional[str] = pydantic.Field(None, description='')
    tlog_access_mode: typing.Optional[str] = pydantic.Field(None, description='')
    trim_space_in_char: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    use_bcp_full_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    username: typing.Optional[str] = pydantic.Field(None, description='')
    use_third_party_backup_device: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefMongodbsettingspropertyParams(pydantic.BaseModel):
    auth_mechanism: typing.Optional[str] = pydantic.Field(None, description='')
    auth_source: typing.Optional[str] = pydantic.Field(None, description='')
    auth_type: typing.Optional[str] = pydantic.Field(None, description='')
    database_name: typing.Optional[str] = pydantic.Field(None, description='')
    docs_to_investigate: typing.Optional[str] = pydantic.Field(None, description='')
    extract_doc_id: typing.Optional[str] = pydantic.Field(None, description='')
    nesting_level: typing.Optional[str] = pydantic.Field(None, description='')
    password: typing.Optional[str] = pydantic.Field(None, description='')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    server_name: typing.Optional[str] = pydantic.Field(None, description='')
    username: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefMysqlsettingspropertyParams(pydantic.BaseModel):
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description='')
    clean_source_metadata_on_mismatch: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    events_poll_interval: typing.Union[int, float, None] = pydantic.Field(None, description='')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    parallel_load_threads: typing.Union[int, float, None] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    server_timezone: typing.Optional[str] = pydantic.Field(None, description='')
    target_db_type: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefNeptunesettingspropertyParams(pydantic.BaseModel):
    error_retry_duration: typing.Union[int, float, None] = pydantic.Field(None, description='')
    iam_auth_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    max_retry_count: typing.Union[int, float, None] = pydantic.Field(None, description='')
    s3_bucket_folder: typing.Optional[str] = pydantic.Field(None, description='')
    s3_bucket_name: typing.Optional[str] = pydantic.Field(None, description='')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefOraclesettingspropertyParams(pydantic.BaseModel):
    access_alternate_directly: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    additional_archived_log_dest_id: typing.Union[int, float, None] = pydantic.Field(None, description='')
    add_supplemental_logging: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    allow_select_nested_tables: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    archived_log_dest_id: typing.Union[int, float, None] = pydantic.Field(None, description='')
    archived_logs_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    asm_password: typing.Optional[str] = pydantic.Field(None, description='')
    asm_server: typing.Optional[str] = pydantic.Field(None, description='')
    asm_user: typing.Optional[str] = pydantic.Field(None, description='')
    char_length_semantics: typing.Optional[str] = pydantic.Field(None, description='')
    direct_path_no_log: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    direct_path_parallel_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    enable_homogenous_tablespace: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    extra_archived_log_dest_ids: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[int, float]], None] = pydantic.Field(None, description='')
    fail_tasks_on_lob_truncation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    number_datatype_scale: typing.Union[int, float, None] = pydantic.Field(None, description='')
    oracle_path_prefix: typing.Optional[str] = pydantic.Field(None, description='')
    parallel_asm_read_threads: typing.Union[int, float, None] = pydantic.Field(None, description='')
    read_ahead_blocks: typing.Union[int, float, None] = pydantic.Field(None, description='')
    read_table_space_name: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    replace_path_prefix: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    retry_interval: typing.Union[int, float, None] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_oracle_asm_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_oracle_asm_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    security_db_encryption: typing.Optional[str] = pydantic.Field(None, description='')
    security_db_encryption_name: typing.Optional[str] = pydantic.Field(None, description='')
    spatial_data_option_to_geo_json_function_name: typing.Optional[str] = pydantic.Field(None, description='')
    standby_delay_time: typing.Union[int, float, None] = pydantic.Field(None, description='')
    use_alternate_folder_for_online: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    use_b_file: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    use_direct_path_full_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    use_logminer_reader: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    use_path_prefix: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefPostgresqlsettingspropertyParams(pydantic.BaseModel):
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description='')
    babelfish_database_name: typing.Optional[str] = pydantic.Field(None, description='')
    capture_ddls: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    database_mode: typing.Optional[str] = pydantic.Field(None, description='')
    ddl_artifacts_schema: typing.Optional[str] = pydantic.Field(None, description='')
    execute_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    fail_tasks_on_lob_truncation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    heartbeat_enable: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    heartbeat_frequency: typing.Union[int, float, None] = pydantic.Field(None, description='')
    heartbeat_schema: typing.Optional[str] = pydantic.Field(None, description='')
    map_boolean_as_boolean: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    plugin_name: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    slot_name: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefRedissettingspropertyParams(pydantic.BaseModel):
    auth_password: typing.Optional[str] = pydantic.Field(None, description='')
    auth_type: typing.Optional[str] = pydantic.Field(None, description='')
    auth_user_name: typing.Optional[str] = pydantic.Field(None, description='')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='')
    server_name: typing.Optional[str] = pydantic.Field(None, description='')
    ssl_ca_certificate_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ssl_security_protocol: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefRedshiftsettingspropertyParams(pydantic.BaseModel):
    accept_any_date: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    after_connect_script: typing.Optional[str] = pydantic.Field(None, description='')
    bucket_folder: typing.Optional[str] = pydantic.Field(None, description='')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description='')
    case_sensitive_names: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    comp_update: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    connection_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    date_format: typing.Optional[str] = pydantic.Field(None, description='')
    empty_as_null: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    encryption_mode: typing.Optional[str] = pydantic.Field(None, description='')
    explicit_ids: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    file_transfer_upload_streams: typing.Union[int, float, None] = pydantic.Field(None, description='')
    load_timeout: typing.Union[int, float, None] = pydantic.Field(None, description='')
    map_boolean_as_boolean: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    remove_quotes: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    replace_chars: typing.Optional[str] = pydantic.Field(None, description='')
    replace_invalid_chars: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    server_side_encryption_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    time_format: typing.Optional[str] = pydantic.Field(None, description='')
    trim_blanks: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    truncate_columns: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    write_buffer_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefS3SettingspropertyParams(pydantic.BaseModel):
    add_column_name: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    add_trailing_padding_character: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    bucket_folder: typing.Optional[str] = pydantic.Field(None, description='')
    bucket_name: typing.Optional[str] = pydantic.Field(None, description='')
    canned_acl_for_objects: typing.Optional[str] = pydantic.Field(None, description='')
    cdc_inserts_and_updates: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    cdc_inserts_only: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    cdc_max_batch_interval: typing.Union[int, float, None] = pydantic.Field(None, description='')
    cdc_min_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    cdc_path: typing.Optional[str] = pydantic.Field(None, description='')
    compression_type: typing.Optional[str] = pydantic.Field(None, description='')
    csv_delimiter: typing.Optional[str] = pydantic.Field(None, description='')
    csv_no_sup_value: typing.Optional[str] = pydantic.Field(None, description='')
    csv_null_value: typing.Optional[str] = pydantic.Field(None, description='')
    csv_row_delimiter: typing.Optional[str] = pydantic.Field(None, description='')
    data_format: typing.Optional[str] = pydantic.Field(None, description='')
    data_page_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    date_partition_delimiter: typing.Optional[str] = pydantic.Field(None, description='')
    date_partition_enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    date_partition_sequence: typing.Optional[str] = pydantic.Field(None, description='')
    date_partition_timezone: typing.Optional[str] = pydantic.Field(None, description='')
    dict_page_size_limit: typing.Union[int, float, None] = pydantic.Field(None, description='')
    enable_statistics: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    encoding_type: typing.Optional[str] = pydantic.Field(None, description='')
    encryption_mode: typing.Optional[str] = pydantic.Field(None, description='')
    expected_bucket_owner: typing.Optional[str] = pydantic.Field(None, description='')
    external_table_definition: typing.Optional[str] = pydantic.Field(None, description='')
    glue_catalog_generation: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ignore_header_rows: typing.Union[int, float, None] = pydantic.Field(None, description='')
    include_op_for_full_load: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    max_file_size: typing.Union[int, float, None] = pydantic.Field(None, description='')
    parquet_timestamp_in_millisecond: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    parquet_version: typing.Optional[str] = pydantic.Field(None, description='')
    preserve_transactions: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    rfc4180: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    row_group_length: typing.Union[int, float, None] = pydantic.Field(None, description='')
    server_side_encryption_kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    service_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    timestamp_column_name: typing.Optional[str] = pydantic.Field(None, description='')
    use_csv_no_sup_value: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    use_task_start_time_for_full_load_timestamp: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefSybasesettingspropertyParams(pydantic.BaseModel):
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnEndpointDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnEndpointDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEndpointDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnEndpointDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEndpointDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnEndpointDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnEndpointDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnEndpointDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnEndpointDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnEndpointDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEndpointDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnEndpointDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnEndpointDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEndpointDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnEventSubscription
class CfnEventSubscriptionDef(BaseCfnResource):
    sns_topic_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the Amazon SNS topic created for event notification. The ARN is created by Amazon SNS when you create a topic and subscribe to it.\n')
    enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Indicates whether to activate the subscription. If you don't specify this property, AWS CloudFormation activates the subscription.\n")
    event_categories: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of event categories for a source type that you want to subscribe to. If you don't specify this property, you are notified about all event categories. For more information, see `Working with Events and Notifications <https://docs.aws.amazon.com//dms/latest/userguide/CHAP_Events.html>`_ in the *AWS DMS User Guide* .\n")
    source_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of identifiers for which AWS DMS provides notification events. If you don't specify a value, notifications are provided for all sources. If you specify multiple values, they must be of the same type. For example, if you specify a database instance ID, then all of the other values must be database instance IDs.\n")
    source_type: typing.Optional[str] = pydantic.Field(None, description="The type of AWS DMS resource that generates the events. For example, if you want to be notified of events generated by a replication instance, you set this parameter to ``replication-instance`` . If this value isn't specified, all events are returned. *Valid values* : ``replication-instance`` | ``replication-task``\n")
    subscription_name: typing.Optional[str] = pydantic.Field(None, description='The name of the AWS DMS event notification subscription. This name must be less than 255 characters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the event subscription.')
    _init_params: typing.ClassVar[list[str]] = ['sns_topic_arn', 'enabled', 'event_categories', 'source_ids', 'source_type', 'subscription_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEventSubscription'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnEventSubscriptionDefConfig] = pydantic.Field(None)


class CfnEventSubscriptionDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnEventSubscriptionDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnEventSubscriptionDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnEventSubscriptionDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventSubscriptionDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnEventSubscriptionDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventSubscriptionDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnEventSubscriptionDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnEventSubscriptionDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnEventSubscriptionDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnEventSubscriptionDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnEventSubscriptionDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnEventSubscriptionDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnEventSubscriptionDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnEventSubscriptionDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnEventSubscriptionDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnInstanceProfile
class CfnInstanceProfileDef(BaseCfnResource):
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The Availability Zone where the instance profile runs.\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the instance profile. Descriptions can have up to 31 characters. A description can contain only ASCII letters, digits, and hyphens ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter.\n")
    instance_profile_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the instance profile. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    instance_profile_name: typing.Optional[str] = pydantic.Field(None, description='The user-friendly name for the instance profile.\n')
    kms_key_arn: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the AWS KMS key that is used to encrypt the connection parameters for the instance profile. If you don't specify a value for the ``KmsKeyArn`` parameter, then AWS DMS uses your default encryption key. AWS KMS creates the default encryption key for your AWS account . Your AWS account has a different default encryption key for each AWS Region .\n")
    network_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the network type for the instance profile. A value of ``IPV4`` represents an instance profile with IPv4 network type and only supports IPv4 addressing. A value of ``IPV6`` represents an instance profile with IPv6 network type and only supports IPv6 addressing. A value of ``DUAL`` represents an instance profile with dual network type that supports IPv4 and IPv6 addressing.\n')
    publicly_accessible: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies the accessibility options for the instance profile. A value of ``true`` represents an instance profile with a public IP address. A value of ``false`` represents an instance profile with a private IP address. The default value is ``true`` . Default: - false\n')
    subnet_group_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the subnet group that is associated with the instance profile.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.\n')
    vpc_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The VPC security groups that are used with the instance profile. The VPC security group must work with the VPC containing the instance profile.')
    _init_params: typing.ClassVar[list[str]] = ['availability_zone', 'description', 'instance_profile_identifier', 'instance_profile_name', 'kms_key_arn', 'network_type', 'publicly_accessible', 'subnet_group_identifier', 'tags', 'vpc_security_groups']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnInstanceProfile'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnInstanceProfileDefConfig] = pydantic.Field(None)


class CfnInstanceProfileDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnInstanceProfileDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnInstanceProfileDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnInstanceProfileDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnInstanceProfileDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnInstanceProfileDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnInstanceProfileDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnInstanceProfileDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnInstanceProfileDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnInstanceProfileDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnInstanceProfileDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnInstanceProfileDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnInstanceProfileDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnInstanceProfileDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnInstanceProfileDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnInstanceProfileDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnMigrationProject
class CfnMigrationProjectDef(BaseCfnResource):
    description: typing.Optional[str] = pydantic.Field(None, description='A user-friendly description of the migration project.\n')
    instance_profile_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the instance profile for your migration project.\n')
    instance_profile_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the instance profile for your migration project.\n')
    instance_profile_name: typing.Optional[str] = pydantic.Field(None, description='The name of the associated instance profile.\n')
    migration_project_creation_time: typing.Optional[str] = pydantic.Field(None, description='(deprecated) The property describes a creating time of the migration project.\n')
    migration_project_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the migration project. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    migration_project_name: typing.Optional[str] = pydantic.Field(None, description='The name of the migration project.\n')
    schema_conversion_application_attributes: typing.Union[models.UnsupportedResource, models.aws_dms.CfnMigrationProject_SchemaConversionApplicationAttributesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The schema conversion application attributes, including the Amazon S3 bucket name and Amazon S3 role ARN.\n')
    source_data_provider_descriptors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_dms.CfnMigrationProject_DataProviderDescriptorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Information about the source data provider, including the name or ARN, and AWS Secrets Manager parameters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.\n')
    target_data_provider_descriptors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_dms.CfnMigrationProject_DataProviderDescriptorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Information about the target data provider, including the name or ARN, and AWS Secrets Manager parameters.\n')
    transformation_rules: typing.Optional[str] = pydantic.Field(None, description='The settings in JSON format for migration rules. Migration rules make it possible for you to change the object names according to the rules that you specify. For example, you can change an object name to lowercase or uppercase, add or remove a prefix or suffix, or rename objects.')
    _init_params: typing.ClassVar[list[str]] = ['description', 'instance_profile_arn', 'instance_profile_identifier', 'instance_profile_name', 'migration_project_creation_time', 'migration_project_identifier', 'migration_project_name', 'schema_conversion_application_attributes', 'source_data_provider_descriptors', 'tags', 'target_data_provider_descriptors', 'transformation_rules']
    _method_names: typing.ClassVar[list[str]] = ['DataProviderDescriptorProperty', 'SchemaConversionApplicationAttributesProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnMigrationProject'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnMigrationProjectDefConfig] = pydantic.Field(None)


class CfnMigrationProjectDefConfig(pydantic.BaseModel):
    DataProviderDescriptorProperty: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefDataproviderdescriptorpropertyParams]] = pydantic.Field(None, description='')
    SchemaConversionApplicationAttributesProperty: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefSchemaconversionapplicationattributespropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnMigrationProjectDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnMigrationProjectDefDataproviderdescriptorpropertyParams(pydantic.BaseModel):
    data_provider_arn: typing.Optional[str] = pydantic.Field(None, description='')
    data_provider_identifier: typing.Optional[str] = pydantic.Field(None, description='')
    data_provider_name: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_access_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    secrets_manager_secret_id: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnMigrationProjectDefSchemaconversionapplicationattributespropertyParams(pydantic.BaseModel):
    s3_bucket_path: typing.Optional[str] = pydantic.Field(None, description='')
    s3_bucket_role_arn: typing.Optional[str] = pydantic.Field(None, description='')
    ...

class CfnMigrationProjectDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnMigrationProjectDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMigrationProjectDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnMigrationProjectDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMigrationProjectDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnMigrationProjectDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnMigrationProjectDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnMigrationProjectDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnMigrationProjectDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnMigrationProjectDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnMigrationProjectDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnMigrationProjectDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnMigrationProjectDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnMigrationProjectDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnReplicationConfig
class CfnReplicationConfigDef(BaseCfnResource):
    compute_config: typing.Union[models.UnsupportedResource, models.aws_dms.CfnReplicationConfig_ComputeConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration parameters for provisioning an AWS DMS Serverless replication.\n')
    replication_config_identifier: typing.Optional[str] = pydantic.Field(None, description='A unique identifier that you want to use to create a ``ReplicationConfigArn`` that is returned as part of the output from this action. You can then pass this output ``ReplicationConfigArn`` as the value of the ``ReplicationConfigArn`` option for other actions to identify both AWS DMS Serverless replications and replication configurations that you want those actions to operate on. For some actions, you can also use either this unique identifier or a corresponding ARN in action filters to identify the specific replication and replication configuration to operate on.\n')
    replication_settings: typing.Any = pydantic.Field(None, description='Optional JSON settings for AWS DMS Serverless replications that are provisioned using this replication configuration. For example, see `Change processing tuning settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.ChangeProcessingTuning.html>`_ .\n')
    replication_type: typing.Optional[str] = pydantic.Field(None, description='The type of AWS DMS Serverless replication to provision using this replication configuration. Possible values: - ``"full-load"`` - ``"cdc"`` - ``"full-load-and-cdc"``\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description='Optional unique value or name that you set for a given resource that can be used to construct an Amazon Resource Name (ARN) for that resource. For more information, see `Fine-grained access control using resource names and tags <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#CHAP_Security.FineGrainedAccess>`_ .\n')
    source_endpoint_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the source endpoint for this AWS DMS Serverless replication configuration.\n')
    supplemental_settings: typing.Any = pydantic.Field(None, description='Optional JSON settings for specifying supplemental data. For more information, see `Specifying supplemental data for task settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.TaskData.html>`_ .\n')
    table_mappings: typing.Any = pydantic.Field(None, description='JSON table mappings for AWS DMS Serverless replications that are provisioned using this replication configuration. For more information, see `Specifying table selection and transformations rules using JSON <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.html>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more optional tags associated with resources used by the AWS DMS Serverless replication. For more information, see `Tagging resources in AWS Database Migration Service <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tagging.html>`_ .\n')
    target_endpoint_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the target endpoint for this AWS DMS serverless replication configuration.')
    _init_params: typing.ClassVar[list[str]] = ['compute_config', 'replication_config_identifier', 'replication_settings', 'replication_type', 'resource_identifier', 'source_endpoint_arn', 'supplemental_settings', 'table_mappings', 'tags', 'target_endpoint_arn']
    _method_names: typing.ClassVar[list[str]] = ['ComputeConfigProperty', 'add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationConfig'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnReplicationConfigDefConfig] = pydantic.Field(None)


class CfnReplicationConfigDefConfig(pydantic.BaseModel):
    ComputeConfigProperty: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefComputeconfigpropertyParams]] = pydantic.Field(None, description='')
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnReplicationConfigDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    cdk_tag_manager_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnReplicationConfigDefComputeconfigpropertyParams(pydantic.BaseModel):
    max_capacity_units: typing.Union[int, float] = pydantic.Field(..., description='')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='')
    dns_name_servers: typing.Optional[str] = pydantic.Field(None, description='')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description='')
    min_capacity_units: typing.Union[int, float, None] = pydantic.Field(None, description='')
    multi_az: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='')
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='')
    replication_subnet_group_id: typing.Optional[str] = pydantic.Field(None, description='')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='')
    ...

class CfnReplicationConfigDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnReplicationConfigDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationConfigDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnReplicationConfigDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationConfigDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnReplicationConfigDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnReplicationConfigDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnReplicationConfigDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnReplicationConfigDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnReplicationConfigDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationConfigDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnReplicationConfigDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnReplicationConfigDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationConfigDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnReplicationInstance
class CfnReplicationInstanceDef(BaseCfnResource):
    replication_instance_class: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The compute and memory capacity of the replication instance as defined for the specified replication instance class. For example, to specify the instance class dms.c4.large, set this parameter to ``"dms.c4.large"`` . For more information on the settings and capacities for the available replication instance classes, see `Selecting the right AWS DMS replication instance for your migration <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html#CHAP_ReplicationInstance.InDepth>`_ in the *AWS Database Migration Service User Guide* .\n')
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of storage (in gigabytes) to be initially allocated for the replication instance.\n')
    allow_major_version_upgrade: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Indicates that major version upgrades are allowed. Changing this parameter does not result in an outage, and the change is asynchronously applied as soon as possible. This parameter must be set to ``true`` when specifying a value for the ``EngineVersion`` parameter that is a different major version than the replication instance's current version.\n")
    auto_minor_version_upgrade: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that indicates whether minor engine upgrades are applied automatically to the replication instance during the maintenance window. This parameter defaults to ``true`` . Default: ``true``\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description="The Availability Zone that the replication instance will be created in. The default value is a random, system-chosen Availability Zone in the endpoint's AWS Region , for example ``us-east-1d`` .\n")
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The engine version number of the replication instance. If an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="An AWS KMS key identifier that is used to encrypt the data on the replication instance. If you don't specify a value for the ``KmsKeyId`` parameter, AWS DMS uses your default encryption key. AWS KMS creates the default encryption key for your AWS account . Your AWS account has a different default encryption key for each AWS Region .\n")
    multi_az: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether the replication instance is a Multi-AZ deployment. You can't set the ``AvailabilityZone`` parameter if the Multi-AZ parameter is set to ``true`` .\n")
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range during which system maintenance can occur, in UTC. *Format* : ``ddd:hh24:mi-ddd:hh24:mi`` *Default* : A 30-minute window selected at random from an 8-hour block of time per AWS Region , occurring on a random day of the week. *Valid days* ( ``ddd`` ): ``Mon`` | ``Tue`` | ``Wed`` | ``Thu`` | ``Fri`` | ``Sat`` | ``Sun`` *Constraints* : Minimum 30-minute window.\n')
    publicly_accessible: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies the accessibility options for the replication instance. A value of ``true`` represents an instance with a public IP address. A value of ``false`` represents an instance with a private IP address. The default value is ``true`` .\n')
    replication_instance_identifier: typing.Optional[str] = pydantic.Field(None, description="The replication instance identifier. This parameter is stored as a lowercase string. Constraints: - Must contain 1-63 alphanumeric characters or hyphens. - First character must be a letter. - Can't end with a hyphen or contain two consecutive hyphens. Example: ``myrepinstance``\n")
    replication_subnet_group_identifier: typing.Optional[str] = pydantic.Field(None, description='A subnet group to associate with the replication instance.\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description="A display name for the resource identifier at the end of the ``EndpointArn`` response parameter that is returned in the created ``Endpoint`` object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as ``Example-App-ARN1`` . For example, this value might result in the ``EndpointArn`` value ``arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1`` . If you don't specify a ``ResourceIdentifier`` value, AWS DMS generates a default identifier value for the end of ``EndpointArn`` .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the replication instance.\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Specifies the virtual private cloud (VPC) security group to be used with the replication instance. The VPC security group must work with the VPC containing the replication instance.')
    _init_params: typing.ClassVar[list[str]] = ['replication_instance_class', 'allocated_storage', 'allow_major_version_upgrade', 'auto_minor_version_upgrade', 'availability_zone', 'engine_version', 'kms_key_id', 'multi_az', 'preferred_maintenance_window', 'publicly_accessible', 'replication_instance_identifier', 'replication_subnet_group_identifier', 'resource_identifier', 'tags', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationInstance'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnReplicationInstanceDefConfig] = pydantic.Field(None)


class CfnReplicationInstanceDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnReplicationInstanceDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnReplicationInstanceDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnReplicationInstanceDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationInstanceDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnReplicationInstanceDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationInstanceDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnReplicationInstanceDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnReplicationInstanceDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnReplicationInstanceDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnReplicationInstanceDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnReplicationInstanceDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationInstanceDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnReplicationInstanceDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnReplicationInstanceDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationInstanceDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnReplicationSubnetGroup
class CfnReplicationSubnetGroupDef(BaseCfnResource):
    replication_subnet_group_description: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The description for the subnet group.\n')
    subnet_ids: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='One or more subnet IDs to be assigned to the subnet group.\n')
    replication_subnet_group_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier for the replication subnet group. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the identifier.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the subnet group.')
    _init_params: typing.ClassVar[list[str]] = ['replication_subnet_group_description', 'subnet_ids', 'replication_subnet_group_identifier', 'tags']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationSubnetGroup'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnReplicationSubnetGroupDefConfig] = pydantic.Field(None)


class CfnReplicationSubnetGroupDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnReplicationSubnetGroupDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnReplicationSubnetGroupDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnReplicationSubnetGroupDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationSubnetGroupDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnReplicationSubnetGroupDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationSubnetGroupDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnReplicationSubnetGroupDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnReplicationSubnetGroupDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnReplicationSubnetGroupDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnReplicationSubnetGroupDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnReplicationSubnetGroupDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationSubnetGroupDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnReplicationSubnetGroupDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnReplicationSubnetGroupDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationSubnetGroupDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnReplicationTask
class CfnReplicationTaskDef(BaseCfnResource):
    migration_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The migration type. Valid values: ``full-load`` | ``cdc`` | ``full-load-and-cdc``\n')
    replication_instance_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of a replication instance.\n')
    source_endpoint_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An Amazon Resource Name (ARN) that uniquely identifies the source endpoint.\n')
    table_mappings: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The table mappings for the task, in JSON format. For more information, see `Using Table Mapping to Specify Task Settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    target_endpoint_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An Amazon Resource Name (ARN) that uniquely identifies the target endpoint.\n')
    cdc_start_position: typing.Optional[str] = pydantic.Field(None, description='Indicates when you want a change data capture (CDC) operation to start. Use either ``CdcStartPosition`` or ``CdcStartTime`` to specify when you want a CDC operation to start. Specifying both values results in an error. The value can be in date, checkpoint, log sequence number (LSN), or system change number (SCN) format. Here is a date example: ``--cdc-start-position "2018-03-08T12:12:12"`` Here is a checkpoint example: ``--cdc-start-position "checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93"`` Here is an LSN example: ``--cdc-start-position mysql-bin-changelog.000024:373`` .. epigraph:: When you use this task setting with a source PostgreSQL database, a logical replication slot should already be created and associated with the source endpoint. You can verify this by setting the ``slotName`` extra connection attribute to the name of this logical replication slot. For more information, see `Extra Connection Attributes When Using PostgreSQL as a Source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    cdc_start_time: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates the start time for a change data capture (CDC) operation.\n')
    cdc_stop_position: typing.Optional[str] = pydantic.Field(None, description='Indicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time. Here is a server time example: ``--cdc-stop-position "server_time:2018-02-09T12:12:12"`` Here is a commit time example: ``--cdc-stop-position "commit_time: 2018-02-09T12:12:12"``\n')
    replication_task_identifier: typing.Optional[str] = pydantic.Field(None, description='An identifier for the replication task. Constraints: - Must contain 1-255 alphanumeric characters or hyphens. - First character must be a letter. - Cannot end with a hyphen or contain two consecutive hyphens.\n')
    replication_task_settings: typing.Optional[str] = pydantic.Field(None, description='Overall settings for the task, in JSON format. For more information, see `Specifying Task Settings for AWS Database Migration Service Tasks <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description="A display name for the resource identifier at the end of the ``EndpointArn`` response parameter that is returned in the created ``Endpoint`` object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as ``Example-App-ARN1`` . For example, this value might result in the ``EndpointArn`` value ``arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1`` . If you don't specify a ``ResourceIdentifier`` value, AWS DMS generates a default identifier value for the end of ``EndpointArn`` .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the replication task.\n')
    task_data: typing.Optional[str] = pydantic.Field(None, description='Supplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see `Specifying Supplemental Data for Task Settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.TaskData.html>`_ in the *AWS Database Migration Service User Guide.*')
    _init_params: typing.ClassVar[list[str]] = ['migration_type', 'replication_instance_arn', 'source_endpoint_arn', 'table_mappings', 'target_endpoint_arn', 'cdc_start_position', 'cdc_start_time', 'cdc_stop_position', 'replication_task_identifier', 'replication_task_settings', 'resource_identifier', 'tags', 'task_data']
    _method_names: typing.ClassVar[list[str]] = ['add_deletion_override', 'add_dependency', 'add_depends_on', 'add_metadata', 'add_override', 'add_property_deletion_override', 'add_property_override', 'apply_removal_policy', 'get_att', 'get_metadata', 'inspect', 'obtain_dependencies', 'obtain_resource_dependencies', 'override_logical_id', 'remove_dependency', 'replace_dependency']
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationTask'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...


    resource_config: typing.Optional[models.aws_dms.CfnReplicationTaskDefConfig] = pydantic.Field(None)


class CfnReplicationTaskDefConfig(pydantic.BaseModel):
    add_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefAddDeletionOverrideParams]] = pydantic.Field(None, description='Syntactic sugar for ``addOverride(path, undefined)``.')
    add_dependency: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefAddDependencyParams]] = pydantic.Field(None, description='Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.\nThis can be used for resources across stacks (or nested stack) boundaries\nand the dependency will automatically be transferred to the relevant scope.')
    add_depends_on: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefAddDependsOnParams]] = pydantic.Field(None, description='(deprecated) Indicates that this resource depends on another resource and cannot be provisioned unless the other resource has been successfully provisioned.')
    add_metadata: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefAddMetadataParams]] = pydantic.Field(None, description='Add a value to the CloudFormation Resource Metadata.')
    add_override: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefAddOverrideParams]] = pydantic.Field(None, description='Adds an override to the synthesized CloudFormation resource.\nTo add a\nproperty override, either use ``addPropertyOverride`` or prefix ``path`` with\n"Properties." (i.e. ``Properties.TopicName``).\n\nIf the override is nested, separate each nested level using a dot (.) in the path parameter.\nIf there is an array as part of the nesting, specify the index in the path.\n\nTo include a literal ``.`` in the property name, prefix with a ``\\``. In most\nprogramming languages you will need to write this as ``"\\\\."`` because the\n``\\`` itself will need to be escaped.\n\nFor example::\n\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.0.Projection.NonKeyAttributes", ["myattribute"])\n   cfn_resource.add_override("Properties.GlobalSecondaryIndexes.1.ProjectionType", "INCLUDE")\n\nwould add the overrides Example::\n\n   "Properties": {\n     "GlobalSecondaryIndexes": [\n       {\n         "Projection": {\n           "NonKeyAttributes": [ "myattribute" ]\n           ...\n         }\n         ...\n       },\n       {\n         "ProjectionType": "INCLUDE"\n         ...\n       },\n     ]\n     ...\n   }\n\nThe ``value`` argument to ``addOverride`` will not be processed or translated\nin any way. Pass raw JSON values in here with the correct capitalization\nfor CloudFormation. If you pass CDK classes or structs, they will be\nrendered with lowercased key names, and CloudFormation will reject the\ntemplate.')
    add_property_deletion_override: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefAddPropertyDeletionOverrideParams]] = pydantic.Field(None, description='Adds an override that deletes the value of a property from the resource definition.')
    add_property_override: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefAddPropertyOverrideParams]] = pydantic.Field(None, description='Adds an override to a resource property.\nSyntactic sugar for ``addOverride("Properties.<...>", value)``.')
    apply_removal_policy: typing.Optional[list[models.GenericApplyRemovalPolicyParams]] = pydantic.Field(None)
    get_att: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefGetAttParams]] = pydantic.Field(None, description='Returns a token for an runtime attribute of this resource.\nIdeally, use generated attribute accessors (e.g. ``resource.arn``), but this can be used for future compatibility\nin case there is no generated attribute.')
    get_metadata: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefGetMetadataParams]] = pydantic.Field(None, description='Retrieve a value value from the CloudFormation Resource Metadata.')
    inspect: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefInspectParams]] = pydantic.Field(None, description='Examines the CloudFormation resource and discloses attributes.')
    obtain_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Retrieves an array of resources this resource depends on.\nThis assembles dependencies on resources across stacks (including nested stacks)\nautomatically.')
    obtain_resource_dependencies: typing.Optional[bool] = pydantic.Field(None, description='Get a shallow copy of dependencies between this resource and other resources in the same stack.')
    override_logical_id: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefOverrideLogicalIdParams]] = pydantic.Field(None, description='Overrides the auto-generated logical ID with a specific ID.')
    remove_dependency: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefRemoveDependencyParams]] = pydantic.Field(None, description='Indicates that this resource no longer depends on another resource.\nThis can be used for resources across stacks (including nested stacks)\nand the dependency will automatically be removed from the relevant scope.')
    replace_dependency: typing.Optional[list[models.aws_dms.CfnReplicationTaskDefReplaceDependencyParams]] = pydantic.Field(None, description='Replaces one dependency with another.')
    tags_config: typing.Optional[models.core.TagManagerDefConfig] = pydantic.Field(None)

class CfnReplicationTaskDefAddDeletionOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='The path of the value to delete.')
    ...

class CfnReplicationTaskDefAddDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationTaskDefAddDependsOnParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-\n\n:deprecated: use addDependency\n\n:stability: deprecated\n')
    ...

class CfnReplicationTaskDefAddMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n')
    value: typing.Any = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationTaskDefAddOverrideParams(pydantic.BaseModel):
    path: str = pydantic.Field(..., description='- The path of the property, you can use dot notation to override values in complex types. Any intermediate keys will be created as needed.\n')
    value: typing.Any = pydantic.Field(..., description='- The value. Could be primitive or complex.')
    ...

class CfnReplicationTaskDefAddPropertyDeletionOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path to the property.')
    ...

class CfnReplicationTaskDefAddPropertyOverrideParams(pydantic.BaseModel):
    property_path: str = pydantic.Field(..., description='The path of the property.\n')
    value: typing.Any = pydantic.Field(..., description='The value.')
    ...

class CfnReplicationTaskDefApplyRemovalPolicyParams(pydantic.BaseModel):
    policy: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description='-\n')
    apply_to_update_replace_policy: typing.Optional[bool] = pydantic.Field(None, description='Apply the same deletion policy to the resource\'s "UpdateReplacePolicy". Default: true\n')
    default: typing.Optional[aws_cdk.RemovalPolicy] = pydantic.Field(None, description="The default policy to apply in case the removal policy is not defined. Default: - Default value is resource specific. To determine the default value for a resource, please consult that specific resource's documentation.\n\n:see: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html#aws-attribute-deletionpolicy-options\n")
    ...

class CfnReplicationTaskDefGetAttParams(pydantic.BaseModel):
    attribute_name: str = pydantic.Field(..., description='The name of the attribute.\n')
    type_hint: typing.Optional[aws_cdk.ResolutionTypeHint] = pydantic.Field(None, description='-')
    return_config: typing.Optional[list[models.core.ReferenceDefConfig]] = pydantic.Field(None)
    ...

class CfnReplicationTaskDefGetMetadataParams(pydantic.BaseModel):
    key: str = pydantic.Field(..., description='-\n\n:see:\n\nhttps://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/metadata-section-structure.html\n\nNote that this is a different set of metadata from CDK node metadata; this\nmetadata ends up in the stack template under the resource, whereas CDK\nnode metadata ends up in the Cloud Assembly.\n')
    ...

class CfnReplicationTaskDefInspectParams(pydantic.BaseModel):
    inspector: models.TreeInspectorDef = pydantic.Field(..., description='tree inspector to collect and process attributes.')
    ...

class CfnReplicationTaskDefOverrideLogicalIdParams(pydantic.BaseModel):
    new_logical_id: str = pydantic.Field(..., description='The new logical ID to use for this stack element.')
    ...

class CfnReplicationTaskDefRemoveDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='-')
    ...

class CfnReplicationTaskDefReplaceDependencyParams(pydantic.BaseModel):
    target: models.CfnResourceDef = pydantic.Field(..., description='The dependency to replace.\n')
    new_target: models.CfnResourceDef = pydantic.Field(..., description='The new dependency to add.')
    ...


#  autogenerated from aws_cdk.aws_dms.CfnCertificateProps
class CfnCertificatePropsDef(BaseCfnProperty):
    certificate_identifier: typing.Optional[str] = pydantic.Field(None, description="A customer-assigned name for the certificate. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen or contain two consecutive hyphens.\n")
    certificate_pem: typing.Optional[str] = pydantic.Field(None, description='The contents of a ``.pem`` file, which contains an X.509 certificate.\n')
    certificate_wallet: typing.Optional[str] = pydantic.Field(None, description='The location of an imported Oracle Wallet certificate for use with SSL. An example is: ``filebase64("${path.root}/rds-ca-2019-root.sso")``\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-certificate.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_certificate_props = dms.CfnCertificateProps(\n        certificate_identifier="certificateIdentifier",\n        certificate_pem="certificatePem",\n        certificate_wallet="certificateWallet"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['certificate_identifier', 'certificate_pem', 'certificate_wallet']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnCertificateProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnDataProviderProps
class CfnDataProviderPropsDef(BaseCfnProperty):
    engine: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of database engine for the data provider. Valid values include ``"aurora"`` , ``"aurora-postgresql"`` , ``"mysql"`` , ``"oracle"`` , ``"postgres"`` , ``"sqlserver"`` , ``redshift`` , ``mariadb`` , ``mongodb`` , and ``docdb`` . A value of ``"aurora"`` represents Amazon Aurora MySQL-Compatible Edition.\n')
    data_provider_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the data provider. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    data_provider_name: typing.Optional[str] = pydantic.Field(None, description='The name of the data provider.\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the data provider. Descriptions can have up to 31 characters. A description can contain only ASCII letters, digits, and hyphens ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter.\n")
    exact_settings: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='The property describes the exact settings which can be modified. Default: - false\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-dataprovider.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_data_provider_props = dms.CfnDataProviderProps(\n        engine="engine",\n\n        # the properties below are optional\n        data_provider_identifier="dataProviderIdentifier",\n        data_provider_name="dataProviderName",\n        description="description",\n        exact_settings=False,\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['engine', 'data_provider_identifier', 'data_provider_name', 'description', 'exact_settings', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnDataProviderProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEndpointProps
class CfnEndpointPropsDef(BaseCfnProperty):
    endpoint_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of endpoint. Valid values are ``source`` and ``target`` .\n')
    engine_name: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The type of engine for the endpoint, depending on the ``EndpointType`` value. *Valid values* : ``mysql`` | ``oracle`` | ``postgres`` | ``mariadb`` | ``aurora`` | ``aurora-postgresql`` | ``opensearch`` | ``redshift`` | ``redshift-serverless`` | ``s3`` | ``db2`` | ``azuredb`` | ``sybase`` | ``dynamodb`` | ``mongodb`` | ``kinesis`` | ``kafka`` | ``elasticsearch`` | ``docdb`` | ``sqlserver`` | ``neptune``\n')
    certificate_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) for the certificate.\n')
    database_name: typing.Optional[str] = pydantic.Field(None, description="The name of the endpoint database. For a MySQL source or target endpoint, don't specify ``DatabaseName`` . To migrate to a specific database, use this setting and ``targetDbType`` .\n")
    doc_db_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_DocDbSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target DocumentDB endpoint. For more information about other available settings, see `Using extra connections attributes with Amazon DocumentDB as a source <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.DocumentDB.html#CHAP_Source.DocumentDB.ECAs>`_ and `Using Amazon DocumentDB as a target for AWS Database Migration Service <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DocumentDB.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    dynamo_db_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_DynamoDbSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Amazon DynamoDB endpoint. For information about other available settings, see `Using object mapping to migrate data to DynamoDB <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.DynamoDB.html#CHAP_Target.DynamoDB.ObjectMapping>`_ in the *AWS Database Migration Service User Guide* .\n')
    elasticsearch_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_ElasticsearchSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target OpenSearch endpoint. For more information about the available settings, see `Extra connection attributes when using OpenSearch as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Elasticsearch.html#CHAP_Target.Elasticsearch.Configuration>`_ in the *AWS Database Migration Service User Guide* .\n')
    endpoint_identifier: typing.Optional[str] = pydantic.Field(None, description="The database endpoint identifier. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    extra_connection_attributes: typing.Optional[str] = pydantic.Field(None, description='Additional attributes associated with the connection. Each attribute is specified as a name-value pair associated by an equal sign (=). Multiple attributes are separated by a semicolon (;) with no additional white space. For information on the attributes available for connecting your source or target endpoint, see `Working with AWS DMS Endpoints <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Endpoints.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    gcp_my_sql_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_GcpMySQLSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source GCP MySQL endpoint. These settings are much the same as the settings for any MySQL-compatible endpoint. For more information, see `Extra connection attributes when using MySQL as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    ibm_db2_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_IbmDb2SettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source IBM Db2 LUW endpoint. For information about other available settings, see `Extra connection attributes when using Db2 LUW as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.DB2.html#CHAP_Source.DB2.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    kafka_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_KafkaSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Apache Kafka endpoint. For more information about other available settings, see `Using object mapping to migrate data to a Kafka topic <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kafka.html#CHAP_Target.Kafka.ObjectMapping>`_ in the *AWS Database Migration Service User Guide* .\n')
    kinesis_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_KinesisSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target endpoint for Amazon Kinesis Data Streams. For more information about other available settings, see `Using object mapping to migrate data to a Kinesis data stream <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Kinesis.html#CHAP_Target.Kinesis.ObjectMapping>`_ in the *AWS Database Migration Service User Guide* .\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="An AWS KMS key identifier that is used to encrypt the connection parameters for the endpoint. If you don't specify a value for the ``KmsKeyId`` parameter, AWS DMS uses your default encryption key. AWS KMS creates the default encryption key for your AWS account . Your AWS account has a different default encryption key for each AWS Region .\n")
    microsoft_sql_server_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_MicrosoftSqlServerSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target Microsoft SQL Server endpoint. For information about other available settings, see `Extra connection attributes when using SQL Server as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.SQLServer.html#CHAP_Source.SQLServer.ConnectionAttrib>`_ and `Extra connection attributes when using SQL Server as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.SQLServer.html#CHAP_Target.SQLServer.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    mongo_db_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_MongoDbSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source MongoDB endpoint. For more information about the available settings, see `Using MongoDB as a target for AWS Database Migration Service <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MongoDB.html#CHAP_Source.MongoDB.Configuration>`_ in the *AWS Database Migration Service User Guide* .\n')
    my_sql_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_MySqlSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target MySQL endpoint. For information about other available settings, see `Extra connection attributes when using MySQL as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.ConnectionAttrib>`_ and `Extra connection attributes when using a MySQL-compatible database as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.MySQL.html#CHAP_Target.MySQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    neptune_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_NeptuneSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Amazon Neptune endpoint. For more information about the available settings, see `Specifying endpoint settings for Amazon Neptune as a target <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Neptune.html#CHAP_Target.Neptune.EndpointSettings>`_ in the *AWS Database Migration Service User Guide* .\n')
    oracle_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_OracleSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target Oracle endpoint. For information about other available settings, see `Extra connection attributes when using Oracle as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.Oracle.html#CHAP_Source.Oracle.ConnectionAttrib>`_ and `Extra connection attributes when using Oracle as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Oracle.html#CHAP_Target.Oracle.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    password: typing.Optional[str] = pydantic.Field(None, description='The password to be used to log in to the endpoint database.\n')
    port: typing.Union[int, float, None] = pydantic.Field(None, description='The port used by the endpoint database.\n')
    postgre_sql_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_PostgreSqlSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target PostgreSQL endpoint. For information about other available settings, see `Extra connection attributes when using PostgreSQL as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.ConnectionAttrib>`_ and `Extra connection attributes when using PostgreSQL as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.PostgreSQL.html#CHAP_Target.PostgreSQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    redis_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_RedisSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the target Redis endpoint. For information about other available settings, see `Specifying endpoint settings for Redis as a target <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redis.html#CHAP_Target.Redis.EndpointSettings>`_ in the *AWS Database Migration Service User Guide* .\n')
    redshift_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_RedshiftSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the Amazon Redshift endpoint. For more information about other available settings, see `Extra connection attributes when using Amazon Redshift as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.Redshift.html#CHAP_Target.Redshift.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description="A display name for the resource identifier at the end of the ``EndpointArn`` response parameter that is returned in the created ``Endpoint`` object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as ``Example-App-ARN1`` . For example, this value might result in the ``EndpointArn`` value ``arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1`` . If you don't specify a ``ResourceIdentifier`` value, AWS DMS generates a default identifier value for the end of ``EndpointArn`` .\n")
    s3_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_S3SettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target Amazon S3 endpoint. For more information about other available settings, see `Extra connection attributes when using Amazon S3 as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.S3.html#CHAP_Source.S3.Configuring>`_ and `Extra connection attributes when using Amazon S3 as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.S3.html#CHAP_Target.S3.Configuring>`_ in the *AWS Database Migration Service User Guide* .\n')
    server_name: typing.Optional[str] = pydantic.Field(None, description='The name of the server where the endpoint database resides.\n')
    ssl_mode: typing.Optional[str] = pydantic.Field(None, description='The Secure Sockets Layer (SSL) mode to use for the SSL connection. The default is ``none`` . .. epigraph:: When ``engine_name`` is set to S3, the only allowed value is ``none`` .\n')
    sybase_settings: typing.Union[models.UnsupportedResource, models.aws_dms.CfnEndpoint_SybaseSettingsPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Settings in JSON format for the source and target SAP ASE endpoint. For information about other available settings, see `Extra connection attributes when using SAP ASE as a source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.SAP.html#CHAP_Source.SAP.ConnectionAttrib>`_ and `Extra connection attributes when using SAP ASE as a target for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Target.SAP.html#CHAP_Target.SAP.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the endpoint.\n')
    username: typing.Optional[str] = pydantic.Field(None, description='The user name to be used to log in to the endpoint database.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-endpoint.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_endpoint_props = dms.CfnEndpointProps(\n        endpoint_type="endpointType",\n        engine_name="engineName",\n\n        # the properties below are optional\n        certificate_arn="certificateArn",\n        database_name="databaseName",\n        doc_db_settings=dms.CfnEndpoint.DocDbSettingsProperty(\n            docs_to_investigate=123,\n            extract_doc_id=False,\n            nesting_level="nestingLevel",\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId"\n        ),\n        dynamo_db_settings=dms.CfnEndpoint.DynamoDbSettingsProperty(\n            service_access_role_arn="serviceAccessRoleArn"\n        ),\n        elasticsearch_settings=dms.CfnEndpoint.ElasticsearchSettingsProperty(\n            endpoint_uri="endpointUri",\n            error_retry_duration=123,\n            full_load_error_percentage=123,\n            service_access_role_arn="serviceAccessRoleArn"\n        ),\n        endpoint_identifier="endpointIdentifier",\n        extra_connection_attributes="extraConnectionAttributes",\n        gcp_my_sql_settings=dms.CfnEndpoint.GcpMySQLSettingsProperty(\n            after_connect_script="afterConnectScript",\n            clean_source_metadata_on_mismatch=False,\n            database_name="databaseName",\n            events_poll_interval=123,\n            max_file_size=123,\n            parallel_load_threads=123,\n            password="password",\n            port=123,\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            server_name="serverName",\n            server_timezone="serverTimezone",\n            username="username"\n        ),\n        ibm_db2_settings=dms.CfnEndpoint.IbmDb2SettingsProperty(\n            current_lsn="currentLsn",\n            keep_csv_files=False,\n            load_timeout=123,\n            max_file_size=123,\n            max_kBytes_per_read=123,\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            set_data_capture_changes=False,\n            write_buffer_size=123\n        ),\n        kafka_settings=dms.CfnEndpoint.KafkaSettingsProperty(\n            broker="broker",\n            include_control_details=False,\n            include_null_and_empty=False,\n            include_partition_value=False,\n            include_table_alter_operations=False,\n            include_transaction_details=False,\n            message_format="messageFormat",\n            message_max_bytes=123,\n            no_hex_prefix=False,\n            partition_include_schema_table=False,\n            sasl_password="saslPassword",\n            sasl_user_name="saslUserName",\n            security_protocol="securityProtocol",\n            ssl_ca_certificate_arn="sslCaCertificateArn",\n            ssl_client_certificate_arn="sslClientCertificateArn",\n            ssl_client_key_arn="sslClientKeyArn",\n            ssl_client_key_password="sslClientKeyPassword",\n            topic="topic"\n        ),\n        kinesis_settings=dms.CfnEndpoint.KinesisSettingsProperty(\n            include_control_details=False,\n            include_null_and_empty=False,\n            include_partition_value=False,\n            include_table_alter_operations=False,\n            include_transaction_details=False,\n            message_format="messageFormat",\n            no_hex_prefix=False,\n            partition_include_schema_table=False,\n            service_access_role_arn="serviceAccessRoleArn",\n            stream_arn="streamArn"\n        ),\n        kms_key_id="kmsKeyId",\n        microsoft_sql_server_settings=dms.CfnEndpoint.MicrosoftSqlServerSettingsProperty(\n            bcp_packet_size=123,\n            control_tables_file_group="controlTablesFileGroup",\n            database_name="databaseName",\n            force_lob_lookup=False,\n            password="password",\n            port=123,\n            query_single_always_on_node=False,\n            read_backup_only=False,\n            safeguard_policy="safeguardPolicy",\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            server_name="serverName",\n            tlog_access_mode="tlogAccessMode",\n            trim_space_in_char=False,\n            use_bcp_full_load=False,\n            username="username",\n            use_third_party_backup_device=False\n        ),\n        mongo_db_settings=dms.CfnEndpoint.MongoDbSettingsProperty(\n            auth_mechanism="authMechanism",\n            auth_source="authSource",\n            auth_type="authType",\n            database_name="databaseName",\n            docs_to_investigate="docsToInvestigate",\n            extract_doc_id="extractDocId",\n            nesting_level="nestingLevel",\n            password="password",\n            port=123,\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            server_name="serverName",\n            username="username"\n        ),\n        my_sql_settings=dms.CfnEndpoint.MySqlSettingsProperty(\n            after_connect_script="afterConnectScript",\n            clean_source_metadata_on_mismatch=False,\n            events_poll_interval=123,\n            max_file_size=123,\n            parallel_load_threads=123,\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            server_timezone="serverTimezone",\n            target_db_type="targetDbType"\n        ),\n        neptune_settings=dms.CfnEndpoint.NeptuneSettingsProperty(\n            error_retry_duration=123,\n            iam_auth_enabled=False,\n            max_file_size=123,\n            max_retry_count=123,\n            s3_bucket_folder="s3BucketFolder",\n            s3_bucket_name="s3BucketName",\n            service_access_role_arn="serviceAccessRoleArn"\n        ),\n        oracle_settings=dms.CfnEndpoint.OracleSettingsProperty(\n            access_alternate_directly=False,\n            additional_archived_log_dest_id=123,\n            add_supplemental_logging=False,\n            allow_select_nested_tables=False,\n            archived_log_dest_id=123,\n            archived_logs_only=False,\n            asm_password="asmPassword",\n            asm_server="asmServer",\n            asm_user="asmUser",\n            char_length_semantics="charLengthSemantics",\n            direct_path_no_log=False,\n            direct_path_parallel_load=False,\n            enable_homogenous_tablespace=False,\n            extra_archived_log_dest_ids=[123],\n            fail_tasks_on_lob_truncation=False,\n            number_datatype_scale=123,\n            oracle_path_prefix="oraclePathPrefix",\n            parallel_asm_read_threads=123,\n            read_ahead_blocks=123,\n            read_table_space_name=False,\n            replace_path_prefix=False,\n            retry_interval=123,\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_oracle_asm_access_role_arn="secretsManagerOracleAsmAccessRoleArn",\n            secrets_manager_oracle_asm_secret_id="secretsManagerOracleAsmSecretId",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            security_db_encryption="securityDbEncryption",\n            security_db_encryption_name="securityDbEncryptionName",\n            spatial_data_option_to_geo_json_function_name="spatialDataOptionToGeoJsonFunctionName",\n            standby_delay_time=123,\n            use_alternate_folder_for_online=False,\n            use_bFile=False,\n            use_direct_path_full_load=False,\n            use_logminer_reader=False,\n            use_path_prefix="usePathPrefix"\n        ),\n        password="password",\n        port=123,\n        postgre_sql_settings=dms.CfnEndpoint.PostgreSqlSettingsProperty(\n            after_connect_script="afterConnectScript",\n            babelfish_database_name="babelfishDatabaseName",\n            capture_ddls=False,\n            database_mode="databaseMode",\n            ddl_artifacts_schema="ddlArtifactsSchema",\n            execute_timeout=123,\n            fail_tasks_on_lob_truncation=False,\n            heartbeat_enable=False,\n            heartbeat_frequency=123,\n            heartbeat_schema="heartbeatSchema",\n            map_boolean_as_boolean=False,\n            max_file_size=123,\n            plugin_name="pluginName",\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            slot_name="slotName"\n        ),\n        redis_settings=dms.CfnEndpoint.RedisSettingsProperty(\n            auth_password="authPassword",\n            auth_type="authType",\n            auth_user_name="authUserName",\n            port=123,\n            server_name="serverName",\n            ssl_ca_certificate_arn="sslCaCertificateArn",\n            ssl_security_protocol="sslSecurityProtocol"\n        ),\n        redshift_settings=dms.CfnEndpoint.RedshiftSettingsProperty(\n            accept_any_date=False,\n            after_connect_script="afterConnectScript",\n            bucket_folder="bucketFolder",\n            bucket_name="bucketName",\n            case_sensitive_names=False,\n            comp_update=False,\n            connection_timeout=123,\n            date_format="dateFormat",\n            empty_as_null=False,\n            encryption_mode="encryptionMode",\n            explicit_ids=False,\n            file_transfer_upload_streams=123,\n            load_timeout=123,\n            map_boolean_as_boolean=False,\n            max_file_size=123,\n            remove_quotes=False,\n            replace_chars="replaceChars",\n            replace_invalid_chars="replaceInvalidChars",\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId",\n            server_side_encryption_kms_key_id="serverSideEncryptionKmsKeyId",\n            service_access_role_arn="serviceAccessRoleArn",\n            time_format="timeFormat",\n            trim_blanks=False,\n            truncate_columns=False,\n            write_buffer_size=123\n        ),\n        resource_identifier="resourceIdentifier",\n        s3_settings=dms.CfnEndpoint.S3SettingsProperty(\n            add_column_name=False,\n            add_trailing_padding_character=False,\n            bucket_folder="bucketFolder",\n            bucket_name="bucketName",\n            canned_acl_for_objects="cannedAclForObjects",\n            cdc_inserts_and_updates=False,\n            cdc_inserts_only=False,\n            cdc_max_batch_interval=123,\n            cdc_min_file_size=123,\n            cdc_path="cdcPath",\n            compression_type="compressionType",\n            csv_delimiter="csvDelimiter",\n            csv_no_sup_value="csvNoSupValue",\n            csv_null_value="csvNullValue",\n            csv_row_delimiter="csvRowDelimiter",\n            data_format="dataFormat",\n            data_page_size=123,\n            date_partition_delimiter="datePartitionDelimiter",\n            date_partition_enabled=False,\n            date_partition_sequence="datePartitionSequence",\n            date_partition_timezone="datePartitionTimezone",\n            dict_page_size_limit=123,\n            enable_statistics=False,\n            encoding_type="encodingType",\n            encryption_mode="encryptionMode",\n            expected_bucket_owner="expectedBucketOwner",\n            external_table_definition="externalTableDefinition",\n            glue_catalog_generation=False,\n            ignore_header_rows=123,\n            include_op_for_full_load=False,\n            max_file_size=123,\n            parquet_timestamp_in_millisecond=False,\n            parquet_version="parquetVersion",\n            preserve_transactions=False,\n            rfc4180=False,\n            row_group_length=123,\n            server_side_encryption_kms_key_id="serverSideEncryptionKmsKeyId",\n            service_access_role_arn="serviceAccessRoleArn",\n            timestamp_column_name="timestampColumnName",\n            use_csv_no_sup_value=False,\n            use_task_start_time_for_full_load_timestamp=False\n        ),\n        server_name="serverName",\n        ssl_mode="sslMode",\n        sybase_settings=dms.CfnEndpoint.SybaseSettingsProperty(\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId"\n        ),\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        username="username"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['endpoint_type', 'engine_name', 'certificate_arn', 'database_name', 'doc_db_settings', 'dynamo_db_settings', 'elasticsearch_settings', 'endpoint_identifier', 'extra_connection_attributes', 'gcp_my_sql_settings', 'ibm_db2_settings', 'kafka_settings', 'kinesis_settings', 'kms_key_id', 'microsoft_sql_server_settings', 'mongo_db_settings', 'my_sql_settings', 'neptune_settings', 'oracle_settings', 'password', 'port', 'postgre_sql_settings', 'redis_settings', 'redshift_settings', 'resource_identifier', 's3_settings', 'server_name', 'ssl_mode', 'sybase_settings', 'tags', 'username']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEndpointProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnEventSubscriptionProps
class CfnEventSubscriptionPropsDef(BaseCfnProperty):
    sns_topic_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of the Amazon SNS topic created for event notification. The ARN is created by Amazon SNS when you create a topic and subscribe to it.\n')
    enabled: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Indicates whether to activate the subscription. If you don't specify this property, AWS CloudFormation activates the subscription.\n")
    event_categories: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of event categories for a source type that you want to subscribe to. If you don't specify this property, you are notified about all event categories. For more information, see `Working with Events and Notifications <https://docs.aws.amazon.com//dms/latest/userguide/CHAP_Events.html>`_ in the *AWS DMS User Guide* .\n")
    source_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description="A list of identifiers for which AWS DMS provides notification events. If you don't specify a value, notifications are provided for all sources. If you specify multiple values, they must be of the same type. For example, if you specify a database instance ID, then all of the other values must be database instance IDs.\n")
    source_type: typing.Optional[str] = pydantic.Field(None, description="The type of AWS DMS resource that generates the events. For example, if you want to be notified of events generated by a replication instance, you set this parameter to ``replication-instance`` . If this value isn't specified, all events are returned. *Valid values* : ``replication-instance`` | ``replication-task``\n")
    subscription_name: typing.Optional[str] = pydantic.Field(None, description='The name of the AWS DMS event notification subscription. This name must be less than 255 characters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the event subscription.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-eventsubscription.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_event_subscription_props = dms.CfnEventSubscriptionProps(\n        sns_topic_arn="snsTopicArn",\n\n        # the properties below are optional\n        enabled=False,\n        event_categories=["eventCategories"],\n        source_ids=["sourceIds"],\n        source_type="sourceType",\n        subscription_name="subscriptionName",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['sns_topic_arn', 'enabled', 'event_categories', 'source_ids', 'source_type', 'subscription_name', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnEventSubscriptionProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnInstanceProfileProps
class CfnInstanceProfilePropsDef(BaseCfnProperty):
    availability_zone: typing.Optional[str] = pydantic.Field(None, description='The Availability Zone where the instance profile runs.\n')
    description: typing.Optional[str] = pydantic.Field(None, description="A description of the instance profile. Descriptions can have up to 31 characters. A description can contain only ASCII letters, digits, and hyphens ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter.\n")
    instance_profile_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the instance profile. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    instance_profile_name: typing.Optional[str] = pydantic.Field(None, description='The user-friendly name for the instance profile.\n')
    kms_key_arn: typing.Optional[str] = pydantic.Field(None, description="The Amazon Resource Name (ARN) of the AWS KMS key that is used to encrypt the connection parameters for the instance profile. If you don't specify a value for the ``KmsKeyArn`` parameter, then AWS DMS uses your default encryption key. AWS KMS creates the default encryption key for your AWS account . Your AWS account has a different default encryption key for each AWS Region .\n")
    network_type: typing.Optional[str] = pydantic.Field(None, description='Specifies the network type for the instance profile. A value of ``IPV4`` represents an instance profile with IPv4 network type and only supports IPv4 addressing. A value of ``IPV6`` represents an instance profile with IPv6 network type and only supports IPv6 addressing. A value of ``DUAL`` represents an instance profile with dual network type that supports IPv4 and IPv6 addressing.\n')
    publicly_accessible: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies the accessibility options for the instance profile. A value of ``true`` represents an instance profile with a public IP address. A value of ``false`` represents an instance profile with a private IP address. The default value is ``true`` . Default: - false\n')
    subnet_group_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the subnet group that is associated with the instance profile.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.\n')
    vpc_security_groups: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='The VPC security groups that are used with the instance profile. The VPC security group must work with the VPC containing the instance profile.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-instanceprofile.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_instance_profile_props = dms.CfnInstanceProfileProps(\n        availability_zone="availabilityZone",\n        description="description",\n        instance_profile_identifier="instanceProfileIdentifier",\n        instance_profile_name="instanceProfileName",\n        kms_key_arn="kmsKeyArn",\n        network_type="networkType",\n        publicly_accessible=False,\n        subnet_group_identifier="subnetGroupIdentifier",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        vpc_security_groups=["vpcSecurityGroups"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['availability_zone', 'description', 'instance_profile_identifier', 'instance_profile_name', 'kms_key_arn', 'network_type', 'publicly_accessible', 'subnet_group_identifier', 'tags', 'vpc_security_groups']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnInstanceProfileProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnMigrationProjectProps
class CfnMigrationProjectPropsDef(BaseCfnProperty):
    description: typing.Optional[str] = pydantic.Field(None, description='A user-friendly description of the migration project.\n')
    instance_profile_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the instance profile for your migration project.\n')
    instance_profile_identifier: typing.Optional[str] = pydantic.Field(None, description='The identifier of the instance profile for your migration project.\n')
    instance_profile_name: typing.Optional[str] = pydantic.Field(None, description='The name of the associated instance profile.\n')
    migration_project_creation_time: typing.Optional[str] = pydantic.Field(None, description='(deprecated) The property describes a creating time of the migration project.\n')
    migration_project_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier of the migration project. Identifiers must begin with a letter and must contain only ASCII letters, digits, and hyphens. They can't end with a hyphen, or contain two consecutive hyphens.\n")
    migration_project_name: typing.Optional[str] = pydantic.Field(None, description='The name of the migration project.\n')
    schema_conversion_application_attributes: typing.Union[models.UnsupportedResource, models.aws_dms.CfnMigrationProject_SchemaConversionApplicationAttributesPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='The schema conversion application attributes, including the Amazon S3 bucket name and Amazon S3 role ARN.\n')
    source_data_provider_descriptors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_dms.CfnMigrationProject_DataProviderDescriptorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Information about the source data provider, including the name or ARN, and AWS Secrets Manager parameters.\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='An array of key-value pairs to apply to this resource.\n')
    target_data_provider_descriptors: typing.Union[models.UnsupportedResource, typing.Sequence[typing.Union[models.UnsupportedResource, models.aws_dms.CfnMigrationProject_DataProviderDescriptorPropertyDef, dict[str, typing.Any]]], None] = pydantic.Field(None, description='Information about the target data provider, including the name or ARN, and AWS Secrets Manager parameters.\n')
    transformation_rules: typing.Optional[str] = pydantic.Field(None, description='The settings in JSON format for migration rules. Migration rules make it possible for you to change the object names according to the rules that you specify. For example, you can change an object name to lowercase or uppercase, add or remove a prefix or suffix, or rename objects.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-migrationproject.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_migration_project_props = dms.CfnMigrationProjectProps(\n        description="description",\n        instance_profile_arn="instanceProfileArn",\n        instance_profile_identifier="instanceProfileIdentifier",\n        instance_profile_name="instanceProfileName",\n        migration_project_creation_time="migrationProjectCreationTime",\n        migration_project_identifier="migrationProjectIdentifier",\n        migration_project_name="migrationProjectName",\n        schema_conversion_application_attributes=dms.CfnMigrationProject.SchemaConversionApplicationAttributesProperty(\n            s3_bucket_path="s3BucketPath",\n            s3_bucket_role_arn="s3BucketRoleArn"\n        ),\n        source_data_provider_descriptors=[dms.CfnMigrationProject.DataProviderDescriptorProperty(\n            data_provider_arn="dataProviderArn",\n            data_provider_identifier="dataProviderIdentifier",\n            data_provider_name="dataProviderName",\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId"\n        )],\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        target_data_provider_descriptors=[dms.CfnMigrationProject.DataProviderDescriptorProperty(\n            data_provider_arn="dataProviderArn",\n            data_provider_identifier="dataProviderIdentifier",\n            data_provider_name="dataProviderName",\n            secrets_manager_access_role_arn="secretsManagerAccessRoleArn",\n            secrets_manager_secret_id="secretsManagerSecretId"\n        )],\n        transformation_rules="transformationRules"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['description', 'instance_profile_arn', 'instance_profile_identifier', 'instance_profile_name', 'migration_project_creation_time', 'migration_project_identifier', 'migration_project_name', 'schema_conversion_application_attributes', 'source_data_provider_descriptors', 'tags', 'target_data_provider_descriptors', 'transformation_rules']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnMigrationProjectProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnReplicationConfigProps
class CfnReplicationConfigPropsDef(BaseCfnProperty):
    compute_config: typing.Union[models.UnsupportedResource, models.aws_dms.CfnReplicationConfig_ComputeConfigPropertyDef, dict[str, typing.Any], None] = pydantic.Field(None, description='Configuration parameters for provisioning an AWS DMS Serverless replication.\n')
    replication_config_identifier: typing.Optional[str] = pydantic.Field(None, description='A unique identifier that you want to use to create a ``ReplicationConfigArn`` that is returned as part of the output from this action. You can then pass this output ``ReplicationConfigArn`` as the value of the ``ReplicationConfigArn`` option for other actions to identify both AWS DMS Serverless replications and replication configurations that you want those actions to operate on. For some actions, you can also use either this unique identifier or a corresponding ARN in action filters to identify the specific replication and replication configuration to operate on.\n')
    replication_settings: typing.Any = pydantic.Field(None, description='Optional JSON settings for AWS DMS Serverless replications that are provisioned using this replication configuration. For example, see `Change processing tuning settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.ChangeProcessingTuning.html>`_ .\n')
    replication_type: typing.Optional[str] = pydantic.Field(None, description='The type of AWS DMS Serverless replication to provision using this replication configuration. Possible values: - ``"full-load"`` - ``"cdc"`` - ``"full-load-and-cdc"``\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description='Optional unique value or name that you set for a given resource that can be used to construct an Amazon Resource Name (ARN) for that resource. For more information, see `Fine-grained access control using resource names and tags <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Security.html#CHAP_Security.FineGrainedAccess>`_ .\n')
    source_endpoint_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the source endpoint for this AWS DMS Serverless replication configuration.\n')
    supplemental_settings: typing.Any = pydantic.Field(None, description='Optional JSON settings for specifying supplemental data. For more information, see `Specifying supplemental data for task settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.TaskData.html>`_ .\n')
    table_mappings: typing.Any = pydantic.Field(None, description='JSON table mappings for AWS DMS Serverless replications that are provisioned using this replication configuration. For more information, see `Specifying table selection and transformations rules using JSON <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.SelectionTransformation.html>`_ .\n')
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more optional tags associated with resources used by the AWS DMS Serverless replication. For more information, see `Tagging resources in AWS Database Migration Service <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tagging.html>`_ .\n')
    target_endpoint_arn: typing.Optional[str] = pydantic.Field(None, description='The Amazon Resource Name (ARN) of the target endpoint for this AWS DMS serverless replication configuration.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-replicationconfig.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    # replication_settings: Any\n    # supplemental_settings: Any\n    # table_mappings: Any\n\n    cfn_replication_config_props = dms.CfnReplicationConfigProps(\n        compute_config=dms.CfnReplicationConfig.ComputeConfigProperty(\n            max_capacity_units=123,\n\n            # the properties below are optional\n            availability_zone="availabilityZone",\n            dns_name_servers="dnsNameServers",\n            kms_key_id="kmsKeyId",\n            min_capacity_units=123,\n            multi_az=False,\n            preferred_maintenance_window="preferredMaintenanceWindow",\n            replication_subnet_group_id="replicationSubnetGroupId",\n            vpc_security_group_ids=["vpcSecurityGroupIds"]\n        ),\n        replication_config_identifier="replicationConfigIdentifier",\n        replication_settings=replication_settings,\n        replication_type="replicationType",\n        resource_identifier="resourceIdentifier",\n        source_endpoint_arn="sourceEndpointArn",\n        supplemental_settings=supplemental_settings,\n        table_mappings=table_mappings,\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        target_endpoint_arn="targetEndpointArn"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['compute_config', 'replication_config_identifier', 'replication_settings', 'replication_type', 'resource_identifier', 'source_endpoint_arn', 'supplemental_settings', 'table_mappings', 'tags', 'target_endpoint_arn']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationConfigProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnReplicationInstanceProps
class CfnReplicationInstancePropsDef(BaseCfnProperty):
    replication_instance_class: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The compute and memory capacity of the replication instance as defined for the specified replication instance class. For example, to specify the instance class dms.c4.large, set this parameter to ``"dms.c4.large"`` . For more information on the settings and capacities for the available replication instance classes, see `Selecting the right AWS DMS replication instance for your migration <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_ReplicationInstance.html#CHAP_ReplicationInstance.InDepth>`_ in the *AWS Database Migration Service User Guide* .\n')
    allocated_storage: typing.Union[int, float, None] = pydantic.Field(None, description='The amount of storage (in gigabytes) to be initially allocated for the replication instance.\n')
    allow_major_version_upgrade: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Indicates that major version upgrades are allowed. Changing this parameter does not result in an outage, and the change is asynchronously applied as soon as possible. This parameter must be set to ``true`` when specifying a value for the ``EngineVersion`` parameter that is a different major version than the replication instance's current version.\n")
    auto_minor_version_upgrade: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='A value that indicates whether minor engine upgrades are applied automatically to the replication instance during the maintenance window. This parameter defaults to ``true`` . Default: ``true``\n')
    availability_zone: typing.Optional[str] = pydantic.Field(None, description="The Availability Zone that the replication instance will be created in. The default value is a random, system-chosen Availability Zone in the endpoint's AWS Region , for example ``us-east-1d`` .\n")
    engine_version: typing.Optional[str] = pydantic.Field(None, description='The engine version number of the replication instance. If an engine version number is not specified when a replication instance is created, the default is the latest engine version available.\n')
    kms_key_id: typing.Optional[str] = pydantic.Field(None, description="An AWS KMS key identifier that is used to encrypt the data on the replication instance. If you don't specify a value for the ``KmsKeyId`` parameter, AWS DMS uses your default encryption key. AWS KMS creates the default encryption key for your AWS account . Your AWS account has a different default encryption key for each AWS Region .\n")
    multi_az: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description="Specifies whether the replication instance is a Multi-AZ deployment. You can't set the ``AvailabilityZone`` parameter if the Multi-AZ parameter is set to ``true`` .\n")
    preferred_maintenance_window: typing.Optional[str] = pydantic.Field(None, description='The weekly time range during which system maintenance can occur, in UTC. *Format* : ``ddd:hh24:mi-ddd:hh24:mi`` *Default* : A 30-minute window selected at random from an 8-hour block of time per AWS Region , occurring on a random day of the week. *Valid days* ( ``ddd`` ): ``Mon`` | ``Tue`` | ``Wed`` | ``Thu`` | ``Fri`` | ``Sat`` | ``Sun`` *Constraints* : Minimum 30-minute window.\n')
    publicly_accessible: typing.Union[bool, models.UnsupportedResource, None] = pydantic.Field(None, description='Specifies the accessibility options for the replication instance. A value of ``true`` represents an instance with a public IP address. A value of ``false`` represents an instance with a private IP address. The default value is ``true`` .\n')
    replication_instance_identifier: typing.Optional[str] = pydantic.Field(None, description="The replication instance identifier. This parameter is stored as a lowercase string. Constraints: - Must contain 1-63 alphanumeric characters or hyphens. - First character must be a letter. - Can't end with a hyphen or contain two consecutive hyphens. Example: ``myrepinstance``\n")
    replication_subnet_group_identifier: typing.Optional[str] = pydantic.Field(None, description='A subnet group to associate with the replication instance.\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description="A display name for the resource identifier at the end of the ``EndpointArn`` response parameter that is returned in the created ``Endpoint`` object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as ``Example-App-ARN1`` . For example, this value might result in the ``EndpointArn`` value ``arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1`` . If you don't specify a ``ResourceIdentifier`` value, AWS DMS generates a default identifier value for the end of ``EndpointArn`` .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the replication instance.\n')
    vpc_security_group_ids: typing.Optional[typing.Sequence[str]] = pydantic.Field(None, description='Specifies the virtual private cloud (VPC) security group to be used with the replication instance. The VPC security group must work with the VPC containing the replication instance.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-replicationinstance.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_replication_instance_props = dms.CfnReplicationInstanceProps(\n        replication_instance_class="replicationInstanceClass",\n\n        # the properties below are optional\n        allocated_storage=123,\n        allow_major_version_upgrade=False,\n        auto_minor_version_upgrade=False,\n        availability_zone="availabilityZone",\n        engine_version="engineVersion",\n        kms_key_id="kmsKeyId",\n        multi_az=False,\n        preferred_maintenance_window="preferredMaintenanceWindow",\n        publicly_accessible=False,\n        replication_instance_identifier="replicationInstanceIdentifier",\n        replication_subnet_group_identifier="replicationSubnetGroupIdentifier",\n        resource_identifier="resourceIdentifier",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        vpc_security_group_ids=["vpcSecurityGroupIds"]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['replication_instance_class', 'allocated_storage', 'allow_major_version_upgrade', 'auto_minor_version_upgrade', 'availability_zone', 'engine_version', 'kms_key_id', 'multi_az', 'preferred_maintenance_window', 'publicly_accessible', 'replication_instance_identifier', 'replication_subnet_group_identifier', 'resource_identifier', 'tags', 'vpc_security_group_ids']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationInstanceProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnReplicationSubnetGroupProps
class CfnReplicationSubnetGroupPropsDef(BaseCfnProperty):
    replication_subnet_group_description: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The description for the subnet group.\n')
    subnet_ids: typing.Union[typing.Sequence[str], _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='One or more subnet IDs to be assigned to the subnet group.\n')
    replication_subnet_group_identifier: typing.Optional[str] = pydantic.Field(None, description="The identifier for the replication subnet group. If you don't specify a name, AWS CloudFormation generates a unique ID and uses that ID for the identifier.\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the subnet group.\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-replicationsubnetgroup.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_replication_subnet_group_props = dms.CfnReplicationSubnetGroupProps(\n        replication_subnet_group_description="replicationSubnetGroupDescription",\n        subnet_ids=["subnetIds"],\n\n        # the properties below are optional\n        replication_subnet_group_identifier="replicationSubnetGroupIdentifier",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )]\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['replication_subnet_group_description', 'subnet_ids', 'replication_subnet_group_identifier', 'tags']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationSubnetGroupProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




#  autogenerated from aws_cdk.aws_dms.CfnReplicationTaskProps
class CfnReplicationTaskPropsDef(BaseCfnProperty):
    migration_type: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The migration type. Valid values: ``full-load`` | ``cdc`` | ``full-load-and-cdc``\n')
    replication_instance_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The Amazon Resource Name (ARN) of a replication instance.\n')
    source_endpoint_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An Amazon Resource Name (ARN) that uniquely identifies the source endpoint.\n')
    table_mappings: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='The table mappings for the task, in JSON format. For more information, see `Using Table Mapping to Specify Task Settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TableMapping.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    target_endpoint_arn: typing.Union[str, _REQUIRED_INIT_PARAM] = pydantic.Field(REQUIRED_INIT_PARAM, description='An Amazon Resource Name (ARN) that uniquely identifies the target endpoint.\n')
    cdc_start_position: typing.Optional[str] = pydantic.Field(None, description='Indicates when you want a change data capture (CDC) operation to start. Use either ``CdcStartPosition`` or ``CdcStartTime`` to specify when you want a CDC operation to start. Specifying both values results in an error. The value can be in date, checkpoint, log sequence number (LSN), or system change number (SCN) format. Here is a date example: ``--cdc-start-position "2018-03-08T12:12:12"`` Here is a checkpoint example: ``--cdc-start-position "checkpoint:V1#27#mysql-bin-changelog.157832:1975:-1:2002:677883278264080:mysql-bin-changelog.157832:1876#0#0#*#0#93"`` Here is an LSN example: ``--cdc-start-position mysql-bin-changelog.000024:373`` .. epigraph:: When you use this task setting with a source PostgreSQL database, a logical replication slot should already be created and associated with the source endpoint. You can verify this by setting the ``slotName`` extra connection attribute to the name of this logical replication slot. For more information, see `Extra Connection Attributes When Using PostgreSQL as a Source for AWS DMS <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.PostgreSQL.html#CHAP_Source.PostgreSQL.ConnectionAttrib>`_ in the *AWS Database Migration Service User Guide* .\n')
    cdc_start_time: typing.Union[int, float, None] = pydantic.Field(None, description='Indicates the start time for a change data capture (CDC) operation.\n')
    cdc_stop_position: typing.Optional[str] = pydantic.Field(None, description='Indicates when you want a change data capture (CDC) operation to stop. The value can be either server time or commit time. Here is a server time example: ``--cdc-stop-position "server_time:2018-02-09T12:12:12"`` Here is a commit time example: ``--cdc-stop-position "commit_time: 2018-02-09T12:12:12"``\n')
    replication_task_identifier: typing.Optional[str] = pydantic.Field(None, description='An identifier for the replication task. Constraints: - Must contain 1-255 alphanumeric characters or hyphens. - First character must be a letter. - Cannot end with a hyphen or contain two consecutive hyphens.\n')
    replication_task_settings: typing.Optional[str] = pydantic.Field(None, description='Overall settings for the task, in JSON format. For more information, see `Specifying Task Settings for AWS Database Migration Service Tasks <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.html>`_ in the *AWS Database Migration Service User Guide* .\n')
    resource_identifier: typing.Optional[str] = pydantic.Field(None, description="A display name for the resource identifier at the end of the ``EndpointArn`` response parameter that is returned in the created ``Endpoint`` object. The value for this parameter can have up to 31 characters. It can contain only ASCII letters, digits, and hyphen ('-'). Also, it can't end with a hyphen or contain two consecutive hyphens, and can only begin with a letter, such as ``Example-App-ARN1`` . For example, this value might result in the ``EndpointArn`` value ``arn:aws:dms:eu-west-1:012345678901:rep:Example-App-ARN1`` . If you don't specify a ``ResourceIdentifier`` value, AWS DMS generates a default identifier value for the end of ``EndpointArn`` .\n")
    tags: typing.Optional[typing.Sequence[typing.Union[models.CfnTagDef, dict[str, typing.Any]]]] = pydantic.Field(None, description='One or more tags to be assigned to the replication task.\n')
    task_data: typing.Optional[str] = pydantic.Field(None, description='Supplemental information that the task requires to migrate the data for certain source and target endpoints. For more information, see `Specifying Supplemental Data for Task Settings <https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.TaskData.html>`_ in the *AWS Database Migration Service User Guide.*\n\n:see: http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-dms-replicationtask.html\n:exampleMetadata: fixture=_generated\n\nExample::\n\n    # The code below shows an example of how to instantiate this type.\n    # The values are placeholders you should change.\n    from aws_cdk import aws_dms as dms\n\n    cfn_replication_task_props = dms.CfnReplicationTaskProps(\n        migration_type="migrationType",\n        replication_instance_arn="replicationInstanceArn",\n        source_endpoint_arn="sourceEndpointArn",\n        table_mappings="tableMappings",\n        target_endpoint_arn="targetEndpointArn",\n\n        # the properties below are optional\n        cdc_start_position="cdcStartPosition",\n        cdc_start_time=123,\n        cdc_stop_position="cdcStopPosition",\n        replication_task_identifier="replicationTaskIdentifier",\n        replication_task_settings="replicationTaskSettings",\n        resource_identifier="resourceIdentifier",\n        tags=[CfnTag(\n            key="key",\n            value="value"\n        )],\n        task_data="taskData"\n    )\n')
    _init_params: typing.ClassVar[list[str]] = ['migration_type', 'replication_instance_arn', 'source_endpoint_arn', 'table_mappings', 'target_endpoint_arn', 'cdc_start_position', 'cdc_start_time', 'cdc_stop_position', 'replication_task_identifier', 'replication_task_settings', 'resource_identifier', 'tags', 'task_data']
    _method_names: typing.ClassVar[list[str]] = []
    _classmethod_names: typing.ClassVar[list[str]] = []
    _cdk_class_fqn: typing.ClassVar[str] = 'aws_cdk.aws_dms.CfnReplicationTaskProps'
    _alternate_constructor_method_names: typing.ClassVar[list[str]] = []
    ...




class ModuleModel(pydantic.BaseModel):
    CfnEndpoint_DocDbSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_DocDbSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_DynamoDbSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_DynamoDbSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_ElasticsearchSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_ElasticsearchSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_GcpMySQLSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_GcpMySQLSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_IbmDb2SettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_IbmDb2SettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_KafkaSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_KafkaSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_KinesisSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_KinesisSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_MicrosoftSqlServerSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_MicrosoftSqlServerSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_MongoDbSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_MongoDbSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_MySqlSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_MySqlSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_NeptuneSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_NeptuneSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_OracleSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_OracleSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_PostgreSqlSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_PostgreSqlSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_RedisSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_RedisSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_RedshiftSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_RedshiftSettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_S3SettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_S3SettingsPropertyDef]] = pydantic.Field(None)
    CfnEndpoint_SybaseSettingsProperty: typing.Optional[dict[str, models.aws_dms.CfnEndpoint_SybaseSettingsPropertyDef]] = pydantic.Field(None)
    CfnMigrationProject_DataProviderDescriptorProperty: typing.Optional[dict[str, models.aws_dms.CfnMigrationProject_DataProviderDescriptorPropertyDef]] = pydantic.Field(None)
    CfnMigrationProject_SchemaConversionApplicationAttributesProperty: typing.Optional[dict[str, models.aws_dms.CfnMigrationProject_SchemaConversionApplicationAttributesPropertyDef]] = pydantic.Field(None)
    CfnReplicationConfig_ComputeConfigProperty: typing.Optional[dict[str, models.aws_dms.CfnReplicationConfig_ComputeConfigPropertyDef]] = pydantic.Field(None)
    CfnCertificate: typing.Optional[dict[str, models.aws_dms.CfnCertificateDef]] = pydantic.Field(None)
    CfnDataProvider: typing.Optional[dict[str, models.aws_dms.CfnDataProviderDef]] = pydantic.Field(None)
    CfnEndpoint: typing.Optional[dict[str, models.aws_dms.CfnEndpointDef]] = pydantic.Field(None)
    CfnEventSubscription: typing.Optional[dict[str, models.aws_dms.CfnEventSubscriptionDef]] = pydantic.Field(None)
    CfnInstanceProfile: typing.Optional[dict[str, models.aws_dms.CfnInstanceProfileDef]] = pydantic.Field(None)
    CfnMigrationProject: typing.Optional[dict[str, models.aws_dms.CfnMigrationProjectDef]] = pydantic.Field(None)
    CfnReplicationConfig: typing.Optional[dict[str, models.aws_dms.CfnReplicationConfigDef]] = pydantic.Field(None)
    CfnReplicationInstance: typing.Optional[dict[str, models.aws_dms.CfnReplicationInstanceDef]] = pydantic.Field(None)
    CfnReplicationSubnetGroup: typing.Optional[dict[str, models.aws_dms.CfnReplicationSubnetGroupDef]] = pydantic.Field(None)
    CfnReplicationTask: typing.Optional[dict[str, models.aws_dms.CfnReplicationTaskDef]] = pydantic.Field(None)
    CfnCertificateProps: typing.Optional[dict[str, models.aws_dms.CfnCertificatePropsDef]] = pydantic.Field(None)
    CfnDataProviderProps: typing.Optional[dict[str, models.aws_dms.CfnDataProviderPropsDef]] = pydantic.Field(None)
    CfnEndpointProps: typing.Optional[dict[str, models.aws_dms.CfnEndpointPropsDef]] = pydantic.Field(None)
    CfnEventSubscriptionProps: typing.Optional[dict[str, models.aws_dms.CfnEventSubscriptionPropsDef]] = pydantic.Field(None)
    CfnInstanceProfileProps: typing.Optional[dict[str, models.aws_dms.CfnInstanceProfilePropsDef]] = pydantic.Field(None)
    CfnMigrationProjectProps: typing.Optional[dict[str, models.aws_dms.CfnMigrationProjectPropsDef]] = pydantic.Field(None)
    CfnReplicationConfigProps: typing.Optional[dict[str, models.aws_dms.CfnReplicationConfigPropsDef]] = pydantic.Field(None)
    CfnReplicationInstanceProps: typing.Optional[dict[str, models.aws_dms.CfnReplicationInstancePropsDef]] = pydantic.Field(None)
    CfnReplicationSubnetGroupProps: typing.Optional[dict[str, models.aws_dms.CfnReplicationSubnetGroupPropsDef]] = pydantic.Field(None)
    CfnReplicationTaskProps: typing.Optional[dict[str, models.aws_dms.CfnReplicationTaskPropsDef]] = pydantic.Field(None)
    ...

import models
